{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Final\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n: Final[int] = 900  # number of points\n",
    "\n",
    "# 1. Create three arrays of normally distributed numbers\n",
    "x1: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "x2: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "\n",
    "y: Final[np.ndarray] = np.random.choice(3, n)\n",
    "\n",
    "lat: Final[np.ndarray] = (y - 1) * 4 + np.random.normal(0, .5, n) # latent variable\n",
    "\n",
    "\n",
    "r = np.zeros((n, n))\n",
    "for i, j in product(range(n), range(n)):\n",
    "    r[i, j] = y[i] == y[j]\n",
    "r_numpy = r.copy()\n",
    "\n",
    "x: Final[np.ndarray] = np.stack((x1, x2)).T\n",
    "\n",
    "n_train: Final[int] = 200\n",
    "x_train, x_test = x[:n_train], x[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "r_train = r[:n_train][:, :n_train]\n",
    "r_test_intra = r[n_train:][:, n_train:]\n",
    "r_test_inter = r[:n_train][:, n_train:]\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add scatter points with colors based on y\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=x1,\n",
    "    y=x2,\n",
    "    z=lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=y,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Data points'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='3D Classification with Decision Boundary',\n",
    "    scene=dict(\n",
    "        xaxis_title='x1',\n",
    "        yaxis_title='x2',\n",
    "        zaxis_title='lat',\n",
    "    ),\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tabrel.sklearn_interface import TabRelClassifier\n",
    "from tabrel.utils.config import ProjectConfig, ClassifierConfig, TrainingConfig\n",
    "\n",
    "def run(X_train: np.ndarray, X_test: np.ndarray, use_rel: bool) -> TabRelClassifier:\n",
    "    config = ProjectConfig(\n",
    "        training=TrainingConfig(\n",
    "            backgnd_size=100,\n",
    "            query_size=25,\n",
    "            n_batches=4,\n",
    "            lr=1e-4,\n",
    "            n_epochs=50,\n",
    "            log_dir=Path(\"out/logs\"),\n",
    "            log_level=logging.DEBUG,\n",
    "            print_logs_to_console=True,\n",
    "            checkpoints_dir=Path(\"out/checkpoints\"),\n",
    "            allow_dirs_exist=True,\n",
    "            random_seed=42,\n",
    "        ),\n",
    "        model=ClassifierConfig(\n",
    "            n_features=X_train.shape[1],\n",
    "            d_embedding=20,\n",
    "            d_model=8,\n",
    "            nhead=1,\n",
    "            dim_feedforward=1,\n",
    "            num_layers=1,\n",
    "            num_classes=len(np.unique(y)),\n",
    "            activation=\"relu\",\n",
    "            rel=use_rel,\n",
    "            dropout=0.,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = TabRelClassifier(config)\n",
    "    model.fit(X=X_train, y=y_train, r=r_train)\n",
    "    metrics = model.evaluate(X=X_test, r_inter=r_test_inter, r_intra=r_test_intra, y=y_test)\n",
    "    print(metrics, f\"rel: {use_rel}\")\n",
    "    return model\n",
    "\n",
    "clf_rel = run(x_train, x_test, use_rel=True)\n",
    "clf_norel = run(x_train, x_test, use_rel=False)\n",
    "\n",
    "x_full = np.stack((x1, x2, lat)).T\n",
    "clf_full_norel = run(x_full[:n_train], x_full[n_train:], use_rel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7862c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix_with_y_track(matrix, y, title, xlabel, ylabel, \n",
    "                             cmap='viridis',\n",
    "                             square: bool = True,\n",
    "                             ):\n",
    "    gridspec_kw = {\"hspace\": 0.}\n",
    "    if square:\n",
    "        gridspec_kw[\"height_ratios\"] = [10, 1]\n",
    "    else:\n",
    "        gridspec_kw[\"height_ratios\"] = [len(matrix) // 3, 1]\n",
    "    fig, (ax0, ax1) = plt.subplots(\n",
    "        nrows=2, ncols=1,\n",
    "        figsize=(8, 9 if square else len(matrix) // 3 + 1),\n",
    "        gridspec_kw=gridspec_kw,\n",
    "        sharex=True,\n",
    "    )\n",
    "    cbar_ax = fig.add_axes([0.92, 0.2, 0.03, 0.65])\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        ax=ax0,\n",
    "        cmap=cmap,\n",
    "        square=square,\n",
    "        cbar_ax=cbar_ax\n",
    "    )\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_ylabel(ylabel)\n",
    "    n_samples = matrix.shape[1]\n",
    "    ticks = np.arange(0, n_samples, max(1, n_samples // 10))\n",
    "    ax0.set_xticks(ticks)\n",
    "    ax0.set_xticklabels(ticks)\n",
    "    if square:\n",
    "        ax0.set_yticks(ticks)\n",
    "        ax0.set_yticklabels(ticks)\n",
    "    ax0.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "    # Y track\n",
    "    y_bar = y.reshape(1, -1)\n",
    "    sns.heatmap(\n",
    "        y_bar,\n",
    "        ax=ax1,\n",
    "        cmap='viridis',\n",
    "        cbar=False,\n",
    "    )\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72135193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "attention_maps = []\n",
    "q_matrices = []\n",
    "k_matrices = []\n",
    "v_matrices = []\n",
    "\n",
    "def patch_attention_forward(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if not hasattr(module, \"_original_forward\"):\n",
    "                def new_forward(self, s, attn_mask):\n",
    "                    n_samples, d_model = s.x.shape\n",
    "                    q, k, v = (\n",
    "                        (\n",
    "                            proj(s.x)\n",
    "                            .reshape(n_samples, self.num_heads, self.head_dim)\n",
    "                            .transpose(0, 1)\n",
    "                        )\n",
    "                        for proj in (self.q_proj, self.k_proj, self.v_proj)\n",
    "                    )\n",
    "                    attn_scores = (q @ k.transpose(-2, -1)) * self.scaling_factor\n",
    "                    if self.rel:\n",
    "                        attn_scores += s.r.unsqueeze(0) * self.r_scale + self.r_bias\n",
    "                    attn_scores = attn_scores.masked_fill(attn_mask != 0, -torch.inf)\n",
    "                    weights = torch.softmax(attn_scores, dim=-1)\n",
    "                    self._last_attention = weights.detach().cpu()\n",
    "                    self._last_q = q.detach().cpu()\n",
    "                    self._last_k = k.detach().cpu()\n",
    "                    self._last_v = v.detach().cpu()\n",
    "                    weights = self.dropout(weights)\n",
    "                    res = weights @ v\n",
    "                    res = res.transpose(0, 1).flatten(1)\n",
    "                    return self.out_proj(res)\n",
    "                module._original_forward = module.forward\n",
    "                module.forward = new_forward.__get__(module, module.__class__)\n",
    "\n",
    "def register_attention_hooks(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if getattr(module, \"_hook_registered\", False):\n",
    "                continue\n",
    "            def hook(module, input, output):\n",
    "                attention_maps.append(module._last_attention)\n",
    "                q_matrices.append(module._last_q)\n",
    "                k_matrices.append(module._last_k)\n",
    "                v_matrices.append(module._last_v)\n",
    "            module.register_forward_hook(hook)\n",
    "            module._hook_registered = True\n",
    "\n",
    "for title, clf, X, r_inter, r_intra in (\n",
    "    (\"full X\", clf_full_norel, x_full[n_train:], np.zeros_like(r_test_inter), np.zeros_like(r_test_intra)),\n",
    "    (\"X + rel\", clf_rel, x_test, r_test_inter, r_test_intra),\n",
    "):\n",
    "    model = clf.fit_data_.model.eval()\n",
    "    patch_attention_forward(model)\n",
    "    register_attention_hooks(model)\n",
    "    xb = clf.fit_data_.x_train\n",
    "    yb = clf.fit_data_.y_train\n",
    "    xq = torch.tensor(X, dtype=torch.float32)\n",
    "    n_train, n_query = len(xb), len(xq)\n",
    "    r = torch.eye(n_train + n_query)\n",
    "    r[:n_train, :n_train] = clf.fit_data_.r_train\n",
    "    r[n_train:, n_train:] = torch.tensor(r_intra)\n",
    "    r[:n_train, n_train:] = torch.tensor(r_inter)\n",
    "    from tabrel.utils.linalg import mirror_triu\n",
    "    r = mirror_triu(r)\n",
    "    # Run inference\n",
    "    attention_maps.clear()\n",
    "    q_matrices.clear()\n",
    "    k_matrices.clear()\n",
    "    v_matrices.clear()\n",
    "    with torch.no_grad():\n",
    "        _ = model(xb, yb, xq, r)\n",
    "    # Visualize\n",
    "    if attention_maps:\n",
    "        for i_layer, i_head in product(range(len(attention_maps)), range(len(attention_maps[0]))):\n",
    "            attn = attention_maps[i_layer][i_head].numpy()\n",
    "            q = q_matrices[i_layer][i_head].numpy().T  # shape: (head_dim, n_samples)\n",
    "            k = k_matrices[i_layer][i_head].numpy().T\n",
    "            v = v_matrices[i_layer][i_head].numpy().T\n",
    "            y_bar = y\n",
    "            plot_matrix_with_y_track(attn, y_bar, f\"Attention Map ({title}, Head {i_head}, Layer {i_layer})\", \"Key Index\", \"Query Index\")\n",
    "            plot_matrix_with_y_track(q, y_bar, f\"Q Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"Q dim\", square=False)\n",
    "            plot_matrix_with_y_track(k, y_bar, f\"K Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"K dim\", square=False)\n",
    "            plot_matrix_with_y_track(v, y_bar, f\"V Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"V dim\", square=False)\n",
    "    else:\n",
    "        print(\"No attention maps captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da195bcb",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def np_to_geometric(x_: np.ndarray, y_: np.ndarray, r_: np.ndarray) -> Data:\n",
    "    x_tensor = torch.tensor(x_, dtype=torch.float)\n",
    "    y_tensor = torch.tensor(y_, dtype=torch.long)\n",
    "\n",
    "    # Build edge index from r matrix (only where r[i,j] == 1)\n",
    "    edge_index = torch.nonzero(torch.tensor(r_, dtype=torch.long)).T\n",
    "\n",
    "    return Data(x=x_tensor, y=y_tensor, edge_index=edge_index)\n",
    "\n",
    "data = np_to_geometric(x, y, r_numpy)\n",
    "data_train = np_to_geometric(x_train, y_train, r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab254ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GATv2Net(Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads)\n",
    "        self.gat2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "model = GATv2Net(in_channels=2, hidden_channels=8, out_channels=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[train_mask], data.y[train_mask])\n",
    "    # out = model(data_train.x, data_train.edge_index)\n",
    "    # loss = loss_fn(out, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516134f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out[test_mask].argmax(dim=1)\n",
    "    true = data.y[test_mask]\n",
    "\n",
    "# Convert to NumPy\n",
    "y_pred = pred.cpu().numpy()\n",
    "y_true = true.cpu().numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall:    {recall}\")\n",
    "print(f\"Test F1 Score:  {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdd28b",
   "metadata": {},
   "source": [
    "# RelMHANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tabrel.model import RelationalMultiheadAttention, SampleWithRelations\n",
    "\n",
    "class RelMHANet(nn.Module):\n",
    "    def __init__(self, in_dim, embed_dim, num_heads, num_classes, num_layers=2, dropout=0.2, rel=True):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_dim, embed_dim)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            RelationalMultiheadAttention(embed_dim, num_heads, dropout, rel)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, r, attn_mask=None):\n",
    "        if attn_mask is None:\n",
    "            attn_mask = torch.zeros_like(r)\n",
    "        \n",
    "        out = self.input_proj(x)\n",
    "        for attn_layer in self.attn_layers:\n",
    "            s = SampleWithRelations(out, r)\n",
    "            out = attn_layer(s, attn_mask)\n",
    "        \n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "n_train: Final[int] = n // 3 * 2\n",
    "n_backgnd: Final[int] = n // 3\n",
    "\n",
    "# Prepare tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "r_tensor = torch.tensor(r_numpy, dtype=torch.float32)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "backgnd_mask = train_mask.clone()\n",
    "backgnd_mask[n_backgnd:] = False\n",
    "probe_mask = train_mask.clone()\n",
    "probe_mask[:n_backgnd] = False\n",
    "\n",
    "xy_train = torch.cat(\n",
    "    [\n",
    "        x_tensor,\n",
    "        y_tensor.masked_fill(~backgnd_mask, 0).unsqueeze(1)\n",
    "    ], 1)\n",
    "\n",
    "in_dim = xy_train.shape[1]\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = RelMHANet(in_dim=in_dim, embed_dim=embed_dim, num_heads=num_heads, num_classes=num_classes, rel=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xy_train, r_tensor)\n",
    "    loss = loss_fn(logits[probe_mask], y_tensor[probe_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(xy_train, r_tensor)\n",
    "    pred = logits[test_mask].argmax(dim=1)\n",
    "    true = y_tensor[test_mask]\n",
    "\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_true = true.cpu().numpy()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Recall:    {recall}\")\n",
    "    print(f\"Test F1 Score:  {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrel.dataset import QueryUniqueBatchDataset\n",
    "\n",
    "# Split data\n",
    "n_test = n // 9\n",
    "n_backgnd = 2 * n_test\n",
    "n_train = n - n_test\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "r_tensor = torch.tensor(r_numpy, dtype=torch.float32)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "x_train, y_train, r_train = x_tensor[train_mask], y_tensor[train_mask], r_tensor[train_mask][:, train_mask]\n",
    "x_test, y_test = x_tensor[test_mask], y_tensor[test_mask]\n",
    "\n",
    "# QueryUniqueBatchDataset setup\n",
    "query_size = n_test\n",
    "back_size = n_backgnd\n",
    "n_batches = 6\n",
    "random_state = 42\n",
    "\n",
    "train_dataset = QueryUniqueBatchDataset(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    r=r_train,\n",
    "    query_size=query_size,\n",
    "    backgnd_size=back_size,\n",
    "    n_batches=n_batches,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "in_dim = x_train.shape[1] + 1\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "torch.manual_seed(random_state)\n",
    "model = RelMHANet(in_dim=in_dim, embed_dim=embed_dim, num_heads=num_heads, num_classes=num_classes, rel=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb, xq, yq, r in train_dataset:\n",
    "        # Construct input: concatenate xb, yb, xq, zeros_like(yq)\n",
    "        yb_input = yb.unsqueeze(1)\n",
    "        yq_input = torch.zeros_like(yq).unsqueeze(1)\n",
    "        x_input = torch.cat([torch.cat([xb, yb_input], 1), torch.cat([xq, yq_input], 1)], 0)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_input, r)\n",
    "        loss = loss_fn(logits[len(yb):], yq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {total_loss / n_batches:.4f}\")\n",
    "\n",
    "# Inference: use last background and test samples as query\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Use last batch's background indices for inference\n",
    "    # Get background indices from train_dataset\n",
    "    g = torch.Generator().manual_seed(random_state)\n",
    "    perm = torch.randperm(len(x_train), generator=g)\n",
    "    back_idx = perm[query_size * n_batches : query_size * n_batches + back_size]\n",
    "    xb = x_train[back_idx]\n",
    "    yb = y_train[back_idx]\n",
    "    xq = x_test\n",
    "    yq = torch.zeros_like(y_test)\n",
    "    s_idx = torch.cat([torch.arange(len(x_train))[back_idx], torch.arange(len(x_train), len(x_train) + len(x_test))])\n",
    "    # Build r for inference: background + test\n",
    "    r_inf = torch.zeros(len(xb) + len(xq), len(xb) + len(xq))\n",
    "    r_inf[:len(xb), :len(xb)] = r_train[back_idx][:, back_idx]\n",
    "    # No relations between background and test, nor within test\n",
    "    x_input = torch.cat([torch.cat([xb, yb.unsqueeze(1)], 1), torch.cat([xq, yq.unsqueeze(1)], 1)], 0)\n",
    "    logits = model(x_input, r_inf)\n",
    "    pred = logits[len(xb):].argmax(dim=1)\n",
    "    true = y_test\n",
    "\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_true = true.cpu().numpy()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Recall:    {recall}\")\n",
    "    print(f\"Test F1 Score:  {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2830b2d",
   "metadata": {},
   "source": [
    "# RelMHARegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b58015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tabrel.train import train_relnet\n",
    "from tabrel.benchmark.nw_regr import generate_toy_regr_data\n",
    "\n",
    "n_samples = 300\n",
    "n_backgnd = n_samples // 3\n",
    "n_query = n_backgnd\n",
    "n_train = n_backgnd + n_query\n",
    "n_test = n_samples - n_train\n",
    "\n",
    "def train_relnet_shorthand(n_layers_, n_heads_, lr_, seed_):\n",
    "    x_numpy, y_numpy, clusters = generate_toy_regr_data(n_samples=n_samples, n_clusters=3, seed=seed_, distr=\"uniform\", y_func=\"square\")\n",
    "    x_numpy, y_numpy = x_numpy.numpy(), y_numpy.numpy()\n",
    "    r_numpy = (clusters.unsqueeze(1) == clusters.unsqueeze(0)).float().numpy()\n",
    "\n",
    "    return train_relnet(\n",
    "        x=x_numpy,\n",
    "        y=y_numpy,\n",
    "        r=r_numpy,\n",
    "        backgnd_indices=np.arange(0, n_backgnd),\n",
    "        query_indices=np.arange(n_backgnd, n_train),\n",
    "        val_indices=np.arange(n_train, n_samples),\n",
    "        lr=lr_,\n",
    "        n_epochs=1500,\n",
    "        n_layers=n_layers_,\n",
    "        periodic_embed_dim=None,\n",
    "        num_heads=n_heads_,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_layers, n_heads, lr in product(\n",
    "    (1, 2, \n",
    "     # 3 # 3 layers: learns nothing at all - R2 close to zero\n",
    "     ),\n",
    "     (1, 2),\n",
    "    (\n",
    "        # .05, # too high; TODO maybe schedule LR decay?\n",
    "        .02, \n",
    "        .01,\n",
    "        .007,\n",
    "        .005,\n",
    "        .001)\n",
    "):\n",
    "    mse, r2, mae, _, _ = train_relnet_shorthand(\n",
    "        n_layers_=n_layers,\n",
    "        n_heads_=n_heads,\n",
    "        lr_=lr,\n",
    "        seed_=42\n",
    "    )\n",
    "    print(f\"n_layers={n_layers}, lr={lr}, n_heads={n_heads}\")\n",
    "    print(f\"Val MSE: {mse:.4f} | R²: {r2:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "# best for uniform X distr, square y_func:\n",
    "# n_layers=2, lr=0.007, n_heads = 2\n",
    "# Val MSE: 0.0036 | R²: 0.9882 | MAE: 0.0448\n",
    "\n",
    "# best for normal X distr, square y_func:\n",
    "# n_layers=2, lr=0.005 (however, n_layers=1 => avg R2 > 0.95, and converges better with 1 head)\n",
    "# Val MSE: 0.0354 | R²: 0.9813 | MAE: 0.1223\n",
    "\n",
    "# best for normal X distr, abs y_func:\n",
    "# n_layers=2, lr=0.01, n_heads=2\n",
    "# Val MSE: 0.0067 | R²: 0.9941 | MAE: 0.0703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2s = []\n",
    "mses = []\n",
    "for seed in range(30):\n",
    "    mse, r2, _, _, _ = train_relnet_shorthand(\n",
    "        n_layers_=2,\n",
    "        n_heads_=2,\n",
    "        lr_=0.007,\n",
    "        seed_=seed\n",
    "    )\n",
    "    mses.append(mse)\n",
    "    r2s.append(r2)\n",
    "for lbl, vals in (\n",
    "    (\"r2\", r2s),\n",
    "    (\"mse\", mses),\n",
    "):\n",
    "    print(f\"{lbl}: {np.mean(vals):.3f} & {np.std(vals):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29502534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrel.benchmark.nw_regr import NwTrainConfig, NwModelConfig, train_nw\n",
    "\n",
    "model_cfg = NwModelConfig()\n",
    "for lr in (.05, .02, .01, .005, .001):\n",
    "    train_cfg = NwTrainConfig(n_backgnd=n_backgnd, n_query=n_query, n_clusters=3, n_epochs=1000, x_distr=\"norm\", lr=lr, y_func=\"sign\")\n",
    "    reg = train_nw(model_cfg, train_cfg)\n",
    "    print(f\"lr: {lr}\")\n",
    "    print(\"| \".join(f\"{k}: {v:.4f}\" for k, v in reg.evaluate().items()))\n",
    "\n",
    "# best for x_distr norm, y_func square\n",
    "# lr: 0.02\n",
    "# r2: 0.9846| mae: 0.0818| mse: 0.0232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "x = torch.randn(50)\n",
    "y = torch.sign(x)\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
