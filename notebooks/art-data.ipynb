{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n: Final[int] = 900  # number of points\n",
    "\n",
    "# 1. Create three arrays of normally distributed numbers\n",
    "x1: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "x2: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "\n",
    "y: Final[np.ndarray] = np.random.choice(3, n)\n",
    "\n",
    "lat: Final[np.ndarray] = (y - 1) * 4 + np.random.normal(0, .5, n) # latent variable\n",
    "\n",
    "\n",
    "r = np.zeros((n, n))\n",
    "for i, j in product(range(n), range(n)):\n",
    "    r[i, j] = y[i] == y[j]\n",
    "r_numpy = r.copy()\n",
    "\n",
    "x: Final[np.ndarray] = np.stack((x1, x2)).T\n",
    "\n",
    "# n_train: Final[int] = 200\n",
    "# x_train, x_test = x[:n_train], x[n_train:]\n",
    "# y_train, y_test = y[:n_train], y[n_train:]\n",
    "# r_train = r[:n_train][:, :n_train]\n",
    "# r_test_intra = r[n_train:][:, n_train:]\n",
    "# r_test_inter = r[:n_train][:, n_train:]\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add scatter points with colors based on y\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=x1,\n",
    "    y=x2,\n",
    "    z=lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=y,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Data points'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='3D Classification with Decision Boundary',\n",
    "    scene=dict(\n",
    "        xaxis_title='x1',\n",
    "        yaxis_title='x2',\n",
    "        zaxis_title='lat',\n",
    "    ),\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tabrel.sklearn_interface import TabRelClassifier\n",
    "from tabrel.utils.config import ProjectConfig, ClassifierConfig, TrainingConfig\n",
    "\n",
    "def run(X_train: np.ndarray, X_test: np.ndarray, use_rel: bool) -> TabRelClassifier:\n",
    "    config = ProjectConfig(\n",
    "        training=TrainingConfig(\n",
    "            backgnd_size=100,\n",
    "            query_size=25,\n",
    "            n_batches=4,\n",
    "            lr=1e-4,\n",
    "            n_epochs=50,\n",
    "            log_dir=Path(\"out/logs\"),\n",
    "            log_level=logging.DEBUG,\n",
    "            print_logs_to_console=True,\n",
    "            checkpoints_dir=Path(\"out/checkpoints\"),\n",
    "            allow_dirs_exist=True,\n",
    "            random_seed=42,\n",
    "        ),\n",
    "        model=ClassifierConfig(\n",
    "            n_features=X_train.shape[1],\n",
    "            d_embedding=20,\n",
    "            d_model=8,\n",
    "            nhead=1,\n",
    "            dim_feedforward=1,\n",
    "            num_layers=1,\n",
    "            num_classes=len(np.unique(y)),\n",
    "            activation=\"relu\",\n",
    "            rel=use_rel,\n",
    "            dropout=0.,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = TabRelClassifier(config)\n",
    "    model.fit(X=X_train, y=y_train, r=r_train)\n",
    "    metrics = model.evaluate(X=X_test, r_inter=r_test_inter, r_intra=r_test_intra, y=y_test)\n",
    "    print(metrics, f\"rel: {use_rel}\")\n",
    "    return model\n",
    "\n",
    "clf_rel = run(x_train, x_test, use_rel=True)\n",
    "clf_norel = run(x_train, x_test, use_rel=False)\n",
    "\n",
    "x_full = np.stack((x1, x2, lat)).T\n",
    "clf_full_norel = run(x_full[:n_train], x_full[n_train:], use_rel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7862c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix_with_y_track(matrix, y, title, xlabel, ylabel, \n",
    "                             cmap='viridis',\n",
    "                             square: bool = True,\n",
    "                             ):\n",
    "    gridspec_kw = {\"hspace\": 0.}\n",
    "    if square:\n",
    "        gridspec_kw[\"height_ratios\"] = [10, 1]\n",
    "    else:\n",
    "        gridspec_kw[\"height_ratios\"] = [len(matrix) // 3, 1]\n",
    "    fig, (ax0, ax1) = plt.subplots(\n",
    "        nrows=2, ncols=1,\n",
    "        figsize=(8, 9 if square else len(matrix) // 3 + 1),\n",
    "        gridspec_kw=gridspec_kw,\n",
    "        sharex=True,\n",
    "    )\n",
    "    cbar_ax = fig.add_axes([0.92, 0.2, 0.03, 0.65])\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        ax=ax0,\n",
    "        cmap=cmap,\n",
    "        square=square,\n",
    "        cbar_ax=cbar_ax\n",
    "    )\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_ylabel(ylabel)\n",
    "    n_samples = matrix.shape[1]\n",
    "    ticks = np.arange(0, n_samples, max(1, n_samples // 10))\n",
    "    ax0.set_xticks(ticks)\n",
    "    ax0.set_xticklabels(ticks)\n",
    "    if square:\n",
    "        ax0.set_yticks(ticks)\n",
    "        ax0.set_yticklabels(ticks)\n",
    "    ax0.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "    # Y track\n",
    "    y_bar = y.reshape(1, -1)\n",
    "    sns.heatmap(\n",
    "        y_bar,\n",
    "        ax=ax1,\n",
    "        cmap='viridis',\n",
    "        cbar=False,\n",
    "    )\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72135193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "attention_maps = []\n",
    "q_matrices = []\n",
    "k_matrices = []\n",
    "v_matrices = []\n",
    "\n",
    "def patch_attention_forward(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if not hasattr(module, \"_original_forward\"):\n",
    "                def new_forward(self, s, attn_mask):\n",
    "                    n_samples, d_model = s.x.shape\n",
    "                    q, k, v = (\n",
    "                        (\n",
    "                            proj(s.x)\n",
    "                            .reshape(n_samples, self.num_heads, self.head_dim)\n",
    "                            .transpose(0, 1)\n",
    "                        )\n",
    "                        for proj in (self.q_proj, self.k_proj, self.v_proj)\n",
    "                    )\n",
    "                    attn_scores = (q @ k.transpose(-2, -1)) * self.scaling_factor\n",
    "                    if self.rel:\n",
    "                        attn_scores += s.r.unsqueeze(0) * self.r_scale + self.r_bias\n",
    "                    attn_scores = attn_scores.masked_fill(attn_mask != 0, -torch.inf)\n",
    "                    weights = torch.softmax(attn_scores, dim=-1)\n",
    "                    self._last_attention = weights.detach().cpu()\n",
    "                    self._last_q = q.detach().cpu()\n",
    "                    self._last_k = k.detach().cpu()\n",
    "                    self._last_v = v.detach().cpu()\n",
    "                    weights = self.dropout(weights)\n",
    "                    res = weights @ v\n",
    "                    res = res.transpose(0, 1).flatten(1)\n",
    "                    return self.out_proj(res)\n",
    "                module._original_forward = module.forward\n",
    "                module.forward = new_forward.__get__(module, module.__class__)\n",
    "\n",
    "def register_attention_hooks(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if getattr(module, \"_hook_registered\", False):\n",
    "                continue\n",
    "            def hook(module, input, output):\n",
    "                attention_maps.append(module._last_attention)\n",
    "                q_matrices.append(module._last_q)\n",
    "                k_matrices.append(module._last_k)\n",
    "                v_matrices.append(module._last_v)\n",
    "            module.register_forward_hook(hook)\n",
    "            module._hook_registered = True\n",
    "\n",
    "for title, clf, X, r_inter, r_intra in (\n",
    "    (\"full X\", clf_full_norel, x_full[n_train:], np.zeros_like(r_test_inter), np.zeros_like(r_test_intra)),\n",
    "    (\"X + rel\", clf_rel, x_test, r_test_inter, r_test_intra),\n",
    "):\n",
    "    model = clf.fit_data_.model.eval()\n",
    "    patch_attention_forward(model)\n",
    "    register_attention_hooks(model)\n",
    "    xb = clf.fit_data_.x_train\n",
    "    yb = clf.fit_data_.y_train\n",
    "    xq = torch.tensor(X, dtype=torch.float32)\n",
    "    n_train, n_query = len(xb), len(xq)\n",
    "    r = torch.eye(n_train + n_query)\n",
    "    r[:n_train, :n_train] = clf.fit_data_.r_train\n",
    "    r[n_train:, n_train:] = torch.tensor(r_intra)\n",
    "    r[:n_train, n_train:] = torch.tensor(r_inter)\n",
    "    from tabrel.utils.linalg import mirror_triu\n",
    "    r = mirror_triu(r)\n",
    "    # Run inference\n",
    "    attention_maps.clear()\n",
    "    q_matrices.clear()\n",
    "    k_matrices.clear()\n",
    "    v_matrices.clear()\n",
    "    with torch.no_grad():\n",
    "        _ = model(xb, yb, xq, r)\n",
    "    # Visualize\n",
    "    if attention_maps:\n",
    "        for i_layer, i_head in product(range(len(attention_maps)), range(len(attention_maps[0]))):\n",
    "            attn = attention_maps[i_layer][i_head].numpy()\n",
    "            q = q_matrices[i_layer][i_head].numpy().T  # shape: (head_dim, n_samples)\n",
    "            k = k_matrices[i_layer][i_head].numpy().T\n",
    "            v = v_matrices[i_layer][i_head].numpy().T\n",
    "            y_bar = y\n",
    "            plot_matrix_with_y_track(attn, y_bar, f\"Attention Map ({title}, Head {i_head}, Layer {i_layer})\", \"Key Index\", \"Query Index\")\n",
    "            plot_matrix_with_y_track(q, y_bar, f\"Q Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"Q dim\", square=False)\n",
    "            plot_matrix_with_y_track(k, y_bar, f\"K Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"K dim\", square=False)\n",
    "            plot_matrix_with_y_track(v, y_bar, f\"V Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"V dim\", square=False)\n",
    "    else:\n",
    "        print(\"No attention maps captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da195bcb",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def np_to_geometric(x_: np.ndarray, y_: np.ndarray, r_: np.ndarray) -> Data:\n",
    "    x_tensor = torch.tensor(x_, dtype=torch.float)\n",
    "    y_tensor = torch.tensor(y_, dtype=torch.long)\n",
    "\n",
    "    # Build edge index from r matrix (only where r[i,j] == 1)\n",
    "    edge_index = torch.nonzero(torch.tensor(r_, dtype=torch.long)).T\n",
    "\n",
    "    return Data(x=x_tensor, y=y_tensor, edge_index=edge_index)\n",
    "\n",
    "data = np_to_geometric(x, y, r_numpy)\n",
    "data_train = np_to_geometric(x_train, y_train, r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04369b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nonzero(torch.tensor(r_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab254ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GATv2Net(Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads)\n",
    "        self.gat2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "model = GATv2Net(in_channels=2, hidden_channels=8, out_channels=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[train_mask], data.y[train_mask])\n",
    "    # out = model(data_train.x, data_train.edge_index)\n",
    "    # loss = loss_fn(out, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516134f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out[test_mask].argmax(dim=1)\n",
    "    true = data.y[test_mask]\n",
    "\n",
    "# Convert to NumPy\n",
    "y_pred = pred.cpu().numpy()\n",
    "y_true = true.cpu().numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall:    {recall}\")\n",
    "print(f\"Test F1 Score:  {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdd28b",
   "metadata": {},
   "source": [
    "# RelMHANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tabrel.model import RelationalMultiheadAttention, SampleWithRelations\n",
    "\n",
    "class RelMHANet(nn.Module):\n",
    "    def __init__(self, in_dim, embed_dim, num_heads, num_classes, num_layers=2, dropout=0.2, rel=True):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_dim, embed_dim)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            RelationalMultiheadAttention(embed_dim, num_heads, dropout, rel)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, r, attn_mask=None):\n",
    "        if attn_mask is None:\n",
    "            attn_mask = torch.zeros_like(r)\n",
    "        \n",
    "        out = self.input_proj(x)\n",
    "        for attn_layer in self.attn_layers:\n",
    "            s = SampleWithRelations(out, r)\n",
    "            out = attn_layer(s, attn_mask)\n",
    "        \n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "n_train: Final[int] = n // 3 * 2\n",
    "n_backgnd: Final[int] = n // 3\n",
    "\n",
    "# Prepare tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "r_tensor = torch.tensor(r_numpy, dtype=torch.float32)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "backgnd_mask = train_mask.clone()\n",
    "backgnd_mask[n_backgnd:] = False\n",
    "probe_mask = train_mask.clone()\n",
    "probe_mask[:n_backgnd] = False\n",
    "\n",
    "xy_train = torch.cat(\n",
    "    [\n",
    "        x_tensor,\n",
    "        y_tensor.masked_fill(~backgnd_mask, 0).unsqueeze(1)\n",
    "    ], 1)\n",
    "\n",
    "in_dim = xy_train.shape[1]\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = RelMHANet(in_dim=in_dim, embed_dim=embed_dim, num_heads=num_heads, num_classes=num_classes, rel=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xy_train, r_tensor)\n",
    "    loss = loss_fn(logits[probe_mask], y_tensor[probe_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(xy_train, r_tensor)\n",
    "    pred = logits[test_mask].argmax(dim=1)\n",
    "    true = y_tensor[test_mask]\n",
    "\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_true = true.cpu().numpy()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Recall:    {recall}\")\n",
    "    print(f\"Test F1 Score:  {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
