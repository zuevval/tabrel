{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n: Final[int] = 70  # number of points\n",
    "\n",
    "# 1. Create three arrays of normally distributed numbers\n",
    "x1: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "x2: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "\n",
    "y: Final[np.ndarray] = np.random.choice(3, n)\n",
    "\n",
    "lat: Final[np.ndarray] = (y - 1) * 4 + np.random.normal(0, .5, n) # latent variable\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add scatter points with colors based on y\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=x1,\n",
    "    y=x2,\n",
    "    z=lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=y,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Data points'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='3D Classification with Decision Boundary',\n",
    "    scene=dict(\n",
    "        xaxis_title='x1',\n",
    "        yaxis_title='x2',\n",
    "        zaxis_title='lat',\n",
    "    ),\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afb2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "r = np.zeros((n, n))\n",
    "for i, j in product(range(n), range(n)):\n",
    "    r[i, j] = y[i] == y[j]\n",
    "\n",
    "x: Final[np.ndarray] = np.stack((x1, x2)).T\n",
    "\n",
    "n_train: Final[int] = 50\n",
    "x_train, x_test = x[:n_train], x[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "r_train = r[:n_train][:, :n_train]\n",
    "r_test_intra = r[n_train:][:, n_train:]\n",
    "r_test_inter = r[:n_train][:, n_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tabrel.sklearn_interface import TabRelClassifier\n",
    "from tabrel.utils.config import ProjectConfig, ClassifierConfig, TrainingConfig\n",
    "\n",
    "def run(X_train: np.ndarray, X_test: np.ndarray, use_rel: bool) -> TabRelClassifier:\n",
    "    config = ProjectConfig(\n",
    "        training=TrainingConfig(\n",
    "            batch_size=20,\n",
    "            query_size=10,\n",
    "            n_batches=3,\n",
    "            lr=1e-4,\n",
    "            n_epochs=50,\n",
    "            log_dir=Path(\"out/logs\"),\n",
    "            log_level=logging.DEBUG,\n",
    "            print_logs_to_console=True,\n",
    "            checkpoints_dir=Path(\"out/checkpoints\"),\n",
    "            allow_dirs_exist=True,\n",
    "            random_seed=42,\n",
    "        ),\n",
    "        model=ClassifierConfig(\n",
    "            n_features=X_train.shape[1],\n",
    "            d_embedding=20,\n",
    "            d_model=8,\n",
    "            nhead=1,\n",
    "            dim_feedforward=1,\n",
    "            num_layers=1,\n",
    "            num_classes=len(np.unique(y)),\n",
    "            activation=\"relu\",\n",
    "            rel=use_rel,\n",
    "            dropout=0.,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = TabRelClassifier(config)\n",
    "    model.fit(X=X_train, y=y_train, r=r_train)\n",
    "    metrics = model.evaluate(X=X_test, r_inter=r_test_inter, r_intra=r_test_intra, y=y_test)\n",
    "    print(metrics, f\"rel: {use_rel}\")\n",
    "    return model\n",
    "\n",
    "clf_rel = run(x_train, x_test, use_rel=True)\n",
    "clf_norel = run(x_train, x_test, use_rel=False)\n",
    "\n",
    "x_full = np.stack((x1, x2, lat)).T\n",
    "clf_full_norel = run(x_full[:n_train], x_full[n_train:], use_rel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7862c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix_with_y_track(matrix, y, title, xlabel, ylabel, \n",
    "                             cmap='viridis',\n",
    "                             square: bool = True,\n",
    "                             ):\n",
    "    gridspec_kw = {\"hspace\": 0.}\n",
    "    if square:\n",
    "        gridspec_kw[\"height_ratios\"] = [10, 1]\n",
    "    else:\n",
    "        gridspec_kw[\"height_ratios\"] = [len(matrix) // 3, 1]\n",
    "    fig, (ax0, ax1) = plt.subplots(\n",
    "        nrows=2, ncols=1,\n",
    "        figsize=(8, 9 if square else len(matrix) // 3 + 1),\n",
    "        gridspec_kw=gridspec_kw,\n",
    "        sharex=True,\n",
    "    )\n",
    "    cbar_ax = fig.add_axes([0.92, 0.2, 0.03, 0.65])\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        ax=ax0,\n",
    "        cmap=cmap,\n",
    "        square=square,\n",
    "        cbar_ax=cbar_ax\n",
    "    )\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_ylabel(ylabel)\n",
    "    n_samples = matrix.shape[1]\n",
    "    ticks = np.arange(0, n_samples, max(1, n_samples // 10))\n",
    "    ax0.set_xticks(ticks)\n",
    "    ax0.set_xticklabels(ticks)\n",
    "    if square:\n",
    "        ax0.set_yticks(ticks)\n",
    "        ax0.set_yticklabels(ticks)\n",
    "    ax0.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "    # Y track\n",
    "    y_bar = y.reshape(1, -1)\n",
    "    sns.heatmap(\n",
    "        y_bar,\n",
    "        ax=ax1,\n",
    "        cmap='viridis',\n",
    "        cbar=False,\n",
    "    )\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72135193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "attention_maps = []\n",
    "q_matrices = []\n",
    "k_matrices = []\n",
    "v_matrices = []\n",
    "\n",
    "def patch_attention_forward(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if not hasattr(module, \"_original_forward\"):\n",
    "                def new_forward(self, s, attn_mask):\n",
    "                    n_samples, d_model = s.x.shape\n",
    "                    q, k, v = (\n",
    "                        (\n",
    "                            proj(s.x)\n",
    "                            .reshape(n_samples, self.num_heads, self.head_dim)\n",
    "                            .transpose(0, 1)\n",
    "                        )\n",
    "                        for proj in (self.q_proj, self.k_proj, self.v_proj)\n",
    "                    )\n",
    "                    attn_scores = (q @ k.transpose(-2, -1)) * self.scaling_factor\n",
    "                    if self.rel:\n",
    "                        attn_scores += s.r.unsqueeze(0) * self.r_scale + self.r_bias\n",
    "                    attn_scores = attn_scores.masked_fill(attn_mask != 0, -torch.inf)\n",
    "                    weights = torch.softmax(attn_scores, dim=-1)\n",
    "                    self._last_attention = weights.detach().cpu()\n",
    "                    self._last_q = q.detach().cpu()\n",
    "                    self._last_k = k.detach().cpu()\n",
    "                    self._last_v = v.detach().cpu()\n",
    "                    weights = self.dropout(weights)\n",
    "                    res = weights @ v\n",
    "                    res = res.transpose(0, 1).flatten(1)\n",
    "                    return self.out_proj(res)\n",
    "                module._original_forward = module.forward\n",
    "                module.forward = new_forward.__get__(module, module.__class__)\n",
    "\n",
    "def register_attention_hooks(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if getattr(module, \"_hook_registered\", False):\n",
    "                continue\n",
    "            def hook(module, input, output):\n",
    "                attention_maps.append(module._last_attention)\n",
    "                q_matrices.append(module._last_q)\n",
    "                k_matrices.append(module._last_k)\n",
    "                v_matrices.append(module._last_v)\n",
    "            module.register_forward_hook(hook)\n",
    "            module._hook_registered = True\n",
    "\n",
    "for title, clf, X, r_inter, r_intra in (\n",
    "    (\"full X\", clf_full_norel, x_full[n_train:], np.zeros_like(r_test_inter), np.zeros_like(r_test_intra)),\n",
    "    (\"X + rel\", clf_rel, x_test, r_test_inter, r_test_intra),\n",
    "):\n",
    "    model = clf.fit_data_.model.eval()\n",
    "    patch_attention_forward(model)\n",
    "    register_attention_hooks(model)\n",
    "    xb = clf.fit_data_.x_train\n",
    "    yb = clf.fit_data_.y_train\n",
    "    xq = torch.tensor(X, dtype=torch.float32)\n",
    "    n_train, n_query = len(xb), len(xq)\n",
    "    r = torch.eye(n_train + n_query)\n",
    "    r[:n_train, :n_train] = clf.fit_data_.r_train\n",
    "    r[n_train:, n_train:] = torch.tensor(r_intra)\n",
    "    r[:n_train, n_train:] = torch.tensor(r_inter)\n",
    "    from tabrel.utils.linalg import mirror_triu\n",
    "    r = mirror_triu(r)\n",
    "    # Run inference\n",
    "    attention_maps.clear()\n",
    "    q_matrices.clear()\n",
    "    k_matrices.clear()\n",
    "    v_matrices.clear()\n",
    "    with torch.no_grad():\n",
    "        _ = model(xb, yb, xq, r)\n",
    "    # Visualize\n",
    "    if attention_maps:\n",
    "        for i_layer, i_head in product(range(len(attention_maps)), range(len(attention_maps[0]))):\n",
    "            attn = attention_maps[i_layer][i_head].numpy()\n",
    "            q = q_matrices[i_layer][i_head].numpy().T  # shape: (head_dim, n_samples)\n",
    "            k = k_matrices[i_layer][i_head].numpy().T\n",
    "            v = v_matrices[i_layer][i_head].numpy().T\n",
    "            y_bar = y\n",
    "            plot_matrix_with_y_track(attn, y_bar, f\"Attention Map ({title}, Head {i_head}, Layer {i_layer})\", \"Key Index\", \"Query Index\")\n",
    "            plot_matrix_with_y_track(q, y_bar, f\"Q Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"Q dim\", square=False)\n",
    "            plot_matrix_with_y_track(k, y_bar, f\"K Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"K dim\", square=False)\n",
    "            plot_matrix_with_y_track(v, y_bar, f\"V Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"V dim\", square=False)\n",
    "    else:\n",
    "        print(\"No attention maps captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
