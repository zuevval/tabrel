{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n: Final[int] = 900  # number of points\n",
    "\n",
    "# 1. Create three arrays of normally distributed numbers\n",
    "x1: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "x2: Final[np.ndarray] = np.random.normal(0, 1, n)\n",
    "\n",
    "y: Final[np.ndarray] = np.random.choice(3, n)\n",
    "\n",
    "lat: Final[np.ndarray] = (y - 1) * 4 + np.random.normal(0, .5, n) # latent variable\n",
    "\n",
    "\n",
    "r = np.zeros((n, n))\n",
    "for i, j in product(range(n), range(n)):\n",
    "    r[i, j] = y[i] == y[j]\n",
    "r_numpy = r.copy()\n",
    "\n",
    "x: Final[np.ndarray] = np.stack((x1, x2)).T\n",
    "\n",
    "n_train: Final[int] = 200\n",
    "x_train, x_test = x[:n_train], x[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "r_train = r[:n_train][:, :n_train]\n",
    "r_test_intra = r[n_train:][:, n_train:]\n",
    "r_test_inter = r[:n_train][:, n_train:]\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add scatter points with colors based on y\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=x1,\n",
    "    y=x2,\n",
    "    z=lat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=y,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Data points'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='3D Classification with Decision Boundary',\n",
    "    scene=dict(\n",
    "        xaxis_title='x1',\n",
    "        yaxis_title='x2',\n",
    "        zaxis_title='lat',\n",
    "    ),\n",
    "    width=800,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tabrel.sklearn_interface import TabRelClassifier\n",
    "from tabrel.utils.config import ProjectConfig, ClassifierConfig, TrainingConfig\n",
    "\n",
    "def run(X_train: np.ndarray, X_test: np.ndarray, use_rel: bool) -> TabRelClassifier:\n",
    "    config = ProjectConfig(\n",
    "        training=TrainingConfig(\n",
    "            backgnd_size=100,\n",
    "            query_size=25,\n",
    "            n_batches=4,\n",
    "            lr=1e-4,\n",
    "            n_epochs=50,\n",
    "            log_dir=Path(\"out/logs\"),\n",
    "            log_level=logging.DEBUG,\n",
    "            print_logs_to_console=True,\n",
    "            checkpoints_dir=Path(\"out/checkpoints\"),\n",
    "            allow_dirs_exist=True,\n",
    "            random_seed=42,\n",
    "        ),\n",
    "        model=ClassifierConfig(\n",
    "            n_features=X_train.shape[1],\n",
    "            d_embedding=20,\n",
    "            d_model=8,\n",
    "            nhead=1,\n",
    "            dim_feedforward=1,\n",
    "            num_layers=1,\n",
    "            num_classes=len(np.unique(y)),\n",
    "            activation=\"relu\",\n",
    "            rel=use_rel,\n",
    "            dropout=0.,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model = TabRelClassifier(config)\n",
    "    model.fit(X=X_train, y=y_train, r=r_train)\n",
    "    metrics = model.evaluate(X=X_test, r_inter=r_test_inter, r_intra=r_test_intra, y=y_test)\n",
    "    print(metrics, f\"rel: {use_rel}\")\n",
    "    return model\n",
    "\n",
    "clf_rel = run(x_train, x_test, use_rel=True)\n",
    "clf_norel = run(x_train, x_test, use_rel=False)\n",
    "\n",
    "x_full = np.stack((x1, x2, lat)).T\n",
    "clf_full_norel = run(x_full[:n_train], x_full[n_train:], use_rel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7862c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_matrix_with_y_track(matrix, y, title, xlabel, ylabel, \n",
    "                             cmap='viridis',\n",
    "                             square: bool = True,\n",
    "                             ):\n",
    "    gridspec_kw = {\"hspace\": 0.}\n",
    "    if square:\n",
    "        gridspec_kw[\"height_ratios\"] = [10, 1]\n",
    "    else:\n",
    "        gridspec_kw[\"height_ratios\"] = [len(matrix) // 3, 1]\n",
    "    fig, (ax0, ax1) = plt.subplots(\n",
    "        nrows=2, ncols=1,\n",
    "        figsize=(8, 9 if square else len(matrix) // 3 + 1),\n",
    "        gridspec_kw=gridspec_kw,\n",
    "        sharex=True,\n",
    "    )\n",
    "    cbar_ax = fig.add_axes([0.92, 0.2, 0.03, 0.65])\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        ax=ax0,\n",
    "        cmap=cmap,\n",
    "        square=square,\n",
    "        cbar_ax=cbar_ax\n",
    "    )\n",
    "    ax0.set_title(title)\n",
    "    ax0.set_ylabel(ylabel)\n",
    "    n_samples = matrix.shape[1]\n",
    "    ticks = np.arange(0, n_samples, max(1, n_samples // 10))\n",
    "    ax0.set_xticks(ticks)\n",
    "    ax0.set_xticklabels(ticks)\n",
    "    if square:\n",
    "        ax0.set_yticks(ticks)\n",
    "        ax0.set_yticklabels(ticks)\n",
    "    ax0.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "    # Y track\n",
    "    y_bar = y.reshape(1, -1)\n",
    "    sns.heatmap(\n",
    "        y_bar,\n",
    "        ax=ax1,\n",
    "        cmap='viridis',\n",
    "        cbar=False,\n",
    "    )\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72135193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "attention_maps = []\n",
    "q_matrices = []\n",
    "k_matrices = []\n",
    "v_matrices = []\n",
    "\n",
    "def patch_attention_forward(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if not hasattr(module, \"_original_forward\"):\n",
    "                def new_forward(self, s, attn_mask):\n",
    "                    n_samples, d_model = s.x.shape\n",
    "                    q, k, v = (\n",
    "                        (\n",
    "                            proj(s.x)\n",
    "                            .reshape(n_samples, self.num_heads, self.head_dim)\n",
    "                            .transpose(0, 1)\n",
    "                        )\n",
    "                        for proj in (self.q_proj, self.k_proj, self.v_proj)\n",
    "                    )\n",
    "                    attn_scores = (q @ k.transpose(-2, -1)) * self.scaling_factor\n",
    "                    if self.rel:\n",
    "                        attn_scores += s.r.unsqueeze(0) * self.r_scale + self.r_bias\n",
    "                    attn_scores = attn_scores.masked_fill(attn_mask != 0, -torch.inf)\n",
    "                    weights = torch.softmax(attn_scores, dim=-1)\n",
    "                    self._last_attention = weights.detach().cpu()\n",
    "                    self._last_q = q.detach().cpu()\n",
    "                    self._last_k = k.detach().cpu()\n",
    "                    self._last_v = v.detach().cpu()\n",
    "                    weights = self.dropout(weights)\n",
    "                    res = weights @ v\n",
    "                    res = res.transpose(0, 1).flatten(1)\n",
    "                    return self.out_proj(res)\n",
    "                module._original_forward = module.forward\n",
    "                module.forward = new_forward.__get__(module, module.__class__)\n",
    "\n",
    "def register_attention_hooks(model):\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__ == \"RelationalMultiheadAttention\":\n",
    "            if getattr(module, \"_hook_registered\", False):\n",
    "                continue\n",
    "            def hook(module, input, output):\n",
    "                attention_maps.append(module._last_attention)\n",
    "                q_matrices.append(module._last_q)\n",
    "                k_matrices.append(module._last_k)\n",
    "                v_matrices.append(module._last_v)\n",
    "            module.register_forward_hook(hook)\n",
    "            module._hook_registered = True\n",
    "\n",
    "for title, clf, X, r_inter, r_intra in (\n",
    "    (\"full X\", clf_full_norel, x_full[n_train:], np.zeros_like(r_test_inter), np.zeros_like(r_test_intra)),\n",
    "    (\"X + rel\", clf_rel, x_test, r_test_inter, r_test_intra),\n",
    "):\n",
    "    model = clf.fit_data_.model.eval()\n",
    "    patch_attention_forward(model)\n",
    "    register_attention_hooks(model)\n",
    "    xb = clf.fit_data_.x_train\n",
    "    yb = clf.fit_data_.y_train\n",
    "    xq = torch.tensor(X, dtype=torch.float32)\n",
    "    n_train, n_query = len(xb), len(xq)\n",
    "    r = torch.eye(n_train + n_query)\n",
    "    r[:n_train, :n_train] = clf.fit_data_.r_train\n",
    "    r[n_train:, n_train:] = torch.tensor(r_intra)\n",
    "    r[:n_train, n_train:] = torch.tensor(r_inter)\n",
    "    from tabrel.utils.linalg import mirror_triu\n",
    "    r = mirror_triu(r)\n",
    "    # Run inference\n",
    "    attention_maps.clear()\n",
    "    q_matrices.clear()\n",
    "    k_matrices.clear()\n",
    "    v_matrices.clear()\n",
    "    with torch.no_grad():\n",
    "        _ = model(xb, yb, xq, r)\n",
    "    # Visualize\n",
    "    if attention_maps:\n",
    "        for i_layer, i_head in product(range(len(attention_maps)), range(len(attention_maps[0]))):\n",
    "            attn = attention_maps[i_layer][i_head].numpy()\n",
    "            q = q_matrices[i_layer][i_head].numpy().T  # shape: (head_dim, n_samples)\n",
    "            k = k_matrices[i_layer][i_head].numpy().T\n",
    "            v = v_matrices[i_layer][i_head].numpy().T\n",
    "            y_bar = y\n",
    "            plot_matrix_with_y_track(attn, y_bar, f\"Attention Map ({title}, Head {i_head}, Layer {i_layer})\", \"Key Index\", \"Query Index\")\n",
    "            plot_matrix_with_y_track(q, y_bar, f\"Q Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"Q dim\", square=False)\n",
    "            plot_matrix_with_y_track(k, y_bar, f\"K Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"K dim\", square=False)\n",
    "            plot_matrix_with_y_track(v, y_bar, f\"V Matrix ({title}, Head {i_head}, Layer {i_layer})\", \"Sample Index\", \"V dim\", square=False)\n",
    "    else:\n",
    "        print(\"No attention maps captured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da195bcb",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def np_to_geometric(x_: np.ndarray, y_: np.ndarray, r_: np.ndarray) -> Data:\n",
    "    x_tensor = torch.tensor(x_, dtype=torch.float)\n",
    "    y_tensor = torch.tensor(y_, dtype=torch.long)\n",
    "\n",
    "    # Build edge index from r matrix (only where r[i,j] == 1)\n",
    "    edge_index = torch.nonzero(torch.tensor(r_, dtype=torch.long)).T\n",
    "\n",
    "    return Data(x=x_tensor, y=y_tensor, edge_index=edge_index)\n",
    "\n",
    "data = np_to_geometric(x, y, r_numpy)\n",
    "data_train = np_to_geometric(x_train, y_train, r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab254ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GATv2Net(Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads)\n",
    "        self.gat2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "model = GATv2Net(in_channels=2, hidden_channels=8, out_channels=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[train_mask], data.y[train_mask])\n",
    "    # out = model(data_train.x, data_train.edge_index)\n",
    "    # loss = loss_fn(out, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516134f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out[test_mask].argmax(dim=1)\n",
    "    true = data.y[test_mask]\n",
    "\n",
    "# Convert to NumPy\n",
    "y_pred = pred.cpu().numpy()\n",
    "y_true = true.cpu().numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall:    {recall}\")\n",
    "print(f\"Test F1 Score:  {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdd28b",
   "metadata": {},
   "source": [
    "# RelMHANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tabrel.model import RelationalMultiheadAttention, SampleWithRelations\n",
    "\n",
    "class RelMHANet(nn.Module):\n",
    "    def __init__(self, in_dim, embed_dim, num_heads, num_classes, num_layers=2, dropout=0.2, rel=True):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_dim, embed_dim)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            RelationalMultiheadAttention(embed_dim, num_heads, dropout, rel)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, r, attn_mask=None):\n",
    "        if attn_mask is None:\n",
    "            attn_mask = torch.zeros_like(r)\n",
    "        \n",
    "        out = self.input_proj(x)\n",
    "        for attn_layer in self.attn_layers:\n",
    "            s = SampleWithRelations(out, r)\n",
    "            out = attn_layer(s, attn_mask)\n",
    "        \n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "n_train: Final[int] = n // 3 * 2\n",
    "n_backgnd: Final[int] = n // 3\n",
    "\n",
    "# Prepare tensors\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "r_tensor = torch.tensor(r_numpy, dtype=torch.float32)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "backgnd_mask = train_mask.clone()\n",
    "backgnd_mask[n_backgnd:] = False\n",
    "probe_mask = train_mask.clone()\n",
    "probe_mask[:n_backgnd] = False\n",
    "\n",
    "xy_train = torch.cat(\n",
    "    [\n",
    "        x_tensor,\n",
    "        y_tensor.masked_fill(~backgnd_mask, 0).unsqueeze(1)\n",
    "    ], 1)\n",
    "\n",
    "in_dim = xy_train.shape[1]\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = RelMHANet(in_dim=in_dim, embed_dim=embed_dim, num_heads=num_heads, num_classes=num_classes, rel=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xy_train, r_tensor)\n",
    "    loss = loss_fn(logits[probe_mask], y_tensor[probe_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(xy_train, r_tensor)\n",
    "    pred = logits[test_mask].argmax(dim=1)\n",
    "    true = y_tensor[test_mask]\n",
    "\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_true = true.cpu().numpy()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Recall:    {recall}\")\n",
    "    print(f\"Test F1 Score:  {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrel.dataset import QueryUniqueBatchDataset\n",
    "\n",
    "# Split data\n",
    "n_test = n // 9\n",
    "n_backgnd = 2 * n_test\n",
    "n_train = n - n_test\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "r_tensor = torch.tensor(r_numpy, dtype=torch.float32)\n",
    "\n",
    "train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "train_mask[:n_train] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "x_train, y_train, r_train = x_tensor[train_mask], y_tensor[train_mask], r_tensor[train_mask][:, train_mask]\n",
    "x_test, y_test = x_tensor[test_mask], y_tensor[test_mask]\n",
    "\n",
    "# QueryUniqueBatchDataset setup\n",
    "query_size = n_test\n",
    "back_size = n_backgnd\n",
    "n_batches = 6\n",
    "random_state = 42\n",
    "\n",
    "train_dataset = QueryUniqueBatchDataset(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    r=r_train,\n",
    "    query_size=query_size,\n",
    "    backgnd_size=back_size,\n",
    "    n_batches=n_batches,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "in_dim = x_train.shape[1] + 1\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "torch.manual_seed(random_state)\n",
    "model = RelMHANet(in_dim=in_dim, embed_dim=embed_dim, num_heads=num_heads, num_classes=num_classes, rel=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb, xq, yq, r in train_dataset:\n",
    "        # Construct input: concatenate xb, yb, xq, zeros_like(yq)\n",
    "        yb_input = yb.unsqueeze(1)\n",
    "        yq_input = torch.zeros_like(yq).unsqueeze(1)\n",
    "        x_input = torch.cat([torch.cat([xb, yb_input], 1), torch.cat([xq, yq_input], 1)], 0)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_input, r)\n",
    "        loss = loss_fn(logits[len(yb):], yq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch} - Loss: {total_loss / n_batches:.4f}\")\n",
    "\n",
    "# Inference: use last background and test samples as query\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Use last batch's background indices for inference\n",
    "    # Get background indices from train_dataset\n",
    "    g = torch.Generator().manual_seed(random_state)\n",
    "    perm = torch.randperm(len(x_train), generator=g)\n",
    "    back_idx = perm[query_size * n_batches : query_size * n_batches + back_size]\n",
    "    xb = x_train[back_idx]\n",
    "    yb = y_train[back_idx]\n",
    "    xq = x_test\n",
    "    yq = torch.zeros_like(y_test)\n",
    "    s_idx = torch.cat([torch.arange(len(x_train))[back_idx], torch.arange(len(x_train), len(x_train) + len(x_test))])\n",
    "    # Build r for inference: background + test\n",
    "    r_inf = torch.zeros(len(xb) + len(xq), len(xb) + len(xq))\n",
    "    r_inf[:len(xb), :len(xb)] = r_train[back_idx][:, back_idx]\n",
    "    # No relations between background and test, nor within test\n",
    "    x_input = torch.cat([torch.cat([xb, yb.unsqueeze(1)], 1), torch.cat([xq, yq.unsqueeze(1)], 1)], 0)\n",
    "    logits = model(x_input, r_inf)\n",
    "    pred = logits[len(xb):].argmax(dim=1)\n",
    "    true = y_test\n",
    "\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_true = true.cpu().numpy()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision}\")\n",
    "    print(f\"Test Recall:    {recall}\")\n",
    "    print(f\"Test F1 Score:  {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2830b2d",
   "metadata": {},
   "source": [
    "# RelMHARegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b58015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrel.train import train_relnet\n",
    "from tabrel.benchmark.nw_regr import generate_toy_regr_data\n",
    "\n",
    "x_numpy, y_numpy, clusters = generate_toy_regr_data(n_samples=900, n_clusters=3, seed=42)\n",
    "r_numpy = (clusters.unsqueeze(1) == clusters.unsqueeze(0)).float().numpy()\n",
    "\n",
    "mse, r2, mae = train_relnet(\n",
    "    x=x_numpy,\n",
    "    y=y_numpy,\n",
    "    r=r_numpy,\n",
    "    backgnd_indices=np.arange(0, n_backgnd),\n",
    "    query_indices=np.arange(n_backgnd, n_train),\n",
    "    val_indices=np.arange(n_train, n),\n",
    "    lr=0.01,\n",
    "    n_epochs=200,\n",
    ")\n",
    "print(f\"Val MSE: {mse:.4f} | R²: {r2:.4f} | MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4963d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7645],\n",
       "        [ 0.8300],\n",
       "        [-0.2343],\n",
       "        [ 0.9186],\n",
       "        [-0.2191],\n",
       "        [ 0.2018],\n",
       "        [-0.4869],\n",
       "        [ 0.5873],\n",
       "        [ 0.8815],\n",
       "        [-0.7336],\n",
       "        [ 0.8692],\n",
       "        [ 0.1872],\n",
       "        [ 0.7388],\n",
       "        [ 0.1354],\n",
       "        [ 0.4822],\n",
       "        [-0.1412],\n",
       "        [ 0.7709],\n",
       "        [ 0.1478],\n",
       "        [-0.4668],\n",
       "        [ 0.2549],\n",
       "        [-0.4607],\n",
       "        [-0.1173],\n",
       "        [-0.4062],\n",
       "        [ 0.6634],\n",
       "        [-0.7894],\n",
       "        [-0.4610],\n",
       "        [-0.2824],\n",
       "        [-0.6013],\n",
       "        [ 0.0944],\n",
       "        [-0.9877],\n",
       "        [ 0.9031],\n",
       "        [-0.8495],\n",
       "        [ 0.7720],\n",
       "        [ 0.1664],\n",
       "        [-0.3247],\n",
       "        [ 0.6179],\n",
       "        [ 0.1559],\n",
       "        [ 0.8080],\n",
       "        [ 0.1093],\n",
       "        [-0.3154],\n",
       "        [ 0.2687],\n",
       "        [-0.2712],\n",
       "        [ 0.4209],\n",
       "        [ 0.8928],\n",
       "        [ 0.5781],\n",
       "        [-0.4372],\n",
       "        [ 0.5773],\n",
       "        [ 0.1789],\n",
       "        [ 0.5078],\n",
       "        [-0.6095],\n",
       "        [-0.9899],\n",
       "        [-0.3864],\n",
       "        [-0.7670],\n",
       "        [ 0.8205],\n",
       "        [ 0.2880],\n",
       "        [ 0.4142],\n",
       "        [ 0.3163],\n",
       "        [-0.0174],\n",
       "        [ 0.7826],\n",
       "        [-0.7105],\n",
       "        [ 0.0630],\n",
       "        [-0.6825],\n",
       "        [ 0.3084],\n",
       "        [-0.3444],\n",
       "        [ 0.3064],\n",
       "        [-0.2083],\n",
       "        [ 0.8294],\n",
       "        [-0.5927],\n",
       "        [-0.5964],\n",
       "        [-0.5964],\n",
       "        [ 0.8994],\n",
       "        [ 0.3333],\n",
       "        [ 0.9623],\n",
       "        [-0.8253],\n",
       "        [-0.9919],\n",
       "        [-0.7824],\n",
       "        [-0.6727],\n",
       "        [ 0.4050],\n",
       "        [ 0.3581],\n",
       "        [ 0.8309],\n",
       "        [-0.5164],\n",
       "        [-0.6817],\n",
       "        [ 0.5306],\n",
       "        [-0.4042],\n",
       "        [ 0.6069],\n",
       "        [-0.2373],\n",
       "        [ 0.5720],\n",
       "        [-0.7770],\n",
       "        [-0.5046],\n",
       "        [ 0.3049],\n",
       "        [ 0.2114],\n",
       "        [-0.2550],\n",
       "        [ 0.5961],\n",
       "        [ 0.6798],\n",
       "        [-0.7252],\n",
       "        [-0.5339],\n",
       "        [ 0.9157],\n",
       "        [-0.3374],\n",
       "        [-0.3545],\n",
       "        [-0.9676],\n",
       "        [-0.5727],\n",
       "        [ 0.2498],\n",
       "        [-0.1320],\n",
       "        [-0.7259],\n",
       "        [ 0.0235],\n",
       "        [-0.6831],\n",
       "        [-0.8484],\n",
       "        [-0.5507],\n",
       "        [-0.8752],\n",
       "        [-0.6367],\n",
       "        [ 0.9996],\n",
       "        [ 0.1889],\n",
       "        [ 0.3082],\n",
       "        [-0.9327],\n",
       "        [-0.6568],\n",
       "        [-0.3329],\n",
       "        [ 0.1564],\n",
       "        [-0.8799],\n",
       "        [-0.4309],\n",
       "        [-0.5987],\n",
       "        [ 0.0028],\n",
       "        [-0.3721],\n",
       "        [-0.0693],\n",
       "        [-0.6776],\n",
       "        [-0.6864],\n",
       "        [-0.5834],\n",
       "        [-0.3423],\n",
       "        [-0.7893],\n",
       "        [ 0.8385],\n",
       "        [-0.1985],\n",
       "        [ 0.8604],\n",
       "        [ 0.3116],\n",
       "        [-0.8468],\n",
       "        [ 0.6920],\n",
       "        [-0.2751],\n",
       "        [-0.3833],\n",
       "        [-0.8301],\n",
       "        [-0.9942],\n",
       "        [ 0.2861],\n",
       "        [-0.2184],\n",
       "        [ 0.3893],\n",
       "        [-0.8207],\n",
       "        [ 0.7424],\n",
       "        [-0.7341],\n",
       "        [-0.1727],\n",
       "        [ 0.2089],\n",
       "        [ 0.5163],\n",
       "        [ 0.8073],\n",
       "        [ 0.9110],\n",
       "        [-0.7929],\n",
       "        [ 0.2517],\n",
       "        [-0.4301],\n",
       "        [-0.1096],\n",
       "        [-0.7485],\n",
       "        [ 0.9109],\n",
       "        [-0.7340],\n",
       "        [ 0.5345],\n",
       "        [ 0.3514],\n",
       "        [ 0.3250],\n",
       "        [-0.5406],\n",
       "        [ 0.9090],\n",
       "        [ 0.2198],\n",
       "        [ 0.1286],\n",
       "        [-0.8813],\n",
       "        [ 0.4198],\n",
       "        [-0.1500],\n",
       "        [-0.4581],\n",
       "        [ 0.8589],\n",
       "        [ 0.2229],\n",
       "        [-0.5533],\n",
       "        [-0.5061],\n",
       "        [-0.0478],\n",
       "        [ 0.5584],\n",
       "        [-0.2555],\n",
       "        [-0.5706],\n",
       "        [-0.3424],\n",
       "        [-0.7471],\n",
       "        [ 0.3566],\n",
       "        [ 0.7740],\n",
       "        [-0.9414],\n",
       "        [ 0.2323],\n",
       "        [ 0.5166],\n",
       "        [ 0.1813],\n",
       "        [-0.3561],\n",
       "        [ 0.5219],\n",
       "        [ 0.5255],\n",
       "        [ 0.3739],\n",
       "        [-0.1757],\n",
       "        [-0.2648],\n",
       "        [ 0.1070],\n",
       "        [-0.1767],\n",
       "        [-0.2980],\n",
       "        [ 0.6392],\n",
       "        [ 0.8594],\n",
       "        [-0.0990],\n",
       "        [-0.2239],\n",
       "        [ 0.0146],\n",
       "        [-0.0597],\n",
       "        [ 0.2404],\n",
       "        [ 0.2802],\n",
       "        [-0.9083],\n",
       "        [-0.3690],\n",
       "        [ 0.8421],\n",
       "        [ 0.3896],\n",
       "        [-0.0497],\n",
       "        [-0.6029],\n",
       "        [-0.6118],\n",
       "        [-0.8958],\n",
       "        [-0.3260],\n",
       "        [ 0.3377],\n",
       "        [ 0.6376],\n",
       "        [ 0.4617],\n",
       "        [-0.8839],\n",
       "        [-0.6014],\n",
       "        [-0.1578],\n",
       "        [ 0.9673],\n",
       "        [ 0.1447],\n",
       "        [-0.2590],\n",
       "        [ 0.4137],\n",
       "        [-0.3809],\n",
       "        [-0.6473],\n",
       "        [ 0.7299],\n",
       "        [-0.4547],\n",
       "        [-0.2005],\n",
       "        [-0.9948],\n",
       "        [ 0.6693],\n",
       "        [ 0.7576],\n",
       "        [ 0.3644],\n",
       "        [-0.6973],\n",
       "        [-0.9869],\n",
       "        [-0.8122],\n",
       "        [ 0.7457],\n",
       "        [ 0.4801],\n",
       "        [ 0.8415],\n",
       "        [ 0.5239],\n",
       "        [ 0.2531],\n",
       "        [-0.0098],\n",
       "        [-0.7605],\n",
       "        [-0.8568],\n",
       "        [-0.9353],\n",
       "        [ 0.4094],\n",
       "        [-0.4910],\n",
       "        [-0.2013],\n",
       "        [-0.5755],\n",
       "        [-0.1822],\n",
       "        [-0.7038],\n",
       "        [-0.6534],\n",
       "        [ 0.3317],\n",
       "        [-0.2972],\n",
       "        [ 0.6173],\n",
       "        [-0.3208],\n",
       "        [-0.7336],\n",
       "        [-0.1764],\n",
       "        [-0.4847],\n",
       "        [-0.3059],\n",
       "        [-0.9520],\n",
       "        [ 0.5595],\n",
       "        [-0.6962],\n",
       "        [ 0.5026],\n",
       "        [ 0.4538],\n",
       "        [ 0.7144],\n",
       "        [-0.7671],\n",
       "        [ 0.7192],\n",
       "        [-0.4728],\n",
       "        [ 0.3711],\n",
       "        [ 0.9391],\n",
       "        [-0.1410],\n",
       "        [-0.0077],\n",
       "        [-0.2302],\n",
       "        [-0.8350],\n",
       "        [ 0.4799],\n",
       "        [-0.9927],\n",
       "        [ 0.6208],\n",
       "        [ 0.7482],\n",
       "        [ 0.9457],\n",
       "        [-0.2359],\n",
       "        [-0.8216],\n",
       "        [ 0.2248],\n",
       "        [ 0.5524],\n",
       "        [-0.9953],\n",
       "        [-0.2270],\n",
       "        [-0.5995],\n",
       "        [-0.0875],\n",
       "        [-0.4922],\n",
       "        [-0.4088],\n",
       "        [-0.3175],\n",
       "        [-0.9503],\n",
       "        [ 0.8205],\n",
       "        [ 0.8383],\n",
       "        [-0.1569],\n",
       "        [-0.1139],\n",
       "        [-0.4081],\n",
       "        [-0.9031],\n",
       "        [-0.9731],\n",
       "        [ 0.3717],\n",
       "        [-0.5490],\n",
       "        [-0.6429],\n",
       "        [-0.0780],\n",
       "        [-0.3330],\n",
       "        [-0.3235],\n",
       "        [ 0.0321],\n",
       "        [-0.2121],\n",
       "        [-0.3443],\n",
       "        [-0.4788],\n",
       "        [-0.8138],\n",
       "        [ 0.8385],\n",
       "        [-0.4002],\n",
       "        [ 0.2650],\n",
       "        [-0.3470],\n",
       "        [ 0.0813],\n",
       "        [ 0.9323],\n",
       "        [ 0.4607],\n",
       "        [-0.8666],\n",
       "        [ 0.3969],\n",
       "        [ 0.9492],\n",
       "        [ 0.2631],\n",
       "        [ 0.6704],\n",
       "        [ 0.9859],\n",
       "        [-0.1532],\n",
       "        [ 0.2076],\n",
       "        [-0.6950],\n",
       "        [-0.2061],\n",
       "        [ 0.7406],\n",
       "        [ 0.5126],\n",
       "        [-0.6328],\n",
       "        [-0.8019],\n",
       "        [-0.6834],\n",
       "        [-0.9869],\n",
       "        [-0.7716],\n",
       "        [-0.2473],\n",
       "        [ 0.6749],\n",
       "        [ 0.1674],\n",
       "        [-0.7606],\n",
       "        [-0.8022],\n",
       "        [ 0.4975],\n",
       "        [-0.7438],\n",
       "        [-0.1231],\n",
       "        [ 0.4797],\n",
       "        [-0.4628],\n",
       "        [-0.1090],\n",
       "        [-0.0870],\n",
       "        [-0.2366],\n",
       "        [-0.5070],\n",
       "        [-0.8914],\n",
       "        [-0.8084],\n",
       "        [-0.5355],\n",
       "        [ 0.9658],\n",
       "        [-0.4830],\n",
       "        [-0.6715],\n",
       "        [ 0.2424],\n",
       "        [ 0.2756],\n",
       "        [ 0.5479],\n",
       "        [ 0.7601],\n",
       "        [ 0.5569],\n",
       "        [-0.9915],\n",
       "        [ 0.0887],\n",
       "        [ 0.6058],\n",
       "        [-0.0924],\n",
       "        [-0.5893],\n",
       "        [ 0.9533],\n",
       "        [-0.3740],\n",
       "        [-0.5693],\n",
       "        [-0.9016],\n",
       "        [ 0.0447],\n",
       "        [ 0.4431],\n",
       "        [ 0.2214],\n",
       "        [ 0.1977],\n",
       "        [-0.7584],\n",
       "        [-0.9339],\n",
       "        [ 0.0176],\n",
       "        [ 0.9118],\n",
       "        [ 0.5769],\n",
       "        [-0.5822],\n",
       "        [-0.1298],\n",
       "        [-0.7372],\n",
       "        [-0.4824],\n",
       "        [ 0.1811],\n",
       "        [ 0.5445],\n",
       "        [ 0.8284],\n",
       "        [-0.9181],\n",
       "        [ 0.6686],\n",
       "        [-0.7053],\n",
       "        [ 0.3745],\n",
       "        [ 0.8462],\n",
       "        [ 0.0140],\n",
       "        [ 0.9098],\n",
       "        [-0.8521],\n",
       "        [-0.3820],\n",
       "        [ 0.5833],\n",
       "        [-0.2179],\n",
       "        [-0.2047],\n",
       "        [-0.4168],\n",
       "        [ 0.6893],\n",
       "        [ 0.4905],\n",
       "        [ 0.3205],\n",
       "        [-0.5620],\n",
       "        [-0.8117],\n",
       "        [ 0.1082],\n",
       "        [ 0.2963],\n",
       "        [-0.4617],\n",
       "        [-0.2798],\n",
       "        [ 0.6754],\n",
       "        [ 0.0797],\n",
       "        [ 0.0451],\n",
       "        [-0.2461],\n",
       "        [-0.9056],\n",
       "        [-0.9403],\n",
       "        [-0.4780],\n",
       "        [-0.5083],\n",
       "        [ 0.3116],\n",
       "        [-0.2911],\n",
       "        [-0.3912],\n",
       "        [ 0.9534],\n",
       "        [ 0.3483],\n",
       "        [ 0.7129],\n",
       "        [-0.4841],\n",
       "        [-0.4085],\n",
       "        [ 0.3675],\n",
       "        [-0.6663],\n",
       "        [-0.6537],\n",
       "        [-0.0483],\n",
       "        [-0.3658],\n",
       "        [-0.7497],\n",
       "        [ 0.5932],\n",
       "        [ 0.8042],\n",
       "        [ 0.1622],\n",
       "        [-0.1741],\n",
       "        [-0.9263],\n",
       "        [-0.3642],\n",
       "        [ 0.2546],\n",
       "        [ 0.4715],\n",
       "        [-0.1264],\n",
       "        [-0.3954],\n",
       "        [ 0.5572],\n",
       "        [-0.7964],\n",
       "        [ 0.6320],\n",
       "        [-0.3880],\n",
       "        [ 0.0153],\n",
       "        [-0.1976],\n",
       "        [ 0.1212],\n",
       "        [-0.3022],\n",
       "        [ 0.7271],\n",
       "        [-0.0260],\n",
       "        [ 0.7806],\n",
       "        [ 0.9615],\n",
       "        [-0.4872],\n",
       "        [-0.7295],\n",
       "        [ 0.8023],\n",
       "        [ 0.7836],\n",
       "        [-0.7635],\n",
       "        [-0.0773],\n",
       "        [-0.9861],\n",
       "        [-0.8186],\n",
       "        [ 0.1931],\n",
       "        [ 0.2660],\n",
       "        [ 0.2120],\n",
       "        [-0.2722],\n",
       "        [ 0.9226],\n",
       "        [ 0.1430],\n",
       "        [-0.5901],\n",
       "        [-0.0566],\n",
       "        [ 0.2401],\n",
       "        [ 0.3502],\n",
       "        [-0.7071],\n",
       "        [ 0.3748],\n",
       "        [-0.5109],\n",
       "        [-0.8309],\n",
       "        [-0.5462],\n",
       "        [ 0.9644],\n",
       "        [ 0.8549],\n",
       "        [ 0.8955],\n",
       "        [ 0.5870],\n",
       "        [ 0.7554],\n",
       "        [-0.1338],\n",
       "        [-0.5502],\n",
       "        [ 0.4997],\n",
       "        [-0.5182],\n",
       "        [-0.6749],\n",
       "        [-0.3193],\n",
       "        [ 0.2099],\n",
       "        [ 0.5148],\n",
       "        [-0.3884],\n",
       "        [-0.5886],\n",
       "        [ 0.1349],\n",
       "        [-0.5894],\n",
       "        [-0.6511],\n",
       "        [ 0.5213],\n",
       "        [-0.1680],\n",
       "        [ 0.9138],\n",
       "        [ 0.9728],\n",
       "        [ 0.2991],\n",
       "        [ 0.3442],\n",
       "        [ 0.2303],\n",
       "        [ 0.0157],\n",
       "        [-0.0727],\n",
       "        [ 0.0137],\n",
       "        [ 0.3734],\n",
       "        [ 0.9298],\n",
       "        [-0.2592],\n",
       "        [-0.4227],\n",
       "        [-0.2422],\n",
       "        [-0.4831],\n",
       "        [ 0.1700],\n",
       "        [ 0.7464],\n",
       "        [ 0.7820],\n",
       "        [ 0.4591],\n",
       "        [-0.7359],\n",
       "        [-0.5367],\n",
       "        [-0.2197],\n",
       "        [-0.1843],\n",
       "        [ 0.0822],\n",
       "        [-0.9180],\n",
       "        [ 0.3112],\n",
       "        [-0.7629],\n",
       "        [-0.6327],\n",
       "        [-0.8314],\n",
       "        [ 0.8713],\n",
       "        [-0.9469],\n",
       "        [ 0.7544],\n",
       "        [-0.0336],\n",
       "        [-0.1163],\n",
       "        [ 0.6255],\n",
       "        [-0.0924],\n",
       "        [ 0.6272],\n",
       "        [ 0.7230],\n",
       "        [-0.8682],\n",
       "        [ 0.3848],\n",
       "        [ 0.1888],\n",
       "        [ 0.2150],\n",
       "        [ 0.1460],\n",
       "        [ 0.2735],\n",
       "        [-0.4811],\n",
       "        [-0.1279],\n",
       "        [ 0.9501],\n",
       "        [ 0.6718],\n",
       "        [-0.0376],\n",
       "        [-0.9405],\n",
       "        [ 0.0438],\n",
       "        [-0.6810],\n",
       "        [ 0.8132],\n",
       "        [-0.6071],\n",
       "        [-0.0722],\n",
       "        [-0.2219],\n",
       "        [ 0.1780],\n",
       "        [ 0.9410],\n",
       "        [ 0.0950],\n",
       "        [ 0.5792],\n",
       "        [ 0.7762],\n",
       "        [ 0.8073],\n",
       "        [-0.3454],\n",
       "        [-0.2237],\n",
       "        [ 0.4819],\n",
       "        [-0.2729],\n",
       "        [ 0.4683],\n",
       "        [-0.2185],\n",
       "        [-0.6783],\n",
       "        [ 0.4070],\n",
       "        [ 0.1533],\n",
       "        [ 0.4458],\n",
       "        [ 0.9935],\n",
       "        [ 0.6827],\n",
       "        [ 0.9480],\n",
       "        [ 0.0535],\n",
       "        [-0.8602],\n",
       "        [-0.7015],\n",
       "        [-0.6212],\n",
       "        [-0.8812],\n",
       "        [-0.5012],\n",
       "        [-0.9206],\n",
       "        [-0.9226],\n",
       "        [-0.5976],\n",
       "        [-0.9858],\n",
       "        [-0.6138],\n",
       "        [ 0.3813],\n",
       "        [ 0.8341],\n",
       "        [-0.2975],\n",
       "        [-0.2909],\n",
       "        [ 0.5340],\n",
       "        [-0.4934],\n",
       "        [-0.4728],\n",
       "        [ 0.6161],\n",
       "        [-0.8713],\n",
       "        [ 0.1223],\n",
       "        [ 0.8834],\n",
       "        [ 0.1715],\n",
       "        [ 0.2719],\n",
       "        [-0.5824],\n",
       "        [-0.0138],\n",
       "        [ 0.0550],\n",
       "        [ 0.2454],\n",
       "        [ 0.3885],\n",
       "        [ 0.8689],\n",
       "        [-0.7633],\n",
       "        [ 0.0300],\n",
       "        [-0.4996],\n",
       "        [-0.7911],\n",
       "        [-0.0801],\n",
       "        [-0.8802],\n",
       "        [ 0.6979],\n",
       "        [ 0.1158],\n",
       "        [-0.5390],\n",
       "        [ 0.5226],\n",
       "        [-0.9464],\n",
       "        [-0.3868],\n",
       "        [-0.1948],\n",
       "        [-0.8498],\n",
       "        [-0.6359],\n",
       "        [-0.1632],\n",
       "        [ 0.7588],\n",
       "        [ 0.9657],\n",
       "        [ 0.6363],\n",
       "        [-0.5971],\n",
       "        [-0.6542],\n",
       "        [ 0.8727],\n",
       "        [ 0.3537],\n",
       "        [ 0.0266],\n",
       "        [ 0.1353],\n",
       "        [-0.8037],\n",
       "        [-0.3339],\n",
       "        [ 0.9626],\n",
       "        [-0.2466],\n",
       "        [-0.0502],\n",
       "        [-0.8303],\n",
       "        [-0.5594],\n",
       "        [-0.0204],\n",
       "        [-0.6212],\n",
       "        [-0.1240],\n",
       "        [ 0.4070],\n",
       "        [-0.9782],\n",
       "        [ 0.2970],\n",
       "        [-0.6612],\n",
       "        [-0.4881],\n",
       "        [ 0.3839],\n",
       "        [ 0.7951],\n",
       "        [-0.2733],\n",
       "        [-0.4106],\n",
       "        [-0.9042],\n",
       "        [-0.5157],\n",
       "        [-0.8756],\n",
       "        [-0.2289],\n",
       "        [ 0.2041],\n",
       "        [-0.9369],\n",
       "        [ 0.8731],\n",
       "        [ 0.6274],\n",
       "        [-0.9789],\n",
       "        [-0.4776],\n",
       "        [ 0.3262],\n",
       "        [-0.2055],\n",
       "        [-0.1090],\n",
       "        [-0.4515],\n",
       "        [ 0.8032],\n",
       "        [-0.5590],\n",
       "        [ 0.8293],\n",
       "        [ 0.0645],\n",
       "        [ 0.2010],\n",
       "        [ 0.7801],\n",
       "        [-0.1648],\n",
       "        [-0.5693],\n",
       "        [-0.1617],\n",
       "        [ 0.8111],\n",
       "        [-0.7420],\n",
       "        [ 0.2270],\n",
       "        [-0.9828],\n",
       "        [ 0.5243],\n",
       "        [ 0.3695],\n",
       "        [ 0.0424],\n",
       "        [ 0.4292],\n",
       "        [ 0.0011],\n",
       "        [ 0.5534],\n",
       "        [-0.7916],\n",
       "        [-0.1469],\n",
       "        [ 0.4436],\n",
       "        [ 0.9958],\n",
       "        [ 0.5094],\n",
       "        [-0.7272],\n",
       "        [ 0.7691],\n",
       "        [-0.2230],\n",
       "        [-0.2135],\n",
       "        [-0.9089],\n",
       "        [-0.1574],\n",
       "        [ 0.7073],\n",
       "        [ 0.1394],\n",
       "        [-0.5825],\n",
       "        [ 0.3078],\n",
       "        [-0.3206],\n",
       "        [ 0.9130],\n",
       "        [-0.8680],\n",
       "        [-0.3159],\n",
       "        [-0.9656],\n",
       "        [-0.3938],\n",
       "        [ 0.3152],\n",
       "        [ 0.9626],\n",
       "        [ 0.1679],\n",
       "        [ 0.9804],\n",
       "        [ 0.1957],\n",
       "        [ 0.5775],\n",
       "        [ 0.8017],\n",
       "        [ 0.8359],\n",
       "        [-0.5597],\n",
       "        [ 0.9194],\n",
       "        [ 0.6058],\n",
       "        [-0.4676],\n",
       "        [-0.4772],\n",
       "        [-0.8387],\n",
       "        [ 0.2511],\n",
       "        [-0.8105],\n",
       "        [ 0.4224],\n",
       "        [ 0.3158],\n",
       "        [-0.8688],\n",
       "        [ 0.2725],\n",
       "        [-0.0813],\n",
       "        [ 0.4568],\n",
       "        [ 0.5738],\n",
       "        [-0.9941],\n",
       "        [ 0.9171],\n",
       "        [ 0.8386],\n",
       "        [ 0.3979],\n",
       "        [-0.9140],\n",
       "        [-0.3572],\n",
       "        [-0.2898],\n",
       "        [-0.2570],\n",
       "        [ 0.5639],\n",
       "        [ 0.3636],\n",
       "        [ 0.7922],\n",
       "        [-0.3745],\n",
       "        [ 0.3365],\n",
       "        [ 0.3558],\n",
       "        [-0.8326],\n",
       "        [-0.9700],\n",
       "        [-0.5189],\n",
       "        [ 0.6845],\n",
       "        [-0.9415],\n",
       "        [-0.8704],\n",
       "        [ 0.5602],\n",
       "        [ 0.5395],\n",
       "        [ 0.8224],\n",
       "        [-0.7549],\n",
       "        [-0.7319],\n",
       "        [ 0.5130],\n",
       "        [ 0.8696],\n",
       "        [ 0.5983],\n",
       "        [ 0.1567],\n",
       "        [ 0.3296],\n",
       "        [ 0.9491],\n",
       "        [-0.6452],\n",
       "        [-0.4540],\n",
       "        [ 0.6995],\n",
       "        [-0.6842],\n",
       "        [-0.5514],\n",
       "        [ 0.7300],\n",
       "        [ 0.3155],\n",
       "        [ 0.3231],\n",
       "        [-0.4238],\n",
       "        [-0.0138],\n",
       "        [ 0.9152],\n",
       "        [-0.6002],\n",
       "        [ 0.0079],\n",
       "        [ 0.4756],\n",
       "        [-0.6904],\n",
       "        [ 0.9712],\n",
       "        [-0.4996],\n",
       "        [-0.2401],\n",
       "        [-0.2706],\n",
       "        [-0.6517],\n",
       "        [-0.9813],\n",
       "        [ 0.5639],\n",
       "        [ 0.2657],\n",
       "        [-0.9366],\n",
       "        [-0.6436],\n",
       "        [ 0.9884],\n",
       "        [ 0.3822],\n",
       "        [ 0.4012],\n",
       "        [-0.5983],\n",
       "        [-0.4384],\n",
       "        [-0.1510],\n",
       "        [-0.1829],\n",
       "        [-0.6852],\n",
       "        [ 0.0824],\n",
       "        [ 0.0994],\n",
       "        [-0.1266],\n",
       "        [ 0.1386],\n",
       "        [-0.3964],\n",
       "        [ 0.2603],\n",
       "        [ 0.3771],\n",
       "        [-0.5267],\n",
       "        [-0.9916],\n",
       "        [ 0.5234],\n",
       "        [ 0.2385],\n",
       "        [-0.5086],\n",
       "        [ 0.9637],\n",
       "        [-0.4522],\n",
       "        [ 0.6757],\n",
       "        [ 0.5073],\n",
       "        [-0.8384],\n",
       "        [ 0.6449],\n",
       "        [-0.9195],\n",
       "        [-0.5540],\n",
       "        [-0.1667],\n",
       "        [-0.6740],\n",
       "        [ 0.9769],\n",
       "        [-0.2006],\n",
       "        [ 0.3972],\n",
       "        [-0.8929],\n",
       "        [ 0.5757],\n",
       "        [-0.3108],\n",
       "        [-0.7607],\n",
       "        [ 0.1462],\n",
       "        [ 0.4845],\n",
       "        [ 0.8654],\n",
       "        [-0.6108],\n",
       "        [-0.4921],\n",
       "        [ 0.1923],\n",
       "        [ 0.2713],\n",
       "        [ 0.3845],\n",
       "        [ 0.5489],\n",
       "        [-0.2268],\n",
       "        [ 0.5556],\n",
       "        [ 0.7373],\n",
       "        [-0.2612],\n",
       "        [ 0.7115],\n",
       "        [ 0.4886],\n",
       "        [ 0.8821],\n",
       "        [-0.5683],\n",
       "        [-0.4938],\n",
       "        [-0.2891],\n",
       "        [ 0.0507],\n",
       "        [ 0.6002],\n",
       "        [-0.5709],\n",
       "        [ 0.5007],\n",
       "        [-0.3584],\n",
       "        [ 0.6041],\n",
       "        [-0.0475],\n",
       "        [-0.8761],\n",
       "        [-0.5502],\n",
       "        [-0.7238],\n",
       "        [ 0.4960],\n",
       "        [-0.6705],\n",
       "        [-0.0833],\n",
       "        [ 0.2158],\n",
       "        [-0.5484],\n",
       "        [ 0.2885],\n",
       "        [-0.9764],\n",
       "        [-0.7155],\n",
       "        [-0.9061],\n",
       "        [-0.3025],\n",
       "        [-0.3643],\n",
       "        [ 0.1432],\n",
       "        [-0.1849],\n",
       "        [ 0.4701],\n",
       "        [ 0.9168],\n",
       "        [ 0.3588],\n",
       "        [-0.3940],\n",
       "        [-0.9364],\n",
       "        [ 0.3622],\n",
       "        [-0.4955],\n",
       "        [ 0.5089],\n",
       "        [ 0.6685],\n",
       "        [ 0.3857],\n",
       "        [ 0.9383],\n",
       "        [ 0.9498],\n",
       "        [ 0.2117],\n",
       "        [-0.7286],\n",
       "        [ 0.8934],\n",
       "        [-0.4745],\n",
       "        [-0.4724],\n",
       "        [ 0.8368],\n",
       "        [ 0.7748],\n",
       "        [ 0.3022],\n",
       "        [ 0.0627],\n",
       "        [-0.8412],\n",
       "        [-0.1038],\n",
       "        [ 0.9591],\n",
       "        [ 0.2547],\n",
       "        [ 0.0856],\n",
       "        [-0.2077],\n",
       "        [-0.3488],\n",
       "        [ 0.5960],\n",
       "        [ 0.0617],\n",
       "        [ 0.6506],\n",
       "        [-0.1770],\n",
       "        [ 0.4369],\n",
       "        [ 0.4128],\n",
       "        [ 0.1595],\n",
       "        [ 0.6284],\n",
       "        [ 0.6266],\n",
       "        [ 0.9269],\n",
       "        [ 0.7688],\n",
       "        [-0.2557],\n",
       "        [-0.8466],\n",
       "        [ 0.1828],\n",
       "        [-0.0087],\n",
       "        [-0.2608],\n",
       "        [-0.1675],\n",
       "        [ 0.0470],\n",
       "        [ 0.7296],\n",
       "        [ 0.3117],\n",
       "        [-0.3551],\n",
       "        [-0.4112],\n",
       "        [-0.2476],\n",
       "        [-0.3865]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
