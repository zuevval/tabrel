{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabrel.utils.treatment import load_ihdp_data, generate_indices\n",
    "\n",
    "\n",
    "ihdp_data, ihdp_exclude_cols, ihdp_tau_colname, _, _, _ = load_ihdp_data(Path(\"../CEVAE/datasets/IHDP\"))\n",
    "ihdp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "x_all = ihdp_data.drop(columns=ihdp_exclude_cols + [ihdp_tau_colname])\n",
    "\n",
    "ihdp_last_numeric_index: Final[int] = 6\n",
    "x_numeric = x_all.iloc[:, :ihdp_last_numeric_index]\n",
    "# x_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x_num_y = x_numeric.copy()\n",
    "x_num_y[ihdp_tau_colname] = ihdp_data[ihdp_tau_colname]\n",
    "# sns.pairplot(x_num_y, hue=ihdp_tau_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_y.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x_all[[\"x1\", \"x2\", \"x4\", \"x6\", \"x14\", \"x16\", \"x18\"]])\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=30)\n",
    "clusters = clusterer.fit_predict(x_scaled)\n",
    "\n",
    "# group_col: Final[str] = \"x4\"\n",
    "# x = x_all.drop(columns=[group_col])\n",
    "x_len = len(x_all)\n",
    "# categories = x_all[group_col].to_numpy()\n",
    "categories = clusters\n",
    "print(\"n_categories\", len(np.unique(categories)))\n",
    "\n",
    "def make_r(cats_: np.ndarray, progress_bar: bool) -> np.ndarray:\n",
    "    n_ = len(cats_)\n",
    "    r_ = np.zeros((n_, n_))\n",
    "    iter_pairs = list(product(range(n_), range(n_)))\n",
    "    for i, j in tqdm(iter_pairs, desc=\"make R\") if progress_bar else iter_pairs:\n",
    "        if np.isclose(cats_[i], cats_[j]):\n",
    "            r_[i, j] = 1\n",
    "    return r_\n",
    "r = make_r(categories, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78411c2b",
   "metadata": {},
   "source": [
    "# S-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tabrel.benchmark.nw_regr import run_training, metrics_mean, train_nw_arbitrary, NwModelConfig, RelNwRegr\n",
    "from tabrel.train import train_relnet\n",
    "from tabrel.utils.misc import to_tensor\n",
    "\n",
    "labels = [\"rel\", \"nrel\", \"lgb\", \"rel-fts\", \"lgb-rel\"]\n",
    "\n",
    "def split_treated_non_treated(x: pd.DataFrame, treatment: np.ndarray, y_fact: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    treated = treatment == 1\n",
    "    x_treated, y_treated = x.loc[treated], y_fact.loc[treated]\n",
    "    x_non_treated, y_non_treated = x.loc[~treated], y_fact.loc[~treated]\n",
    "    return x_treated.to_numpy(), y_treated.to_numpy(), x_non_treated.to_numpy(), y_non_treated.to_numpy()\n",
    "\n",
    "y_fact_colname, y_cfact_colname = \"y_factual\", \"y_cfactual\"\n",
    "data_y_fact, data_y_cfact, data_treatment = ihdp_data[y_fact_colname], ihdp_data[y_cfact_colname], ihdp_data[\"treatment\"]\n",
    "\n",
    "\n",
    "y_s = data_y_fact.to_numpy() # Y for S-learner\n",
    "y_s_cfact = data_y_cfact.to_numpy()\n",
    "treatment_np = data_treatment.to_numpy()\n",
    "tau_true = ihdp_data[ihdp_tau_colname].to_numpy()\n",
    "\n",
    "lr, n_epochs = 1e-3, 50\n",
    "lgb_params: Final[dict[str, str | int]] = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618cb1dd",
   "metadata": {},
   "source": [
    "### S-learner: MLP + Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrel.benchmark.nw_regr import MlpConfig, train_nw_arbitrary\n",
    "\n",
    "def train_mlp_s_learner(\n",
    "        mlp_out_dim: int,\n",
    "        mlp_hidden_dim: int,\n",
    "        dropout: float,\n",
    "        lr: float,\n",
    "        _seed: int,\n",
    ") -> float:\n",
    "    ids_q, ids_b, ids_v = generate_indices(_seed, n_total=x_len)\n",
    "\n",
    "    x_all[\"treatment\"] = data_treatment\n",
    "    x_np = x_all.to_numpy()\n",
    "\n",
    "    xv, yv, yv_cfact, treatment_v = (\n",
    "        x_np[ids_v],\n",
    "        y_s[ids_v],\n",
    "        y_s_cfact[ids_v],\n",
    "        treatment_np[ids_v],\n",
    "    )\n",
    "\n",
    "    yv_s = np.concatenate([yv, yv_cfact])\n",
    "    xv_cfact = xv.copy()\n",
    "    xv_cfact[:, -1] = 1 - treatment_v  # assuming treatment is the last col\n",
    "    xv_s = np.concatenate([xv, xv_cfact])\n",
    "    # x_s_np = np.concatenate([x_np[:x_len], xv_cfact])\n",
    "    cats_s = np.concatenate([categories, categories[ids_v]])\n",
    "    r_s = make_r(cats_s, progress_bar=True)\n",
    "    \n",
    "    r_q_b = r_s[ids_q][:, ids_b]\n",
    "    \n",
    "    ids_v_s = np.concatenate([ids_v, np.arange(start=x_len, stop=len(ids_v) + x_len)])\n",
    "    r_val_train = r_s[ids_v_s][:, np.concatenate([ids_b, ids_q])]\n",
    "\n",
    "    _, _, _, model = train_nw_arbitrary(\n",
    "        x_backgnd=x_np[ids_b],\n",
    "        y_backgnd=y_s[ids_b],\n",
    "        x_query=x_np[ids_q],\n",
    "        y_query=y_s[ids_q],\n",
    "        x_val=xv_s,\n",
    "        y_val=yv_s,\n",
    "        r_query_backgnd=r_q_b,\n",
    "        r_val_nonval=r_val_train,\n",
    "        cfg=NwModelConfig(\n",
    "            init_sigma=1.,\n",
    "            init_r_scale=1.,\n",
    "            input_dim=mlp_out_dim,\n",
    "            trainable_weights_matrix=False,\n",
    "            mlp_config=MlpConfig(\n",
    "                in_dim=x_all.shape[1],\n",
    "                hidden_dim=mlp_hidden_dim,\n",
    "                out_dim=mlp_out_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        ),\n",
    "        lr=lr,\n",
    "        n_epochs=1000,\n",
    "    )\n",
    "\n",
    "    n_val = len(xv)\n",
    "    y_val_pred_fact, y_val_pred_cfact = model.y_val_pred[:n_val], model.y_val_pred[n_val:]\n",
    "    tau_val_pred = (y_val_pred_fact - y_val_pred_cfact) * (-1) ** (1 - treatment_v)\n",
    "    tau_val_true = tau_true[ids_v]\n",
    "\n",
    "    return mean_squared_error(tau_val_true, tau_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d59f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "sqlite_path: Final[str] = \"sqlite:///db.sqlite3\"\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\n",
    "def mlp_objective(trial: optuna.Trial) -> float:\n",
    "    global seed\n",
    "    seed += 1\n",
    "\n",
    "    return train_mlp_s_learner(\n",
    "        mlp_out_dim=trial.suggest_int(\"mlp_out_dim\", 1, 40),\n",
    "        mlp_hidden_dim=trial.suggest_int(\"hidden_dim\", 4, 100),\n",
    "        dropout=trial.suggest_float(\"dropout\", .0, .6),\n",
    "        lr=trial.suggest_float(\"lr\", 1e-4, 1e-1),\n",
    "        _seed=seed\n",
    "    )\n",
    "\n",
    "    \n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=f\"TE_sLearner_mlpNw_{datetime.now()}\",\n",
    "    # storage=sqlite_path,\n",
    ")\n",
    "\n",
    "study.optimize(mlp_objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params\n",
    "\n",
    "# {'mlp_out_dim': 40,\n",
    "#  'hidden_dim': 78,\n",
    "#  'dropout': 0.5335761824224884,\n",
    "#  'lr': 0.014194376718333799}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pehes = []\n",
    "\n",
    "for seed in range(15):\n",
    "    mlp_pehes.append(train_mlp_s_learner(\n",
    "        mlp_out_dim=40,\n",
    "        mlp_hidden_dim=78,\n",
    "        dropout=.534,\n",
    "        lr=.014,\n",
    "        _seed=seed,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(mlp_pehes), 2), round(np.std(mlp_pehes), 2)  # (3.86, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "\n",
    "for x_s, xs_label in ((\n",
    "    #  x,\n",
    "     # x_all.drop(columns=[\"x1\", \"x2\", \"x4\", \"x6\", \"x14\", \"x16\", \"x18\"]),\n",
    "     x_all[[\"x1\", ]],                    \n",
    "    \"x\"), \n",
    "    (x_all, \"x_all\")\n",
    "    ):\n",
    "    x_s[\"treatment\"] = data_treatment\n",
    "    x_s_np = x_s.to_numpy()\n",
    "    n_samples, n_feats = x_s_np.shape\n",
    "    for seed in range(2):\n",
    "        np.random.seed(seed)\n",
    "        ids_q, ids_b, ids_v = generate_indices(seed, n_total=x_len)\n",
    "        ids_train = np.concatenate((ids_b, ids_q))\n",
    "        xb, yb, xq, yq = x_s_np[ids_b], y_s[ids_b], x_s_np[ids_q], y_s[ids_q]\n",
    "        xv, yv, yv_cfact, treatment_v = x_s_np[ids_v], y_s[ids_v], y_s_cfact[ids_v], treatment_np[ids_v]\n",
    "        n_val = len(xv)\n",
    "\n",
    "        yv = np.concatenate([yv, yv_cfact])\n",
    "        xv_cfact = xv.copy()\n",
    "        xv_cfact[:, -1] = 1 - treatment_v # assuming treatment is the last col\n",
    "        xv = np.concatenate([xv, xv_cfact])\n",
    "        x_s_np = np.concatenate([x_s_np[:len(x_s)], xv_cfact])\n",
    "        y_s = np.concatenate([y_s[:len(x_s)], yv_cfact])\n",
    "        cats_s = np.concatenate([categories, categories[ids_v]])\n",
    "        tau_val_true = tau_true[ids_v]\n",
    "        ids_v = np.concatenate([ids_v, np.arange(start=len(x_s), stop=len(ids_v) + len(x_s))])\n",
    "\n",
    "        # NW without rel\n",
    "        _, _, _, model_nrel = train_nw_arbitrary(\n",
    "            x_backgnd=xb,\n",
    "            y_backgnd=yb,\n",
    "            x_query=xq,\n",
    "            y_query=yq,\n",
    "            x_val=xv,\n",
    "            y_val=yv,\n",
    "            r_query_backgnd=np.zeros((len(xq), len(xb))),\n",
    "            r_val_nonval=np.zeros((len(xv), len(xb) + len(xq))),\n",
    "            cfg=NwModelConfig(input_dim=n_feats),\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "\n",
    "        y_val_pred_fact, y_val_pred_cfact = model_nrel.y_val_pred[:n_val], model_nrel.y_val_pred[n_val:]\n",
    "        tau_val_pred = (y_val_pred_fact - y_val_pred_cfact) * (-1) ** (1 + treatment_v)\n",
    "        metrics[f\"nrel_{xs_label}\"].append(mean_squared_error(tau_val_true, tau_val_pred))\n",
    "\n",
    "        # LightGBM\n",
    "        x_train = np.concatenate([xq, xb])\n",
    "        y_train = np.concatenate([yq, yb])\n",
    "        lgb_model = lgb.train(lgb_params, lgb.Dataset(x_train, label=y_train))\n",
    "        y_pred_lgb = lgb_model.predict(xv)\n",
    "\n",
    "        y_lgb_fact, y_lgb_cfact = y_pred_lgb[:n_val], y_pred_lgb[n_val:]\n",
    "        tau_lgb_pred = (y_lgb_fact - y_lgb_cfact) * (-1) ** (1 - treatment_v)\n",
    "\n",
    "        metrics[f\"lgb_{xs_label}\"].append(mean_squared_error(tau_val_true, tau_lgb_pred))\n",
    "\n",
    "        # plt.figure()\n",
    "        # plt.title(f\"LGB {xs_label} seed {seed}\")\n",
    "        # plt.plot(range(len(tau_val_true)), tau_val_true, label=\"tau validate true\")\n",
    "        # plt.plot(range(len(tau_lgb_pred)), tau_lgb_pred, label = \"tau val LGB\")\n",
    "        # plt.show()\n",
    "\n",
    "        # continue # TODO\n",
    "        if xs_label != \"x_all\":\n",
    "             continue\n",
    "        \n",
    "        r_s = make_r(cats_s, progress_bar=True)\n",
    "        r_q_b = r_s[ids_q][:, ids_b]\n",
    "        r_val_train = r_s[ids_v][:, ids_train]\n",
    "\n",
    "        # TabRel\n",
    "        torch.manual_seed(seed)\n",
    "        _, _, _, y_val_pred, _ = train_relnet(\n",
    "            x=x_s_np,\n",
    "            y=y_s,\n",
    "            r=r_s,\n",
    "            backgnd_indices=ids_b,\n",
    "            query_indices=ids_q,\n",
    "            val_indices=ids_v,\n",
    "            lr=0.01,\n",
    "            n_epochs=800,\n",
    "            n_layers=2,\n",
    "            periodic_embed_dim=None,\n",
    "            num_heads=2,\n",
    "            progress_bar=True,\n",
    "        )\n",
    "        y_val_pred_fact, y_val_pred_cfact = y_val_pred[:n_val], y_val_pred[n_val:]\n",
    "        tau_val_pred = (y_val_pred_fact - y_val_pred_cfact) * (-1) ** (1 + treatment_v)\n",
    "        relnet_pehe = mean_squared_error(tau_val_pred, tau_val_true)\n",
    "        metrics[\"relnet\"].append(relnet_pehe)\n",
    "\n",
    "        # NW with rel\n",
    "        _, _, _, model_rel = train_nw_arbitrary(\n",
    "            x_backgnd=xb,\n",
    "            y_backgnd=yb,\n",
    "            x_query=xq,\n",
    "            y_query=yq,\n",
    "            x_val=xv,\n",
    "            y_val=yv,\n",
    "            r_query_backgnd=r_q_b,\n",
    "            r_val_nonval=r_val_train,\n",
    "            cfg=NwModelConfig(input_dim=n_feats),\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "\n",
    "        y_val_pred_fact, y_val_pred_cfact = model_rel.y_val_pred[:n_val], model_rel.y_val_pred[n_val:]\n",
    "        tau_val_pred = (y_val_pred_fact - y_val_pred_cfact) * (-1) ** (1 + treatment_v)\n",
    "        metrics[\"rel\"].append(mean_squared_error(tau_val_true, tau_val_pred))\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        x_back, x_query, x_val  = to_tensor(xb), to_tensor(xq), to_tensor(xv)\n",
    "        y_back, y_query, y_val = to_tensor(yb), to_tensor(yq), to_tensor(yv)\n",
    "        x_train, y_train = torch.cat([x_back, x_query]), torch.cat([y_back, y_query])\n",
    "        for learnable_norm in (True, False):\n",
    "                config = NwModelConfig(\n",
    "                    init_sigma=1.0,\n",
    "                    init_r_scale=1.0,\n",
    "                    input_dim=n_feats, # if x_label == \"xInit\" else n_feats + n_samples,\n",
    "                    trainable_weights_matrix=learnable_norm,\n",
    "                )\n",
    "                model = RelNwRegr(config)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "                loss_fn = nn.MSELoss()\n",
    "                torch.manual_seed(seed)\n",
    "                model.train()\n",
    "\n",
    "                n_epochs = 500\n",
    "                for _ in range(n_epochs):\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(x_back, y_back, x_query, to_tensor(r_q_b))\n",
    "                    loss = loss_fn(y_pred, y_query)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        x_val,\n",
    "                        to_tensor(r_val_train),\n",
    "                    )\n",
    "                    y_pred_np = y_pred.numpy()\n",
    "\n",
    "                    y_val_pred_fact, y_val_pred_cfact = y_pred_np[:n_val], y_pred_np[n_val:]\n",
    "                    tau_val_pred = (y_val_pred_fact - y_val_pred_cfact) * (-1) ** (1 + treatment_v)\n",
    "\n",
    "                    mse = mean_squared_error(tau_val_true, tau_val_pred)\n",
    "                    metrics[f\"learnNorm{learnable_norm}_pehe\"].append(mse)\n",
    "\n",
    "        # NW and LGB with rel as features\n",
    "        x_broad = np.concatenate((x_s_np, r_s), axis=1)\n",
    "        xb_broad, xq_broad, xv_broad = x_broad[ids_b], x_broad[ids_q], x_broad[ids_v]\n",
    " \n",
    "        # NW with rel as features\n",
    "        _, _, _, model_relfts = train_nw_arbitrary(\n",
    "            x_backgnd=xb_broad,\n",
    "            y_backgnd=yb,\n",
    "            x_query=xq_broad,\n",
    "            y_query=yq,\n",
    "            x_val=xv_broad,\n",
    "            y_val=yv,\n",
    "            r_query_backgnd=np.zeros((len(xq_broad), len(xb_broad))),\n",
    "            r_val_nonval=np.zeros((len(xv_broad), len(xb_broad) + len(xq_broad))),\n",
    "            cfg=NwModelConfig(input_dim = len(xb_broad[0])),\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "        y_val_pred_fact, y_val_pred_cfact = model_relfts.y_val_pred[:n_val], model_relfts.y_val_pred[n_val:]\n",
    "        tau_val_pred = (y_val_pred_fact - y_val_pred_cfact) * (-1) ** (1 + treatment_v)\n",
    "        metrics[\"rel-fts\"].append(mean_squared_error(tau_val_true, tau_val_pred))\n",
    "\n",
    "        # LightGBM with rel as features\n",
    "        x_train_broad = np.concatenate([xq_broad, xb_broad])\n",
    "        lgb_model_rel = lgb.train(lgb_params, lgb.Dataset(x_train_broad, label=y_train.numpy()))\n",
    "        y_pred_lgb_rel = lgb_model_rel.predict(xv_broad)\n",
    "\n",
    "        y_lgb_fact, y_lgb_cfact = y_pred_lgb_rel[:n_val], y_pred_lgb_rel[n_val:]\n",
    "        tau_lgb_pred = (y_lgb_fact - y_lgb_cfact) * (-1) ** (1 - treatment_v)\n",
    "        metrics[\"lgb-rel\"].append(mean_squared_error(tau_val_true, tau_lgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae907d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca297045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mean metrics\n",
    "{ k: f\"{round(np.mean(v), 2)} {round(np.std(v), 2)}\" for k, v in metrics.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6132ef0",
   "metadata": {},
   "source": [
    "# T-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_t: Final[str] = \"treated\"\n",
    "label_nt: Final[str] = \"non-treated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = x_all\n",
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(15)):\n",
    "    # seed = 1\n",
    "    np.random.seed(seed)\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed, n_total=x_len)\n",
    "    y_np = data_y_fact.to_numpy()\n",
    "    xq, xb, xv = x.iloc[query_indices], x.iloc[back_indices], x.iloc[val_indices]\n",
    "    yq, yb = data_y_fact.iloc[query_indices], data_y_fact.iloc[back_indices]\n",
    "    tq, tb = data_treatment[query_indices], data_treatment[back_indices]\n",
    "\n",
    "    xt_q, yt_q, xnt_q, ynt_q = split_treated_non_treated(xq, tq, yq)\n",
    "    xt_b, yt_b, xnt_b, ynt_b = split_treated_non_treated(xb, tb, yb)\n",
    "\n",
    "    iq_t = np.array([i for i in query_indices if data_treatment[i] == 1])\n",
    "    iq_nt = np.array([i for i in query_indices if data_treatment[i] == 0])\n",
    "\n",
    "    ib_t = np.array([i for i in back_indices if data_treatment[i] == 1])\n",
    "    ib_nt = np.array([i for i in back_indices if data_treatment[i] == 0])\n",
    "\n",
    "    data_y_cfact = ihdp_data[y_cfact_colname]\n",
    "    yv_t = np.array([data_y_fact[i] if data_treatment[i] == 1 else data_y_cfact[i] for i in val_indices])\n",
    "    yv_nt = np.array([data_y_fact[i] if data_treatment[i] == 0 else data_y_cfact[i] for i in val_indices])\n",
    "\n",
    "    i_train_t = np.concatenate([ib_t, iq_t])\n",
    "    i_train_nt = np.concatenate([ib_nt, iq_nt])\n",
    "\n",
    "    r_q_b_treated = r[iq_t][:, ib_t]\n",
    "    r_q_b_nt = r[iq_nt][:, ib_nt]\n",
    "    r_val_nvt = r[val_indices][:, i_train_t]  # rel between val and treated train\n",
    "    r_val_nvnt = r[val_indices][:, i_train_nt]  # rel between val and non-treated train\n",
    "\n",
    "    nw_broad_key: Final[str] = \"nw_rel-as-features\"\n",
    "    # trained_models = {\n",
    "    #     \"rel=True\": {},\n",
    "    #     \"rel=False\": {},\n",
    "    #     nw_broad_key: {},\n",
    "    # }\n",
    "    trained_models = defaultdict(dict)\n",
    "\n",
    "    y_pred_learn_norm = {}\n",
    "    for metrics_key in (\"mlp\", \"nw_learn_norm\"):\n",
    "        for xqi, xbi, yqi, ybi, yvi, r_q_b, r_v_nvi, label in (\n",
    "            (xt_q, xt_b, yt_q, yt_b, yv_t, r_q_b_treated, r_val_nvt, label_t),\n",
    "            (xnt_q, xnt_b, ynt_q, ynt_b, yv_nt, r_q_b_nt, r_val_nvnt, label_nt),\n",
    "        ):\n",
    "            n_feats = len(xqi[0])\n",
    "            torch.manual_seed(seed)\n",
    "            x_back, x_query, x_val  = to_tensor(xbi), to_tensor(xqi), to_tensor(xv.to_numpy())\n",
    "            y_back, y_query, y_val = to_tensor(ybi), to_tensor(yqi), to_tensor(yvi)\n",
    "            x_train, y_train = torch.cat([x_back, x_query]), torch.cat([y_back, y_query])\n",
    "            \n",
    "            mlp_out_dim = 40\n",
    "            config = NwModelConfig(\n",
    "                    init_sigma=1.0,\n",
    "                    init_r_scale=1.0,\n",
    "                    input_dim=mlp_out_dim if metrics_key == \"mlp\" else n_feats,\n",
    "                    trainable_weights_matrix=True if metrics_key == \"nw_learn_norm\" else False,\n",
    "                    mlp_config= MlpConfig(\n",
    "                        in_dim=n_feats,\n",
    "                        out_dim=mlp_out_dim,\n",
    "                        hidden_dim=78,\n",
    "                        dropout=.534,\n",
    "                    ) if metrics_key == \"mlp\" else None,\n",
    "            )\n",
    "            model = RelNwRegr(config)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                        #   lr=.014, \n",
    "                                          lr=1e-3,\n",
    "                                          )\n",
    "            loss_fn = nn.MSELoss()\n",
    "            torch.manual_seed(seed)\n",
    "            model.train()\n",
    "\n",
    "            n_epochs = 50\n",
    "            for _ in range(n_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_back, y_back, x_query, to_tensor(r_q_b))\n",
    "                loss = loss_fn(y_pred, y_query)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_train, y_train, x_val, to_tensor(r_v_nvi))\n",
    "                y_pred_learn_norm[label] = y_pred\n",
    "        tau_pred_learn_norm = y_pred_learn_norm[label_t] - y_pred_learn_norm[label_nt]\n",
    "        tau_true = yv_t - yv_nt\n",
    "        metrics[metrics_key].append(mean_squared_error(tau_true, tau_pred_learn_norm.numpy())) \n",
    "         \n",
    "\n",
    "    # for rel, (xqi, xbi, yqi, ybi, yvi, r_q_b, r_v_nvi, label) in product(\n",
    "    #     (True, False),\n",
    "    #     (\n",
    "    #         (xt_q, xt_b, yt_q, yt_b, yv_t, r_q_b_treated, r_val_nvt, label_t),\n",
    "    #         (xnt_q, xnt_b, ynt_q, ynt_b, yv_nt, r_q_b_nt, r_val_nvnt, label_nt),\n",
    "    #     ),\n",
    "    # ):\n",
    "    #     _, _, _, model = train_nw_arbitrary(\n",
    "    #         x_backgnd=xbi,\n",
    "    #         y_backgnd=ybi,\n",
    "    #         x_query=xqi,\n",
    "    #         y_query=yqi,\n",
    "    #         x_val=xv.to_numpy(),\n",
    "    #         y_val=yvi,\n",
    "    #         r_query_backgnd=r_q_b if rel else np.zeros_like(r_q_b),\n",
    "    #         r_val_nonval=r_v_nvi if rel else np.zeros_like(r_v_nvi),\n",
    "    #         cfg=NwModelConfig(input_dim=len(xbi[0])),\n",
    "    #         lr=lr,\n",
    "    #         n_epochs=n_epochs,\n",
    "    #     )\n",
    "    #     trained_models[f\"rel={rel}\"][label] = model\n",
    "\n",
    "    # # rel as features\n",
    "    # x_broad = np.concatenate((x.to_numpy(), r), axis=1)\n",
    "    # xb_broad, xq_broad, xv_broad = x_broad[back_indices], x_broad[query_indices], x_broad[val_indices]\n",
    "    # xt_q_broad, xnt_q_broad = x_broad[iq_t], x_broad[iq_nt]\n",
    "    # xt_b_broad, xnt_b_broad = x_broad[ib_t], x_broad[ib_nt]\n",
    "    # xv_broad = x_broad[val_indices]\n",
    "\n",
    "    # for (xqi, xbi, yqi, ybi, yvi, label) in (\n",
    "    #     (xt_q_broad, xt_b_broad, yt_q, yt_b, yv_t, label_t),\n",
    "    #     (xnt_q_broad, xnt_b_broad, ynt_q, ynt_b, yv_nt, label_nt),\n",
    "    # ):\n",
    "    #     trained_models[nw_broad_key][label] = train_nw_arbitrary(\n",
    "    #         x_backgnd=xbi,\n",
    "    #         y_backgnd=ybi,\n",
    "    #         x_query=xqi,\n",
    "    #         y_query=yqi,\n",
    "    #         x_val=xv_broad,\n",
    "    #         y_val=yvi,\n",
    "    #         r_query_backgnd=np.zeros((len(xqi), len(xbi))),\n",
    "    #         r_val_nonval=np.zeros((len(xv_broad), len(xbi) + len(xqi))),\n",
    "    #         cfg=NwModelConfig(input_dim=len(xbi[0])),\n",
    "    #         lr=lr,\n",
    "    #         n_epochs=n_epochs,\n",
    "    #     )[-1]\n",
    "\n",
    "    # # LightGBM\n",
    "    # yt_train, ynt_train = np.concatenate([yt_q, yt_b]), np.concatenate([ynt_q, ynt_b])\n",
    "    # for xq_ti, xb_ti, xq_nti, xb_nti, xv_i, lgb_key in (\n",
    "    #     (xt_q, xt_b, xnt_q, xnt_b, xv, \"lgb\"),\n",
    "    #     # (xt_q_broad, xt_b_broad, xnt_q_broad, xnt_b_broad, xv_broad, \"lgb-rel\"),\n",
    "    # ):\n",
    "    #     xt_train, xnt_train = np.concatenate([xq_ti, xb_ti]), np.concatenate([xq_nti, xb_nti])\n",
    "    #     lgb_model_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=yt_train))\n",
    "    #     lgb_model_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, ynt_train))\n",
    "    #     tau_lgb = lgb_model_t.predict(xv_i) - lgb_model_nt.predict(xv_i)\n",
    "    #     metrics[lgb_key].append(mean_squared_error(tau_true, tau_lgb))\n",
    "\n",
    "    # # TabRel\n",
    "    # def train_relnet_shorthand(x_: np.ndarray, y_: np.ndarray, r_: np.ndarray, \n",
    "    #                             bi_: np.ndarray, qi_: np.ndarray, vi_: np.ndarray) -> torch.Tensor:\n",
    "    #     _, _, _, y_pred, _ = train_relnet(\n",
    "    #         x=x_,\n",
    "    #         y=y_,\n",
    "    #         r=r_,\n",
    "    #         backgnd_indices=bi_,\n",
    "    #         query_indices=qi_,\n",
    "    #         val_indices=vi_,\n",
    "    #         lr=.007,\n",
    "    #         n_epochs=1500,\n",
    "    #         n_layers=2,\n",
    "    #         periodic_embed_dim=None,\n",
    "    #         embed_dim=32,\n",
    "    #         num_heads=2,\n",
    "    #         progress_bar=True,\n",
    "    #     )\n",
    "    #     return y_pred\n",
    "\n",
    "    # torch.manual_seed(seed)\n",
    "    # xt = np.concatenate([xt_b, xt_q, xv])\n",
    "    # bi_t = np.array(range(len(xt_b)))\n",
    "    # qi_t = np.array(range(len(xt_q))) + len(xt_b)\n",
    "    # vi_t = np.array(range(len(xv))) + len(xt_b) + len(xt_q)\n",
    "\n",
    "    # xnt = np.concatenate([xnt_b, xnt_q, xv])\n",
    "    # yt = np.concatenate([yt_b, yt_q, yv_t])\n",
    "    # ynt = np.concatenate([ynt_b, ynt_q, yv_nt])\n",
    "    # bi_nt = np.array(range(len(xnt_b)))\n",
    "    # qi_nt = np.array(range(len(xnt_q))) + len(xnt_b)\n",
    "    # vi_nt = np.array(range(len(xv))) + len(xnt_b) + len(xnt_q)\n",
    "\n",
    "    # i_t = np.concatenate([ib_t, iq_t, val_indices])\n",
    "    # i_nt = np.concatenate([ib_nt, iq_nt, val_indices])\n",
    "    # r_bqv_t = r[i_t][:, i_t]\n",
    "    # r_bqv_nt = r[i_nt][:, i_nt]\n",
    "    # y_pred_relnet_t = train_relnet_shorthand(x_=xt, y_=yt, r_=r_bqv_t, bi_=bi_t, qi_=qi_t, vi_=vi_t)\n",
    "    # y_pred_relnet_nt = train_relnet_shorthand(x_=xnt, y_=ynt, r_=r_bqv_nt, bi_=bi_nt, qi_=qi_nt, vi_=vi_nt)\n",
    "    # tau_pred_relnet = y_pred_relnet_t - y_pred_relnet_nt\n",
    "\n",
    "    # metrics[\"relnet\"].append(mean_squared_error(tau_true, tau_pred_relnet))\n",
    "\n",
    "    for key, models in trained_models.items():\n",
    "        y_pred_treated = models[label_t].y_val_pred\n",
    "        y_pred_nt = models[label_nt].y_val_pred\n",
    "\n",
    "        tau_pred = y_pred_treated - y_pred_nt\n",
    "        metrics[key].append(mean_squared_error(tau_true, tau_pred)) # PEHE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=.014: {'mlp': '4.54 & 0.49', 'nw_learn_norm': '5.97 & 1.00', 'lgb': '3.62 & 0.26'}\n",
    "# lr=1e-3: {'mlp': '4.07 & 0.33', 'nw_learn_norm': '4.74 & 0.38', 'lgb': '3.62 & 0.26'}\n",
    "# FIXED B, Q order, lr=1e-3: {'mlp': '3.72 & 0.31', 'nw_learn_norm': '4.57 & 0.44', 'lgb': '3.62 & 0.26'}\n",
    "# FIXED BQ, lr=1e-3, n_epochs=200 (not 500): {'mlp': '3.49 & 0.42', 'nw_learn_norm': '6.24 & 0.46'}\n",
    "# FIXED BQ, lr=1e-3, n_epochs=150: {'mlp': '3.38 & 0.42', 'nw_learn_norm': '6.43 & 0.46'}\n",
    "# FIXED BQ, lr=1e-3, n_epochs=100: {'mlp': '3.31 & 0.34', 'nw_learn_norm': '6.60 & 0.45'}\n",
    "# FIXED BQ, lr=1e-3, n_epochs=50: {'mlp': '5.19 & 0.62', 'nw_learn_norm': '6.75 & 0.43'}\n",
    "\n",
    "{k: f\"{np.mean(v):.2f} & {np.std(v):.2f}\" for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175af792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.ones(len(metrics[\"relnet\"])), metrics[\"relnet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std([m for m in metrics[\"relnet\"] if m < 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d839a85",
   "metadata": {},
   "source": [
    "# X-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90713996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_relnet_shorthand(x_b_: np.ndarray, x_q_: np.ndarray, x_v_: np.ndarray,\n",
    "                           y_b_: np.ndarray, y_q_: np.ndarray, \n",
    "                           r_: np.ndarray, # r: (q, b, v) x (q, b, v)\n",
    "                           seed: int) -> torch.Tensor:\n",
    "        \n",
    "        x_ = np.concatenate([x_q_, x_b_,  x_v_])\n",
    "        y_ = np.concatenate([y_q_, y_b_, np.zeros(len(x_v_))])\n",
    "        qi_ = np.array(range(len(x_q_)))\n",
    "        bi_ = np.array(range(len(x_q_))) + len(x_q_)\n",
    "        vi_ = np.array(range(len(x_v_))) + len(x_q_) + len(x_b_)\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        _, _, _, y_pred, _ = train_relnet(\n",
    "            x=x_,\n",
    "            y=y_,\n",
    "            r=r_,\n",
    "            backgnd_indices=bi_,\n",
    "            query_indices=qi_,\n",
    "            val_indices=vi_,\n",
    "            lr=.007,\n",
    "            n_epochs=1000,\n",
    "            n_layers=2,\n",
    "            periodic_embed_dim=None,\n",
    "            embed_dim=32,\n",
    "            num_heads=2,\n",
    "            progress_bar=False,\n",
    "        )\n",
    "        return y_pred\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(15)):\n",
    "    # seed = 1\n",
    "    np.random.seed(seed)\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed, n_total=x_len)\n",
    "    y_np = data_y_fact.to_numpy()\n",
    "    xq, xb, xv = x.iloc[query_indices], x.iloc[back_indices], x.iloc[val_indices]\n",
    "    yq, yb = data_y_fact.iloc[query_indices], data_y_fact.iloc[back_indices]\n",
    "    tq, tb = data_treatment[query_indices], data_treatment[back_indices]\n",
    "\n",
    "    xt_q, yt_q, xnt_q, ynt_q = split_treated_non_treated(xq, tq, yq)\n",
    "    xt_b, yt_b, xnt_b, ynt_b = split_treated_non_treated(xb, tb, yb)\n",
    "\n",
    "    iq_t = np.array([i for i in query_indices if data_treatment[i] == 1])\n",
    "    iq_nt = np.array([i for i in query_indices if data_treatment[i] == 0])\n",
    "\n",
    "    ib_t = np.array([i for i in back_indices if data_treatment[i] == 1])\n",
    "    ib_nt = np.array([i for i in back_indices if data_treatment[i] == 0])\n",
    "\n",
    "    data_y_cfact = ihdp_data[y_cfact_colname]\n",
    "    yv_t = np.array([data_y_fact[i] if data_treatment[i] == 1 else data_y_cfact[i] for i in val_indices])\n",
    "    yv_nt = np.array([data_y_fact[i] if data_treatment[i] == 0 else data_y_cfact[i] for i in val_indices])\n",
    "\n",
    "    i_train_t = np.concatenate([iq_t, ib_t]) # TODO check order\n",
    "    i_train_nt = np.concatenate([iq_nt, ib_nt])\n",
    "\n",
    "    r_q_b_treated = r[iq_t][:, ib_t]\n",
    "    r_q_b_nt = r[iq_nt][:, ib_nt]\n",
    "    r_val_nvt = r[val_indices][:, i_train_t]  # rel between val and treated train\n",
    "    r_val_nvnt = r[val_indices][:, i_train_nt]  # rel between val and non-treated train\n",
    "\n",
    "    label_t: Final[str] = \"treated\"\n",
    "    label_nt: Final[str] = \"non-treated\"\n",
    "\n",
    "    # rel as features\n",
    "    x_broad = np.concatenate((x.to_numpy(), r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[back_indices], x_broad[query_indices], x_broad[val_indices]\n",
    "    xt_q_broad, xnt_q_broad = x_broad[iq_t], x_broad[iq_nt]\n",
    "    xt_b_broad, xnt_b_broad = x_broad[ib_t], x_broad[ib_nt]\n",
    "    xv_broad = x_broad[val_indices]\n",
    "\n",
    "    # for LightGBM\n",
    "    tau_true_val = yv_t - yv_nt\n",
    "    yt_train, ynt_train = np.concatenate([yt_q, yt_b]), np.concatenate([ynt_q, ynt_b])\n",
    "    \n",
    "    lgb_usual_key: Final[str] = \"lgb\"\n",
    "    lgb_broad_key: Final[str] = \"lgb-rel\"\n",
    "\n",
    "    tau_train = {\n",
    "        lgb_broad_key:\n",
    "        {\n",
    "            \"t\": {\"q\": None, \"b\": None},\n",
    "            \"nt\": {\"q\": None, \"b\": None},\n",
    "        },\n",
    "        lgb_usual_key:\n",
    "        {\n",
    "            \"t\": {\"q\": None, \"b\": None},\n",
    "            \"nt\": {\"q\": None, \"b\": None},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for xq_ti, xb_ti, xq_nti, xb_nti, xv_i, lgb_key in (\n",
    "        (xt_q, xt_b, xnt_q, xnt_b, xv, lgb_usual_key),\n",
    "        (xt_q_broad, xt_b_broad, xnt_q_broad, xnt_b_broad, xv_broad, lgb_broad_key),\n",
    "    ):\n",
    "        xt_train, xnt_train = np.concatenate([xq_ti, xb_ti]), np.concatenate([xq_nti, xb_nti])\n",
    "        lgb_model_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=yt_train))\n",
    "        lgb_model_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, label=ynt_train))\n",
    "\n",
    "        # impute treatment effects using first-stage learners\n",
    "\n",
    "        yt_train_cfact_lgb = lgb_model_nt.predict(xt_train)\n",
    "        tau_train_t = yt_train - yt_train_cfact_lgb\n",
    "        n_query_t = len(xq_ti)\n",
    "        tau_train[lgb_key][\"t\"] = {\"q\": tau_train_t[:n_query_t], \"b\": tau_train_t[n_query_t:]}\n",
    "\n",
    "        ynt_train_cfact_lgb = lgb_model_t.predict(xnt_train)\n",
    "        tau_train_nt =  ynt_train_cfact_lgb - ynt_train\n",
    "        n_query_nt = len(xq_nti)\n",
    "        tau_train[lgb_key][\"nt\"] = {\"q\": tau_train_nt[:n_query_nt], \"b\": tau_train_nt[n_query_nt:]}\n",
    "\n",
    "    \n",
    "    # stage 2\n",
    "    for train_weights, is_rel, is_mlp, (xq_ti, xb_ti, xq_nti, xb_nti, xv_i, lgb_key), in product(\n",
    "        (True, False), \n",
    "        (True, False), \n",
    "        (True, False),\n",
    "    (\n",
    "        (xt_q, xt_b, xnt_q, xnt_b, xv.to_numpy(), lgb_usual_key),\n",
    "        (xt_q_broad, xt_b_broad, xnt_q_broad, xnt_b_broad, xv_broad, lgb_broad_key),\n",
    "    )):\n",
    "        if lgb_key == lgb_broad_key and (train_weights or is_rel or is_mlp):\n",
    "            continue\n",
    "        if is_mlp and (train_weights or not is_rel):\n",
    "             continue\n",
    "\n",
    "        xt_train, xnt_train = np.concatenate([xq_ti, xb_ti]), np.concatenate([xq_nti, xb_nti])\n",
    "\n",
    "        # retrieve treatment effects imputed by first-stage learners\n",
    "\n",
    "        taus_t = tau_train[lgb_key][\"t\"]\n",
    "        tau_train_t = np.concatenate([taus_t[\"q\"], taus_t[\"b\"]])\n",
    "\n",
    "        taus_nt = tau_train[lgb_key][\"nt\"]\n",
    "        tau_train_nt =  np.concatenate([taus_nt[\"q\"], taus_nt[\"b\"]])\n",
    "\n",
    "        # stage 2 learners: LightGBM\n",
    "        # if not train_weights and not is_rel: # just do it once per lgb_key\n",
    "        #     lgb_model_tau_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=tau_train_t))\n",
    "        #     lgb_model_tau_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, label=tau_train_nt))\n",
    "\n",
    "        #     tau_lgb = .5 * (lgb_model_tau_t.predict(xv_i) + lgb_model_tau_nt.predict(xv_i))\n",
    "        #     metrics[lgb_key].append(mean_squared_error(tau_true, tau_lgb))\n",
    "\n",
    "        # stage 2 learners: NW\n",
    "        taus_t_q, taus_t_b = taus_t[\"q\"], taus_t[\"b\"]\n",
    "        taus_nt_q, taus_nt_b = taus_nt[\"q\"], taus_nt[\"b\"]\n",
    "\n",
    "        cfg = NwModelConfig(\n",
    "             input_dim=len(xb_ti[0]),\n",
    "             trainable_weights_matrix=train_weights,\n",
    "             mlp_config=MlpConfig(\n",
    "                        in_dim=n_feats,\n",
    "                        out_dim=mlp_out_dim,\n",
    "                        hidden_dim=78,\n",
    "                        dropout=.534,\n",
    "                    ) if is_mlp else None\n",
    "             )\n",
    "        n_epochs = 800\n",
    "        _, _, _, model_tau_t = train_nw_arbitrary(\n",
    "            x_backgnd=xb_ti,\n",
    "            y_backgnd=taus_t_b,\n",
    "            x_query=xq_ti,\n",
    "            y_query=taus_t_q,\n",
    "            x_val=xv_i,\n",
    "            y_val=np.zeros(len(xv_i)),\n",
    "            r_query_backgnd= r_q_b_treated if is_rel else np.zeros_like(r_q_b_treated),\n",
    "            r_val_nonval=np.zeros_like(r_val_nvt),\n",
    "            cfg=cfg, # TODO trainable weight matrix\n",
    "            n_epochs=n_epochs,\n",
    "            lr=1e-3,\n",
    "        )\n",
    "\n",
    "        _, _, _, model_tau_nt = train_nw_arbitrary(\n",
    "            x_backgnd=xb_nti,\n",
    "            y_backgnd=taus_nt_b,\n",
    "            x_query=xq_nti,\n",
    "            y_query=taus_nt_q,\n",
    "            x_val=xv_i,\n",
    "            y_val=np.zeros(len(xv_i)),\n",
    "            r_query_backgnd = r_q_b_nt if is_rel else np.zeros_like(r_q_b_nt),\n",
    "            r_val_nonval=np.zeros_like(r_val_nvnt),\n",
    "            cfg=cfg,\n",
    "            n_epochs=n_epochs,\n",
    "            lr=1e-3,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            tau_pred1 = model_tau_t.model(\n",
    "                to_tensor(xt_train),\n",
    "                to_tensor(tau_train_t), \n",
    "                to_tensor(xv_i),\n",
    "                to_tensor(r_val_nvt if is_rel else np.zeros_like(r_val_nvt)),\n",
    "                )\n",
    "            tau_pred0 = model_tau_nt.model(\n",
    "                to_tensor(xnt_train),\n",
    "                to_tensor(tau_train_nt),\n",
    "                to_tensor(xv_i),\n",
    "                to_tensor(r_val_nvnt if is_rel else np.zeros_like(r_val_nvnt)),\n",
    "            )\n",
    "        tau_pred = .5 * (tau_pred0 + tau_pred1)\n",
    "        metrics[f\"NW-rel{is_rel}-broad{lgb_key == lgb_broad_key}-trainWeights{train_weights}_mlp{is_mlp}\"].append(mean_squared_error(tau_true_val, tau_pred))\n",
    "\n",
    "        # TabRel\n",
    "        if train_weights or is_rel or lgb_key == lgb_broad_key:\n",
    "             continue\n",
    "        \n",
    "        i_treated = np.concatenate([ib_t, iq_t, val_indices])  # absolute indexation\n",
    "        i_non_treated = np.concatenate([ib_nt, iq_nt, val_indices])\n",
    "        tau_pred1 = train_relnet_shorthand(\n",
    "             x_b_ = xb_ti,\n",
    "             x_q_ = xq_ti,\n",
    "             x_v_ = xv_i,\n",
    "             y_b_ = taus_t_b,\n",
    "             y_q_ = taus_t_q,\n",
    "             r_ = r[i_treated][:, i_treated],\n",
    "             seed=seed,\n",
    "        )\n",
    "        tau_pred0 = train_relnet_shorthand(\n",
    "             x_b_ = xb_nti,\n",
    "             x_q_ = xq_nti,\n",
    "             x_v_ = xv_i,\n",
    "             y_b_ = taus_nt_b,\n",
    "             y_q_ = taus_nt_q,\n",
    "             r_ = r[i_non_treated][:, i_non_treated],\n",
    "             seed=seed,\n",
    "        )\n",
    "        tau_pred_relnet = .5 * (tau_pred1 + tau_pred0)\n",
    "\n",
    "        metrics[\"relnet\"].append(mean_squared_error(tau_true_val, tau_pred_relnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 800 iterations, B, Q order as before: ---\n",
    "# {'NW-relTrue-broadFalse-trainWeightsTrue_mlpFalse': '5.11 & 0.30',\n",
    "#  'NW-relFalse-broadFalse-trainWeightsTrue_mlpFalse': '5.35 & 0.40',\n",
    "#  'NW-relTrue-broadFalse-trainWeightsFalse_mlpTrue': '4.14 & 0.53',\n",
    "#  'NW-relTrue-broadFalse-trainWeightsFalse_mlpFalse': '6.33 & 1.44',\n",
    "#  'NW-relFalse-broadFalse-trainWeightsFalse_mlpFalse': '6.69 & 1.50',\n",
    "#  'NW-relFalse-broadTrue-trainWeightsFalse_mlpFalse': '6.89 & 1.93'}\n",
    "\n",
    "# --- 200 iterations, B, Q order as before: ---\n",
    "# {'NW-relTrue-broadFalse-trainWeightsTrue_mlpFalse': '5.31 & 0.28',\n",
    "#  'NW-relFalse-broadFalse-trainWeightsTrue_mlpFalse': '5.91 & 1.51',\n",
    "#  'NW-relTrue-broadFalse-trainWeightsFalse_mlpTrue': '4.27 & 0.45',\n",
    "#  'NW-relTrue-broadFalse-trainWeightsFalse_mlpFalse': '6.09 & 1.39',\n",
    "#  'NW-relFalse-broadFalse-trainWeightsFalse_mlpFalse': '6.27 & 1.51',\n",
    "#  'NW-relFalse-broadTrue-trainWeightsFalse_mlpFalse': '6.64 & 1.89'}\n",
    "\n",
    "# --- 800 iterations, changed B, Q order: --- TODO why so? not everywhere changed in Stage 1!\n",
    "# {'NW-relTrue-broadFalse-trainWeightsFalse_mlpTrue': '8.30 & 1.02'}\n",
    "\n",
    "{k: f\"{np.mean(v):.2f} & {np.std(v):.2f}\" for k, v in metrics.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
