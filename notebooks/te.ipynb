{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_ihdp_data(ihdp_path: Path) -> tuple[pd.DataFrame, list[str], str]:\n",
    "    ihdp_cols = [s[:-1] for s in np.loadtxt(ihdp_path / \"columns.txt\", dtype=str)][:-2]\n",
    "    ihdp_cols.extend([f\"x{i}\" for i in range(2, 26)])\n",
    "\n",
    "    csvs = []\n",
    "    for csv_path in (ihdp_path / \"csv\").glob(\"*.csv\"):\n",
    "        csvs.append(pd.read_csv(csv_path, header=None))\n",
    "        break # TODO choose a table, for now using the first table\n",
    "    data = pd.concat(csvs)\n",
    "    data.columns = ihdp_cols\n",
    "\n",
    "    tau_col_name = \"delta_y\"\n",
    "    data[tau_col_name] = (data[\"y_cfactual\"] - data[\"y_factual\"]) * (-1) ** data[\"treatment\"]\n",
    "    exclude_cols = [\"treatment\", \"y_cfactual\", \"y_factual\", \"mu0\", \"mu1\"]\n",
    "    return data, exclude_cols, tau_col_name\n",
    "\n",
    "ihdp_data, ihdp_exclude_cols, ihdp_tau_colname = load_ihdp_data(Path(\"/Users/vzuev/Documents/git/gh_zuevval/tabrel/CEVAE/datasets/IHDP\"))\n",
    "ihdp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "\n",
    "x_all = ihdp_data.drop(columns=ihdp_exclude_cols + [ihdp_tau_colname])\n",
    "\n",
    "ihdp_last_numeric_index: Final[int] = 6\n",
    "x_numeric = x_all.iloc[:, :ihdp_last_numeric_index]\n",
    "x_cat = x_all.iloc[:, ihdp_last_numeric_index:]\n",
    "x_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x_num_y = x_numeric.copy()\n",
    "x_num_y[ihdp_tau_colname] = ihdp_data[ihdp_tau_colname]\n",
    "sns.pairplot(x_num_y, hue=ihdp_tau_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "group_col: Final[str] = \"x4\"\n",
    "x = x_all.drop(columns=[group_col])\n",
    "x_len = len(x)\n",
    "categories = x_all[group_col]\n",
    "print(\"n_categories\", len(categories.unique()))\n",
    "\n",
    "r = np.zeros((x_len, x_len))\n",
    "for i, j in tqdm(list(product(range(x_len), range(x_len)))):\n",
    "    if np.isclose(categories[i], categories[j]):\n",
    "        r[i, j] = 1\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78411c2b",
   "metadata": {},
   "source": [
    "# S-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tabrel.benchmark.nw_regr import run_training, metrics_mean, train_nw_arbitrary, NwModelConfig\n",
    "from tabrel.train import train_relnet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "labels = [\"rel\", \"nrel\", \"lgb\", \"rel-fts\", \"lgb-rel\"]\n",
    "\n",
    "def generate_indices(seed: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(x_len)\n",
    "    n_query, n_back = 200, 300\n",
    "    q_indices = indices[:n_query]\n",
    "    b_indices = indices[n_query:n_back]\n",
    "    v_indices = indices[n_back:]\n",
    "    return q_indices, b_indices, v_indices\n",
    "\n",
    "def split_treated_non_treated(x: pd.DataFrame, treatment: np.ndarray, y_fact: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    treated = treatment == 1\n",
    "    x_treated, y_treated = x.loc[treated], y_fact.loc[treated]\n",
    "    x_non_treated, y_non_treated = x.loc[~treated], y_fact.loc[~treated]\n",
    "    return x_treated.to_numpy(), y_treated.to_numpy(), x_non_treated.to_numpy(), y_non_treated.to_numpy()\n",
    "\n",
    "y_fact_colname, y_cfact_colname = \"y_factual\", \"y_cfactual\"\n",
    "data_y_fact, data_treatment = ihdp_data[y_fact_colname], ihdp_data[\"treatment\"]\n",
    "\n",
    "x_s = x # x for S-learner\n",
    "x_s[\"treatment\"] = data_treatment\n",
    "x_s_np = x_s.to_numpy()\n",
    "y_s = data_y_fact.to_numpy() # Y for S-learner\n",
    "\n",
    "model_cfg, lr, n_epochs = NwModelConfig(), 1e-3, 50\n",
    "lgb_params: Final[dict[str, str | int]] = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(20)):\n",
    "    np.random.seed(seed)\n",
    "    ids_q, ids_b, ids_v = generate_indices(seed)\n",
    "    ids_train = np.concatenate((ids_q, ids_b))\n",
    "    xb, yb, xq, yq = x_s_np[ids_b], y_s[ids_b], x_s_np[ids_q], y_s[ids_q]\n",
    "    xv, yv = x_s_np[ids_v], y_s[ids_v]\n",
    "    r_q_b = r[ids_q][:, ids_b]\n",
    "    r_val_train = r[ids_v][:, ids_train]\n",
    "\n",
    "    # TabRel\n",
    "    relnet_pehe, _, _ = train_relnet(\n",
    "        x=x_s_np,\n",
    "        y=y_s,\n",
    "        r=r,\n",
    "        backgnd_indices=ids_b,\n",
    "        query_indices=ids_q,\n",
    "        val_indices=ids_v,\n",
    "        lr=0.01,\n",
    "        n_epochs=800,\n",
    "        n_layers=2,\n",
    "        periodic_embed_dim=None,\n",
    "        num_heads=2,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "    metrics[\"relnet\"].append(relnet_pehe)\n",
    "\n",
    "    # NW with rel\n",
    "    _, _, _, model_rel = train_nw_arbitrary(\n",
    "        x_backgnd=xb,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq,\n",
    "        y_query=yq,\n",
    "        x_val=xv,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=r_q_b,\n",
    "        r_val_nonval=r_val_train,\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics[\"rel\"].append(mean_squared_error(yv, model_rel.y_val_pred))\n",
    "\n",
    "    # NW without rel\n",
    "    _, _, _, model_nrel = train_nw_arbitrary(\n",
    "        x_backgnd=xb,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq,\n",
    "        y_query=yq,\n",
    "        x_val=xv,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=np.zeros_like(r_q_b),\n",
    "        r_val_nonval=np.zeros_like(r_val_train),\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics[\"nrel\"].append(mean_squared_error(yv, model_nrel.y_val_pred))\n",
    "\n",
    "    # LightGBM\n",
    "    x_train = np.concatenate([xq, xb])\n",
    "    y_train = np.concatenate([yq, yb])\n",
    "    lgb_model = lgb.train(lgb_params, lgb.Dataset(x_train, label=y_train))\n",
    "    y_pred_lgb = lgb_model.predict(xv)\n",
    "    metrics[\"lgb\"].append(mean_squared_error(yv, y_pred_lgb))\n",
    "\n",
    "    # NW and LGB with rel as features\n",
    "    x_broad = np.concatenate((x_s_np, r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[ids_b], x_broad[ids_q], x_broad[ids_v]\n",
    "\n",
    "    # NW with rel as features\n",
    "    _, _, _, model_relfts = train_nw_arbitrary(\n",
    "        x_backgnd=xb_broad,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq_broad,\n",
    "        y_query=yq,\n",
    "        x_val=xv_broad,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=np.zeros((len(xq_broad), len(xb_broad))),\n",
    "        r_val_nonval=np.zeros((len(xv_broad), len(xb_broad) + len(xq_broad))),\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics[\"rel-fts\"].append(mean_squared_error(yv, model_relfts.y_val_pred))\n",
    "\n",
    "    # LightGBM with rel as features\n",
    "    x_train_broad = np.concatenate([xq_broad, xb_broad])\n",
    "    lgb_model_rel = lgb.train(lgb_params, lgb.Dataset(x_train_broad, label=y_train))\n",
    "    y_pred_lgb_rel = lgb_model_rel.predict(xv_broad)\n",
    "    metrics[\"lgb-rel\"].append(mean_squared_error(yv, y_pred_lgb_rel))\n",
    "\n",
    "# Show mean metrics\n",
    "{ k: round(np.mean(v), 2) for k, v in metrics.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6132ef0",
   "metadata": {},
   "source": [
    "# T-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(20)):\n",
    "    np.random.seed(seed)\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed)\n",
    "    y_np = data_y_fact.to_numpy()\n",
    "    xq, xb, xv = x.iloc[query_indices], x.iloc[back_indices], x.iloc[val_indices]\n",
    "    yq, yb = data_y_fact.iloc[query_indices], data_y_fact.iloc[back_indices]\n",
    "    tq, tb = data_treatment[query_indices], data_treatment[back_indices]\n",
    "\n",
    "    xt_q, yt_q, xnt_q, ynt_q = split_treated_non_treated(xq, tq, yq)\n",
    "    xt_b, yt_b, xnt_b, ynt_b = split_treated_non_treated(xb, tb, yb)\n",
    "\n",
    "    iq_t = np.array([i for i in query_indices if data_treatment[i] == 1])\n",
    "    iq_nt = np.array([i for i in query_indices if data_treatment[i] == 0])\n",
    "\n",
    "    ib_t = np.array([i for i in back_indices if data_treatment[i] == 1])\n",
    "    ib_nt = np.array([i for i in back_indices if data_treatment[i] == 0])\n",
    "\n",
    "    data_y_cfact = ihdp_data[y_cfact_colname]\n",
    "    yv_t = np.array([data_y_fact[i] if data_treatment[i] == 1 else data_y_cfact[i] for i in val_indices])\n",
    "    yv_nt = np.array([data_y_fact[i] if data_treatment[i] == 0 else data_y_cfact[i] for i in val_indices])\n",
    "\n",
    "    i_train_t = np.concatenate([iq_t, ib_t])\n",
    "    i_train_nt = np.concatenate([iq_nt, ib_nt])\n",
    "\n",
    "    r_q_b_treated = r[iq_t][:, ib_t]\n",
    "    r_q_b_nt = r[iq_nt][:, ib_nt]\n",
    "    r_val_nvt = r[val_indices][:, i_train_t]  # rel between val and treated train\n",
    "    r_val_nvnt = r[val_indices][:, i_train_nt]  # rel between val and non-treated train\n",
    "\n",
    "    label_t: Final[str] = \"treated\"\n",
    "    label_nt: Final[str] = \"non-treated\"\n",
    "\n",
    "    nw_broad_key: Final[str] = \"nw_rel-as-features\"\n",
    "    trained_models = {\n",
    "        \"rel=True\": {},\n",
    "        \"rel=False\": {},\n",
    "        nw_broad_key: {},\n",
    "    }\n",
    "    for rel, (xqi, xbi, yqi, ybi, yvi, r_q_b, r_v_nvi, label) in product(\n",
    "        (True, False),\n",
    "        (\n",
    "            (xt_q, xt_b, yt_q, yt_b, yv_t, r_q_b_treated, r_val_nvt, label_t),\n",
    "            (xnt_q, xnt_b, ynt_q, ynt_b, yv_nt, r_q_b_nt, r_val_nvnt, label_nt),\n",
    "        ),\n",
    "    ):\n",
    "        _, _, _, model = train_nw_arbitrary(\n",
    "            x_backgnd=xbi,\n",
    "            y_backgnd=ybi,\n",
    "            x_query=xqi,\n",
    "            y_query=yqi,\n",
    "            x_val=xv.to_numpy(),\n",
    "            y_val=yvi,\n",
    "            r_query_backgnd=r_q_b if rel else np.zeros_like(r_q_b),\n",
    "            r_val_nonval=r_v_nvi if rel else np.zeros_like(r_v_nvi),\n",
    "            cfg=model_cfg,\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "        trained_models[f\"rel={rel}\"][label] = model\n",
    "    \n",
    "    # rel as features\n",
    "    x_broad = np.concatenate((x.to_numpy(), r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[back_indices], x_broad[query_indices], x_broad[val_indices]\n",
    "    xt_q_broad, xnt_q_broad = x_broad[iq_t], x_broad[iq_nt]\n",
    "    xt_b_broad, xnt_b_broad = x_broad[ib_t], x_broad[ib_nt]\n",
    "    xv_broad = x_broad[val_indices]\n",
    "    \n",
    "    for (xqi, xbi, yqi, ybi, yvi, label) in (\n",
    "        (xt_q_broad, xt_b_broad, yt_q, yt_b, yv_t, label_t),\n",
    "        (xnt_q_broad, xnt_b_broad, ynt_q, ynt_b, yv_nt, label_nt),\n",
    "    ):\n",
    "        trained_models[nw_broad_key][label] = train_nw_arbitrary(\n",
    "            x_backgnd=xbi,\n",
    "            y_backgnd=ybi,\n",
    "            x_query=xqi,\n",
    "            y_query=yqi,\n",
    "            x_val=xv_broad,\n",
    "            y_val=yvi,\n",
    "            r_query_backgnd=np.zeros((len(xqi), len(xbi))),\n",
    "            r_val_nonval=np.zeros((len(xv_broad), len(xbi) + len(xqi))),\n",
    "            cfg=model_cfg,\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )[-1]\n",
    "    \n",
    "    # LightGBM\n",
    "    yt_train, ynt_train = np.concatenate([yt_q, yt_b]), np.concatenate([ynt_q, ynt_b])\n",
    "    for xq_ti, xb_ti, xq_nti, xb_nti, xv_i, lgb_key in (\n",
    "        (xt_q, xt_b, xnt_q, xnt_b, xv, \"lgb\"),\n",
    "        (xt_q_broad, xt_b_broad, xnt_q_broad, xnt_b_broad, xv_broad, \"lgb-rel\"),\n",
    "    ):\n",
    "        xt_train, xnt_train = np.concatenate([xq_ti, xb_ti]), np.concatenate([xq_nti, xb_nti])\n",
    "        lgb_model_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=yt_train))\n",
    "        lgb_model_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, ynt_train))\n",
    "        tau_lgb = lgb_model_t.predict(xv_i) - lgb_model_nt.predict(xv_i)\n",
    "        tau_true = yv_t - yv_nt\n",
    "        metrics[lgb_key].append(mean_squared_error(tau_true, tau_lgb))\n",
    "\n",
    "\n",
    "\n",
    "    for key, models in trained_models.items():\n",
    "        y_pred_treated = models[label_t].y_val_pred\n",
    "        y_pred_nt = models[label_nt].y_val_pred\n",
    "\n",
    "        tau_pred = y_pred_treated - y_pred_nt\n",
    "        metrics[key].append(mean_squared_error(tau_true, tau_pred)) # PEHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: round(np.mean(v),2) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d839a85",
   "metadata": {},
   "source": [
    "# X-learner\n",
    "## Step 1: two base learners as in T-learner\n",
    "\n",
    "Copilot prompt:\n",
    "<blockquote>\n",
    "Write the code for the first stage of X-learners.\n",
    "\n",
    "1. split data to xt_q, yt_q, xnt_q, ynt_q, ..., yv_t, yv_nt, just like for T-learner\n",
    "2. create x_broad by adding r and split to xt_q_broad, ...\n",
    "3. train estimators: nw rel, nw nonrel, nw nonrel for broad, lightgbm for broad\n",
    "4. Using each estimators, compute y treated for xnt_q, xnt_b, and y non-treated for xt_q, xt_b\n",
    "5. save estimated values and val indices for future analysis\n",
    "</blockquote>\n",
    "\n",
    "Code modified after generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "xlearner_stage1_results = []\n",
    "\n",
    "for seed in tqdm(range(20)):\n",
    "    np.random.seed(seed)\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed)\n",
    "    y_np = data_y_fact.to_numpy()\n",
    "    xq, xb, xv = x.iloc[query_indices], x.iloc[back_indices], x.iloc[val_indices]\n",
    "    yq, yb = data_y_fact.iloc[query_indices], data_y_fact.iloc[back_indices]\n",
    "    tq, tb = data_treatment[query_indices], data_treatment[back_indices]\n",
    "\n",
    "    # Split by treatment\n",
    "    xt_q, yt_q, xnt_q, ynt_q = split_treated_non_treated(xq, tq, yq)\n",
    "    xt_b, yt_b, xnt_b, ynt_b = split_treated_non_treated(xb, tb, yb)\n",
    "\n",
    "    iq_t = np.array([i for i in query_indices if data_treatment[i] == 1])\n",
    "    iq_nt = np.array([i for i in query_indices if data_treatment[i] == 0])\n",
    "    ib_t = np.array([i for i in back_indices if data_treatment[i] == 1])\n",
    "    ib_nt = np.array([i for i in back_indices if data_treatment[i] == 0])\n",
    "\n",
    "    data_y_cfact = ihdp_data[y_cfact_colname]\n",
    "    yv_t = np.array([data_y_fact[i] if data_treatment[i] == 1 else data_y_cfact[i] for i in val_indices])\n",
    "    yv_nt = np.array([data_y_fact[i] if data_treatment[i] == 0 else data_y_cfact[i] for i in val_indices])\n",
    "\n",
    "    # Add r as features\n",
    "    x_broad = np.concatenate((x.to_numpy(), r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[back_indices], x_broad[query_indices], x_broad[val_indices]\n",
    "    xt_q_broad, xnt_q_broad = x_broad[iq_t], x_broad[iq_nt]\n",
    "    xt_b_broad, xnt_b_broad = x_broad[ib_t], x_broad[ib_nt]\n",
    "\n",
    "    # Train NW (rel) on treated\n",
    "    _, _, _, nw_treated_rel = train_nw_arbitrary(\n",
    "        x_backgnd=xt_b, y_backgnd=yt_b,\n",
    "        x_query=xt_q, y_query=yt_q,\n",
    "        x_val=xv.to_numpy(), y_val=yv_t,\n",
    "        r_query_backgnd=r[iq_t][:, ib_t],\n",
    "        r_val_nonval=r[val_indices][:, np.concatenate([iq_t, ib_t])],\n",
    "        cfg=model_cfg, lr=lr, n_epochs=n_epochs,\n",
    "    )\n",
    "    # Train NW (rel) on non-treated\n",
    "    _, _, _, nw_nontreated_rel = train_nw_arbitrary(\n",
    "        x_backgnd=xnt_b, y_backgnd=ynt_b,\n",
    "        x_query=xnt_q, y_query=ynt_q,\n",
    "        x_val=xv.to_numpy(), y_val=yv_nt,\n",
    "        r_query_backgnd=r[iq_nt][:, ib_nt],\n",
    "        r_val_nonval=r[val_indices][:, np.concatenate([iq_nt, ib_nt])],\n",
    "        cfg=model_cfg, lr=lr, n_epochs=n_epochs,\n",
    "    )\n",
    "    # Train NW (nonrel) on treated\n",
    "    _, _, _, nw_treated_norel = train_nw_arbitrary(\n",
    "        x_backgnd=xt_b, y_backgnd=yt_b,\n",
    "        x_query=xt_q, y_query=yt_q,\n",
    "        x_val=xv.to_numpy(), y_val=yv_t,\n",
    "        r_query_backgnd=np.zeros_like(r[iq_t][:, ib_t]),\n",
    "        r_val_nonval=np.zeros_like(r[val_indices][:, np.concatenate([iq_t, ib_t])]),\n",
    "        cfg=model_cfg, lr=lr, n_epochs=n_epochs,\n",
    "    )\n",
    "    # Train NW (nonrel) on non-treated\n",
    "    _, _, _, nw_nontreated_norel = train_nw_arbitrary(\n",
    "        x_backgnd=xnt_b, y_backgnd=ynt_b,\n",
    "        x_query=xnt_q, y_query=ynt_q,\n",
    "        x_val=xv.to_numpy(), y_val=yv_nt,\n",
    "        r_query_backgnd=np.zeros_like(r[iq_nt][:, ib_nt]),\n",
    "        r_val_nonval=np.zeros_like(r[val_indices][:, np.concatenate([iq_nt, ib_nt])]),\n",
    "        cfg=model_cfg, lr=lr, n_epochs=n_epochs,\n",
    "    )\n",
    "    # NW (nonrel) on broad features, treated\n",
    "    _, _, _, nw_treated_broad = train_nw_arbitrary(\n",
    "        x_backgnd=xt_b_broad, y_backgnd=yt_b,\n",
    "        x_query=xt_q_broad, y_query=yt_q,\n",
    "        x_val=xv_broad, y_val=yv_t,\n",
    "        r_query_backgnd=np.zeros((len(xt_q_broad), len(xt_b_broad))),\n",
    "        r_val_nonval=np.zeros((len(xv_broad), len(xt_b_broad) + len(xt_q_broad))),\n",
    "        cfg=model_cfg, lr=lr, n_epochs=n_epochs,\n",
    "    )\n",
    "    # NW (nonrel) on broad features, non-treated\n",
    "    _, _, _, nw_nontreated_broad = train_nw_arbitrary(\n",
    "        x_backgnd=xnt_b_broad, y_backgnd=ynt_b,\n",
    "        x_query=xnt_q_broad, y_query=ynt_q,\n",
    "        x_val=xv_broad, y_val=yv_nt,\n",
    "        r_query_backgnd=np.zeros((len(xnt_q_broad), len(xnt_b_broad))),\n",
    "        r_val_nonval=np.zeros((len(xv_broad), len(xnt_b_broad) + len(xnt_q_broad))),\n",
    "        cfg=model_cfg, lr=lr, n_epochs=n_epochs,\n",
    "    )\n",
    "\n",
    "    # LightGBM\n",
    "    xt_train = np.concatenate([xt_q, xt_b])\n",
    "    yt_train = np.concatenate([yt_q, yt_b])\n",
    "    lgb_model_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=yt_train))\n",
    "\n",
    "    xnt_train = np.concatenate([xnt_q, xnt_b])\n",
    "    ynt_train = np.concatenate([ynt_q, ynt_b])\n",
    "    lgb_model_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, label=ynt_train))\n",
    "\n",
    "    # LightGBM on broad features, treated\n",
    "    xt_train_broad = np.concatenate([xt_q_broad, xt_b_broad])\n",
    "    lgb_model_t_broad = lgb.train(lgb_params, lgb.Dataset(xt_train_broad, label=yt_train))\n",
    "    # LightGBM on broad features, non-treated\n",
    "    xnt_train_broad = np.concatenate([xnt_q_broad, xnt_b_broad])\n",
    "    lgb_model_nt_broad = lgb.train(lgb_params, lgb.Dataset(xnt_train_broad, label=ynt_train))\n",
    "\n",
    "    # 4. Predict counterfactuals for training sets\n",
    "    # For non-treated: predict y_treated for non-treated patients\n",
    "    xnt_b, xnt_q = torch.tensor(xnt_b, dtype=torch.float32), torch.tensor(xnt_q, dtype=torch.float32)\n",
    "    xnt_b_broad, xnt_q_broad = torch.tensor(xnt_b_broad, dtype=torch.float32), torch.tensor(xnt_q_broad, dtype=torch.float32)\n",
    "    ynt_b = torch.tensor(ynt_b, dtype=torch.float32)\n",
    "    rqb_nt = torch.tensor(r[iq_nt][:, ib_nt], dtype=torch.float32)\n",
    "    y_treated_on_xnt_q_nwrel = nw_treated_rel.model(xnt_b, ynt_b, xnt_q, rqb_nt)\n",
    "    # y_treated_on_xnt_b_nwrel = nw_treated_rel.model(xnt_b)\n",
    "    y_treated_on_xnt_q_nwnorel = nw_treated_norel.model(xnt_b, ynt_b, xnt_q, torch.zeros_like(rqb_nt))\n",
    "    # y_treated_on_xnt_b_nwnorel = nw_treated_norel.model(xnt_b)\n",
    "    y_treated_on_xnt_q_broad = nw_treated_broad.model(xnt_b_broad, ynt_b, xnt_q_broad, torch.zeros_like(rqb_nt))\n",
    "    # y_treated_on_xnt_b_broad = nw_treated_broad.model(xnt_b_broad)\n",
    "    y_treated_on_xnt_train_lgb = lgb_model_t.predict(xnt_train)\n",
    "    y_treated_on_xnt_train_lgb_broad = lgb_model_t_broad.predict(xnt_train_broad)\n",
    "\n",
    "    # For treated: predict y_non-treated for xt_q, xt_b\n",
    "    xt_q, xt_b = torch.tensor(xt_q, dtype=torch.float32), torch.tensor(xt_b, dtype=torch.float32)\n",
    "    xt_b_broad, xt_q_broad = torch.tensor(xt_b_broad, dtype=torch.float32), torch.tensor(xt_q_broad, dtype=torch.float32)\n",
    "    yt_b = torch.tensor(yt_b, dtype=torch.float32)\n",
    "    rqb_t = torch.tensor(r[iq_t][:, ib_t], dtype=torch.float32)\n",
    "    y_nontreated_on_xt_q_nwrel = nw_nontreated_rel.model(xt_b, yt_b, xt_q, rqb_t)\n",
    "    # y_nontreated_on_xt_b_nwrel = nw_nontreated_rel.model(xt_b)\n",
    "    y_nontreated_on_xt_q_nwnorel = nw_nontreated_norel.model(xt_b, yt_b, xt_q, torch.zeros_like(rqb_t))\n",
    "    # y_nontreated_on_xt_b_nwnorel = nw_nontreated_norel.model(xt_b)\n",
    "    y_nontreated_on_xt_q_broad = nw_nontreated_broad.model(xt_b_broad, yt_b, xt_q_broad, torch.zeros_like(rqb_t))\n",
    "    # y_nontreated_on_xt_b_broad = nw_nontreated_broad.model(xt_b_broad)\n",
    "    y_nontreated_on_xt_train_lgb = lgb_model_t.predict(xt_train)\n",
    "    y_nontreated_on_xt_train_lgb_broad = lgb_model_nt_broad.predict(xt_train_broad)\n",
    "\n",
    "    # Save all results for future analysis\n",
    "    xlearner_stage1_results.append({\n",
    "        \"seed\": seed,\n",
    "        \"indices\": {\n",
    "            \"iq_t\": iq_t, \"iq_nt\": iq_nt, \"ib_t\": ib_t, \"ib_nt\": ib_nt,\n",
    "            \"query_indices\": query_indices, \"back_indices\": back_indices, \"val_indices\": val_indices\n",
    "        },\n",
    "        \"factual\": {\n",
    "            \"yt_q\": yt_q, \"yt_b\": yt_b, \"ynt_q\": ynt_q, \"ynt_b\": ynt_b\n",
    "        },\n",
    "        \"cfactual_preds\": {\n",
    "            \"y_treated_on_xnt_q_nwrel\": y_treated_on_xnt_q_nwrel,\n",
    "            # \"y_treated_on_xnt_b_nwrel\": y_treated_on_xnt_b_nwrel,\n",
    "            \"y_treated_on_xnt_q_nwnorel\": y_treated_on_xnt_q_nwnorel,\n",
    "            # \"y_treated_on_xnt_b_nwnorel\": y_treated_on_xnt_b_nwnorel,\n",
    "            \"y_treated_on_xnt_q_broad\": y_treated_on_xnt_q_broad,\n",
    "            # \"y_treated_on_xnt_b_broad\": y_treated_on_xnt_b_broad,\n",
    "            \"y_treated_on_xnt_train_lgb\": y_treated_on_xnt_train_lgb,\n",
    "            \"y_treated_on_xnt_train_lgb_broad\": y_treated_on_xnt_train_lgb_broad,\n",
    "            \"y_nontreated_on_xt_q_nwrel\": y_nontreated_on_xt_q_nwrel,\n",
    "            # \"y_nontreated_on_xt_b_nwrel\": y_nontreated_on_xt_b_nwrel,\n",
    "            \"y_nontreated_on_xt_q_nwnorel\": y_nontreated_on_xt_q_nwnorel,\n",
    "            # \"y_nontreated_on_xt_b_nwnorel\": y_nontreated_on_xt_b_nwnorel,\n",
    "            \"y_nontreated_on_xt_q_broad\": y_nontreated_on_xt_q_broad,\n",
    "            # \"y_nontreated_on_xt_b_broad\": y_nontreated_on_xt_b_broad,\n",
    "            \"y_nontreated_on_xt_train_lgb\": y_nontreated_on_xt_train_lgb,\n",
    "            \"y_nontreated_on_xt_train_lgb_broad\": y_nontreated_on_xt_train_lgb_broad,\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbfad8",
   "metadata": {},
   "source": [
    "# Step 2: $\\tau$ imputation for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in tqdm(range(10)):\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed)\n",
    "\n",
    "    res = run_training(\n",
    "        x=x.to_numpy(),\n",
    "        y=ihdp_data[ihdp_tau_colname].to_numpy(),  # TODO generate this in the previous stage, not use pre-computed\n",
    "        r=r,\n",
    "        backgnd_indices=back_indices,\n",
    "        query_indices=query_indices,\n",
    "        val_indices=val_indices,\n",
    "        lr=1e-4,\n",
    "        n_epochs=10,\n",
    "        rel_as_feats=r,\n",
    "    )\n",
    "    for i, v in enumerate(res.values()):\n",
    "        mse = v[0]\n",
    "        metrics[\"pehe\"][i].append(mse)\n",
    "\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    metrics[\"pehe\"][i] = np.array(metrics[\"pehe\"][i])\n",
    "metrics_mean(metrics, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
