{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_ihdp_data(ihdp_path: Path) -> tuple[pd.DataFrame, list[str], str]:\n",
    "    ihdp_cols = [s[:-1] for s in np.loadtxt(ihdp_path / \"columns.txt\", dtype=str)][:-2]\n",
    "    ihdp_cols.extend([f\"x{i}\" for i in range(2, 26)])\n",
    "\n",
    "    csvs = []\n",
    "    for csv_path in (ihdp_path / \"csv\").glob(\"*.csv\"):\n",
    "        csvs.append(pd.read_csv(csv_path, header=None))\n",
    "        break # TODO choose a table, for now using the first table\n",
    "    data = pd.concat(csvs)\n",
    "    data.columns = ihdp_cols\n",
    "\n",
    "    y_col_name = \"delta_y\"\n",
    "    data[y_col_name] = (data[\"y_cfactual\"] - data[\"y_factual\"]) * (-1) ** data[\"treatment\"]\n",
    "    exclude_cols = [\"treatment\", \"y_cfactual\", \"y_factual\", \"mu0\", \"mu1\"]\n",
    "    return data, exclude_cols, y_col_name\n",
    "\n",
    "ihdp_data, ihdp_exclude_cols, ihdp_y_colname = load_ihdp_data(Path(\"/Users/vzuev/Documents/git/gh_zuevval/tabrel/CEVAE/datasets/IHDP\"))\n",
    "ihdp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "\n",
    "x_all = ihdp_data.drop(columns=ihdp_exclude_cols + [ihdp_y_colname])\n",
    "\n",
    "ihdp_last_numeric_index: Final[int] = 6\n",
    "x_numeric = x_all.iloc[:, :ihdp_last_numeric_index]\n",
    "x_cat = x_all.iloc[:, ihdp_last_numeric_index:]\n",
    "x_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x_num_y = x_numeric.copy()\n",
    "x_num_y[ihdp_y_colname] = ihdp_data[ihdp_y_colname]\n",
    "sns.pairplot(x_num_y, hue=ihdp_y_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "group_col: Final[str] = \"x4\"\n",
    "x = x_all.drop(columns=[group_col])\n",
    "x_len = len(x)\n",
    "categories = x_all[group_col]\n",
    "print(\"n_categories\", len(categories.unique()))\n",
    "\n",
    "r = np.zeros((x_len, x_len))\n",
    "for i, j in tqdm(list(product(range(x_len), range(x_len)))):\n",
    "    if np.isclose(categories[i], categories[j]):\n",
    "        r[i, j] = 1\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78411c2b",
   "metadata": {},
   "source": [
    "# S-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabrel.benchmark.nw_regr import run_training, metrics_mean\n",
    "\n",
    "labels = [\"rel\", \"nrel\", \"lgb\", \"rel-fts\", \"lgb-rel\"]\n",
    "metrics = {\n",
    "    \"pehe\": [[] for _ in range(len(labels))]\n",
    "}\n",
    "\n",
    "def generate_indices(seed: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(x_len)\n",
    "    n_query, n_back = 200, 300\n",
    "    q_indices = indices[:n_query]\n",
    "    b_indices = indices[n_query:n_back]\n",
    "    v_indices = indices[n_back:]\n",
    "    return q_indices, b_indices, v_indices\n",
    "\n",
    "\n",
    "for seed in tqdm(range(10)):\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed)\n",
    "\n",
    "\n",
    "\n",
    "    res = run_training(\n",
    "        x=x.to_numpy(),\n",
    "        y=ihdp_data[ihdp_y_colname].to_numpy(),\n",
    "        r=r,\n",
    "        backgnd_indices=back_indices,\n",
    "        query_indices=query_indices,\n",
    "        val_indices=val_indices,\n",
    "        lr=1e-4,\n",
    "        n_epochs=10,\n",
    "        rel_as_feats=r,\n",
    "    )\n",
    "    for i, v in enumerate(res.values()):\n",
    "        mse = v[0]\n",
    "        metrics[\"pehe\"][i].append(mse)\n",
    "\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    metrics[\"pehe\"][i] = np.array(metrics[\"pehe\"][i])\n",
    "metrics_mean(metrics, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6132ef0",
   "metadata": {},
   "source": [
    "# T-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tabrel.benchmark.nw_regr import train_nw_arbitrary, NwModelConfig\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "y_fact_colname, y_cfact_colname = \"y_factual\", \"y_cfactual\"\n",
    "data_y_fact, data_treatment = ihdp_data[y_fact_colname], ihdp_data[\"treatment\"]\n",
    "\n",
    "def split_treated_non_treated(x: pd.DataFrame, treatment: np.ndarray, y_fact: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    treated = treatment == 1\n",
    "    x_treated, y_treated = x.loc[treated], y_fact.loc[treated]\n",
    "    x_non_treated, y_non_treated = x.loc[~treated], y_fact.loc[~treated]\n",
    "    return x_treated.to_numpy(), y_treated.to_numpy(), x_non_treated.to_numpy(), y_non_treated.to_numpy()\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(20)):\n",
    "    np.random.seed(seed)\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed)\n",
    "    y_np = data_y_fact.to_numpy()\n",
    "    xq, xb, xv = x.iloc[query_indices], x.iloc[back_indices], x.iloc[val_indices]\n",
    "    yq, yb = data_y_fact.iloc[query_indices], data_y_fact.iloc[back_indices]\n",
    "    tq, tb = data_treatment[query_indices], data_treatment[back_indices]\n",
    "\n",
    "    xt_q, yt_q, xnt_q, ynt_q = split_treated_non_treated(xq, tq, yq)\n",
    "    xt_b, yt_b, xnt_b, ynt_b = split_treated_non_treated(xb, tb, yb)\n",
    "\n",
    "    iq_t = np.array([i for i in query_indices if data_treatment[i] == 1])\n",
    "    iq_nt = np.array([i for i in query_indices if data_treatment[i] == 0])\n",
    "\n",
    "    ib_t = np.array([i for i in back_indices if data_treatment[i] == 1])\n",
    "    ib_nt = np.array([i for i in back_indices if data_treatment[i] == 0])\n",
    "\n",
    "    data_y_cfact = ihdp_data[y_cfact_colname]\n",
    "    yv_t = np.array([data_y_fact[i] if data_treatment[i] == 1 else data_y_cfact[i] for i in val_indices])\n",
    "    yv_nt = np.array([data_y_fact[i] if data_treatment[i] == 0 else data_y_cfact[i] for i in val_indices])\n",
    "\n",
    "    i_train_t = np.concatenate([iq_t, ib_t])\n",
    "    i_train_nt = np.concatenate([iq_nt, ib_nt])\n",
    "\n",
    "    r_q_b_treated = r[iq_t][:, ib_t]\n",
    "    r_q_b_nt = r[iq_nt][:, ib_nt]\n",
    "    r_val_nvt = r[val_indices][:, i_train_t]  # rel between val and treated train\n",
    "    r_val_nvnt = r[val_indices][:, i_train_nt]  # rel between val and non-treated train\n",
    "\n",
    "    label_t: Final[str] = \"treated\"\n",
    "    label_nt: Final[str] = \"non-treated\"\n",
    "\n",
    "    model_cfg, lr, n_epochs = NwModelConfig(), 1e-3, 50\n",
    "    nw_broad_key: Final[str] = \"nw_rel-as-features\"\n",
    "    trained_models = {\n",
    "        \"rel=True\": {},\n",
    "        \"rel=False\": {},\n",
    "        nw_broad_key: {},\n",
    "    }\n",
    "    for rel, (xqi, xbi, yqi, ybi, yvi, r_q_b, r_v_nvi, label) in product(\n",
    "        (True, False),\n",
    "        (\n",
    "            (xt_q, xt_b, yt_q, yt_b, yv_t, r_q_b_treated, r_val_nvt, label_t),\n",
    "            (xnt_q, xnt_b, ynt_q, ynt_b, yv_nt, r_q_b_nt, r_val_nvnt, label_nt),\n",
    "        ),\n",
    "    ):\n",
    "        _, _, _, model = train_nw_arbitrary(\n",
    "            x_backgnd=xbi,\n",
    "            y_backgnd=ybi,\n",
    "            x_query=xqi,\n",
    "            y_query=yqi,\n",
    "            x_val=xv.to_numpy(),\n",
    "            y_val=yvi,\n",
    "            r_query_backgnd=r_q_b if rel else np.zeros_like(r_q_b),\n",
    "            r_val_nonval=r_v_nvi if rel else np.zeros_like(r_v_nvi),\n",
    "            cfg=model_cfg,\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "        trained_models[f\"rel={rel}\"][label] = model\n",
    "    \n",
    "    # rel as features\n",
    "    x_broad = np.concatenate((x.to_numpy(), r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[back_indices], x_broad[query_indices], x_broad[val_indices]\n",
    "    xt_q_broad, xnt_q_broad = x_broad[iq_t], x_broad[iq_nt]\n",
    "    xt_b_broad, xnt_b_broad = x_broad[ib_t], x_broad[ib_nt]\n",
    "    xv_broad = x_broad[val_indices]\n",
    "    \n",
    "    for (xqi, xbi, yqi, ybi, yvi, label) in (\n",
    "        (xt_q_broad, xt_b_broad, yt_q, yt_b, yv_t, label_t),\n",
    "        (xnt_q_broad, xnt_b_broad, ynt_q, ynt_b, yv_nt, label_nt),\n",
    "    ):\n",
    "        trained_models[nw_broad_key][label] = train_nw_arbitrary(\n",
    "            x_backgnd=xbi,\n",
    "            y_backgnd=ybi,\n",
    "            x_query=xqi,\n",
    "            y_query=yqi,\n",
    "            x_val=xv_broad,\n",
    "            y_val=yvi,\n",
    "            r_query_backgnd=np.zeros((len(xqi), len(xbi))),\n",
    "            r_val_nonval=np.zeros((len(xv_broad), len(xbi) + len(xqi))),\n",
    "            cfg=model_cfg,\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )[-1]\n",
    "    \n",
    "    # LightGBM\n",
    "    yt_train, ynt_train = np.concatenate([yt_q, yt_b]), np.concatenate([ynt_q, ynt_b])\n",
    "    lgb_params: Final[dict[str, str]] = {\"objective\": \"regression\", \"metric\": \"rmse\"}\n",
    "    for xq_ti, xb_ti, xq_nti, xb_nti, xv_i, lgb_key in (\n",
    "        (xt_q, xt_b, xnt_q, xnt_b, xv, \"lgb\"),\n",
    "        (xt_q_broad, xt_b_broad, xnt_q_broad, xnt_b_broad, xv_broad, \"lgb-rel\"),\n",
    "    ):\n",
    "        xt_train, xnt_train = np.concatenate([xq_ti, xb_ti]), np.concatenate([xq_nti, xb_nti])\n",
    "        lgb_model_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=yt_train))\n",
    "        lgb_model_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, ynt_train))\n",
    "        tau_lgb = lgb_model_t.predict(xv_i) - lgb_model_nt.predict(xv_i)\n",
    "        tau_true = yv_t - yv_nt\n",
    "        metrics[lgb_key].append(mean_squared_error(tau_true, tau_lgb))\n",
    "\n",
    "\n",
    "\n",
    "    for key, models in trained_models.items():\n",
    "        y_pred_treated = models[label_t].y_val_pred\n",
    "        y_pred_nt = models[label_nt].y_val_pred\n",
    "\n",
    "        tau_pred = y_pred_treated - y_pred_nt\n",
    "        metrics[key].append(mean_squared_error(tau_true, tau_pred)) # PEHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: round(np.mean(v),2) for k, v in metrics.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
