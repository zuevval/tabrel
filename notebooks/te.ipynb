{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_ihdp_data(ihdp_path: Path) -> tuple[pd.DataFrame, list[str], str]:\n",
    "    ihdp_cols = [s[:-1] for s in np.loadtxt(ihdp_path / \"columns.txt\", dtype=str)][:-2]\n",
    "    ihdp_cols.extend([f\"x{i}\" for i in range(2, 26)])\n",
    "\n",
    "    csvs = []\n",
    "    for csv_path in (ihdp_path / \"csv\").glob(\"*.csv\"):\n",
    "        csvs.append(pd.read_csv(csv_path, header=None))\n",
    "        break # TODO choose a table, for now using the first table\n",
    "    data = pd.concat(csvs)\n",
    "    data.columns = ihdp_cols\n",
    "\n",
    "    tau_col_name = \"delta_y\"\n",
    "    data[tau_col_name] = (data[\"y_cfactual\"] - data[\"y_factual\"]) * (-1) ** data[\"treatment\"]\n",
    "    exclude_cols = [\"treatment\", \"y_cfactual\", \"y_factual\", \"mu0\", \"mu1\"]\n",
    "    return data, exclude_cols, tau_col_name\n",
    "\n",
    "ihdp_data, ihdp_exclude_cols, ihdp_tau_colname = load_ihdp_data(Path(\"/Users/vzuev/Documents/git/gh_zuevval/tabrel/CEVAE/datasets/IHDP\"))\n",
    "ihdp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "\n",
    "x_all = ihdp_data.drop(columns=ihdp_exclude_cols + [ihdp_tau_colname])\n",
    "\n",
    "ihdp_last_numeric_index: Final[int] = 6\n",
    "x_numeric = x_all.iloc[:, :ihdp_last_numeric_index]\n",
    "x_cat = x_all.iloc[:, ihdp_last_numeric_index:]\n",
    "x_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "x_num_y = x_numeric.copy()\n",
    "x_num_y[ihdp_tau_colname] = ihdp_data[ihdp_tau_colname]\n",
    "sns.pairplot(x_num_y, hue=ihdp_tau_colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "group_col: Final[str] = \"x4\"\n",
    "x = x_all.drop(columns=[group_col])\n",
    "x_len = len(x)\n",
    "categories = x_all[group_col]\n",
    "print(\"n_categories\", len(categories.unique()))\n",
    "\n",
    "r = np.zeros((x_len, x_len))\n",
    "for i, j in tqdm(list(product(range(x_len), range(x_len)))):\n",
    "    if np.isclose(categories[i], categories[j]):\n",
    "        r[i, j] = 1\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78411c2b",
   "metadata": {},
   "source": [
    "# S-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tabrel.benchmark.nw_regr import run_training, metrics_mean, train_nw_arbitrary, NwModelConfig\n",
    "from tabrel.train import train_relnet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "labels = [\"rel\", \"nrel\", \"lgb\", \"rel-fts\", \"lgb-rel\"]\n",
    "\n",
    "def generate_indices(seed: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(x_len)\n",
    "    n_query, n_back = 200, 300\n",
    "    q_indices = indices[:n_query]\n",
    "    b_indices = indices[n_query:n_back]\n",
    "    v_indices = indices[n_back:]\n",
    "    return q_indices, b_indices, v_indices\n",
    "\n",
    "def split_treated_non_treated(x: pd.DataFrame, treatment: np.ndarray, y_fact: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    treated = treatment == 1\n",
    "    x_treated, y_treated = x.loc[treated], y_fact.loc[treated]\n",
    "    x_non_treated, y_non_treated = x.loc[~treated], y_fact.loc[~treated]\n",
    "    return x_treated.to_numpy(), y_treated.to_numpy(), x_non_treated.to_numpy(), y_non_treated.to_numpy()\n",
    "\n",
    "y_fact_colname, y_cfact_colname = \"y_factual\", \"y_cfactual\"\n",
    "data_y_fact, data_treatment = ihdp_data[y_fact_colname], ihdp_data[\"treatment\"]\n",
    "\n",
    "x_s = x # x for S-learner\n",
    "x_s[\"treatment\"] = data_treatment\n",
    "x_s_np = x_s.to_numpy()\n",
    "y_s = data_y_fact.to_numpy() # Y for S-learner\n",
    "\n",
    "model_cfg, lr, n_epochs = NwModelConfig(), 1e-3, 50\n",
    "lgb_params: Final[dict[str, str | int]] = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(20)):\n",
    "    np.random.seed(seed)\n",
    "    ids_q, ids_b, ids_v = generate_indices(seed)\n",
    "    ids_train = np.concatenate((ids_q, ids_b))\n",
    "    xb, yb, xq, yq = x_s_np[ids_b], y_s[ids_b], x_s_np[ids_q], y_s[ids_q]\n",
    "    xv, yv = x_s_np[ids_v], y_s[ids_v]\n",
    "    r_q_b = r[ids_q][:, ids_b]\n",
    "    r_val_train = r[ids_v][:, ids_train]\n",
    "\n",
    "    # TabRel\n",
    "    relnet_pehe, _, _, _, _ = train_relnet(\n",
    "        x=x_s_np,\n",
    "        y=y_s,\n",
    "        r=r,\n",
    "        backgnd_indices=ids_b,\n",
    "        query_indices=ids_q,\n",
    "        val_indices=ids_v,\n",
    "        lr=0.01,\n",
    "        n_epochs=800,\n",
    "        n_layers=2,\n",
    "        periodic_embed_dim=None,\n",
    "        num_heads=2,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "    metrics[\"relnet\"].append(relnet_pehe)\n",
    "\n",
    "    # NW with rel\n",
    "    _, _, _, model_rel = train_nw_arbitrary(\n",
    "        x_backgnd=xb,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq,\n",
    "        y_query=yq,\n",
    "        x_val=xv,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=r_q_b,\n",
    "        r_val_nonval=r_val_train,\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics[\"rel\"].append(mean_squared_error(yv, model_rel.y_val_pred))\n",
    "\n",
    "    # NW without rel\n",
    "    _, _, _, model_nrel = train_nw_arbitrary(\n",
    "        x_backgnd=xb,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq,\n",
    "        y_query=yq,\n",
    "        x_val=xv,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=np.zeros_like(r_q_b),\n",
    "        r_val_nonval=np.zeros_like(r_val_train),\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics[\"nrel\"].append(mean_squared_error(yv, model_nrel.y_val_pred))\n",
    "\n",
    "    # LightGBM\n",
    "    x_train = np.concatenate([xq, xb])\n",
    "    y_train = np.concatenate([yq, yb])\n",
    "    lgb_model = lgb.train(lgb_params, lgb.Dataset(x_train, label=y_train))\n",
    "    y_pred_lgb = lgb_model.predict(xv)\n",
    "    metrics[\"lgb\"].append(mean_squared_error(yv, y_pred_lgb))\n",
    "\n",
    "    # NW and LGB with rel as features\n",
    "    x_broad = np.concatenate((x_s_np, r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[ids_b], x_broad[ids_q], x_broad[ids_v]\n",
    "\n",
    "    # NW with rel as features\n",
    "    _, _, _, model_relfts = train_nw_arbitrary(\n",
    "        x_backgnd=xb_broad,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq_broad,\n",
    "        y_query=yq,\n",
    "        x_val=xv_broad,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=np.zeros((len(xq_broad), len(xb_broad))),\n",
    "        r_val_nonval=np.zeros((len(xv_broad), len(xb_broad) + len(xq_broad))),\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics[\"rel-fts\"].append(mean_squared_error(yv, model_relfts.y_val_pred))\n",
    "\n",
    "    # LightGBM with rel as features\n",
    "    x_train_broad = np.concatenate([xq_broad, xb_broad])\n",
    "    lgb_model_rel = lgb.train(lgb_params, lgb.Dataset(x_train_broad, label=y_train))\n",
    "    y_pred_lgb_rel = lgb_model_rel.predict(xv_broad)\n",
    "    metrics[\"lgb-rel\"].append(mean_squared_error(yv, y_pred_lgb_rel))\n",
    "\n",
    "# Show mean metrics\n",
    "{ k: round(np.mean(v), 2) for k, v in metrics.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6132ef0",
   "metadata": {},
   "source": [
    "# T-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(30)):\n",
    "    np.random.seed(seed)\n",
    "    query_indices, back_indices, val_indices = generate_indices(seed)\n",
    "    y_np = data_y_fact.to_numpy()\n",
    "    xq, xb, xv = x.iloc[query_indices], x.iloc[back_indices], x.iloc[val_indices]\n",
    "    yq, yb = data_y_fact.iloc[query_indices], data_y_fact.iloc[back_indices]\n",
    "    tq, tb = data_treatment[query_indices], data_treatment[back_indices]\n",
    "\n",
    "    xt_q, yt_q, xnt_q, ynt_q = split_treated_non_treated(xq, tq, yq)\n",
    "    xt_b, yt_b, xnt_b, ynt_b = split_treated_non_treated(xb, tb, yb)\n",
    "\n",
    "    iq_t = np.array([i for i in query_indices if data_treatment[i] == 1])\n",
    "    iq_nt = np.array([i for i in query_indices if data_treatment[i] == 0])\n",
    "\n",
    "    ib_t = np.array([i for i in back_indices if data_treatment[i] == 1])\n",
    "    ib_nt = np.array([i for i in back_indices if data_treatment[i] == 0])\n",
    "\n",
    "    data_y_cfact = ihdp_data[y_cfact_colname]\n",
    "    yv_t = np.array([data_y_fact[i] if data_treatment[i] == 1 else data_y_cfact[i] for i in val_indices])\n",
    "    yv_nt = np.array([data_y_fact[i] if data_treatment[i] == 0 else data_y_cfact[i] for i in val_indices])\n",
    "\n",
    "    i_train_t = np.concatenate([iq_t, ib_t])\n",
    "    i_train_nt = np.concatenate([iq_nt, ib_nt])\n",
    "\n",
    "    r_q_b_treated = r[iq_t][:, ib_t]\n",
    "    r_q_b_nt = r[iq_nt][:, ib_nt]\n",
    "    r_val_nvt = r[val_indices][:, i_train_t]  # rel between val and treated train\n",
    "    r_val_nvnt = r[val_indices][:, i_train_nt]  # rel between val and non-treated train\n",
    "\n",
    "    label_t: Final[str] = \"treated\"\n",
    "    label_nt: Final[str] = \"non-treated\"\n",
    "\n",
    "    nw_broad_key: Final[str] = \"nw_rel-as-features\"\n",
    "    trained_models = {\n",
    "        \"rel=True\": {},\n",
    "        \"rel=False\": {},\n",
    "        nw_broad_key: {},\n",
    "    }\n",
    "    for rel, (xqi, xbi, yqi, ybi, yvi, r_q_b, r_v_nvi, label) in product(\n",
    "        (True, False),\n",
    "        (\n",
    "            (xt_q, xt_b, yt_q, yt_b, yv_t, r_q_b_treated, r_val_nvt, label_t),\n",
    "            (xnt_q, xnt_b, ynt_q, ynt_b, yv_nt, r_q_b_nt, r_val_nvnt, label_nt),\n",
    "        ),\n",
    "    ):\n",
    "        _, _, _, model = train_nw_arbitrary(\n",
    "            x_backgnd=xbi,\n",
    "            y_backgnd=ybi,\n",
    "            x_query=xqi,\n",
    "            y_query=yqi,\n",
    "            x_val=xv.to_numpy(),\n",
    "            y_val=yvi,\n",
    "            r_query_backgnd=r_q_b if rel else np.zeros_like(r_q_b),\n",
    "            r_val_nonval=r_v_nvi if rel else np.zeros_like(r_v_nvi),\n",
    "            cfg=model_cfg,\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )\n",
    "        trained_models[f\"rel={rel}\"][label] = model\n",
    "    \n",
    "    # rel as features\n",
    "    x_broad = np.concatenate((x.to_numpy(), r), axis=1)\n",
    "    xb_broad, xq_broad, xv_broad = x_broad[back_indices], x_broad[query_indices], x_broad[val_indices]\n",
    "    xt_q_broad, xnt_q_broad = x_broad[iq_t], x_broad[iq_nt]\n",
    "    xt_b_broad, xnt_b_broad = x_broad[ib_t], x_broad[ib_nt]\n",
    "    xv_broad = x_broad[val_indices]\n",
    "    \n",
    "    for (xqi, xbi, yqi, ybi, yvi, label) in (\n",
    "        (xt_q_broad, xt_b_broad, yt_q, yt_b, yv_t, label_t),\n",
    "        (xnt_q_broad, xnt_b_broad, ynt_q, ynt_b, yv_nt, label_nt),\n",
    "    ):\n",
    "        trained_models[nw_broad_key][label] = train_nw_arbitrary(\n",
    "            x_backgnd=xbi,\n",
    "            y_backgnd=ybi,\n",
    "            x_query=xqi,\n",
    "            y_query=yqi,\n",
    "            x_val=xv_broad,\n",
    "            y_val=yvi,\n",
    "            r_query_backgnd=np.zeros((len(xqi), len(xbi))),\n",
    "            r_val_nonval=np.zeros((len(xv_broad), len(xbi) + len(xqi))),\n",
    "            cfg=model_cfg,\n",
    "            lr=lr,\n",
    "            n_epochs=n_epochs,\n",
    "        )[-1]\n",
    "    \n",
    "    # LightGBM\n",
    "    tau_true = yv_t - yv_nt\n",
    "    yt_train, ynt_train = np.concatenate([yt_q, yt_b]), np.concatenate([ynt_q, ynt_b])\n",
    "    for xq_ti, xb_ti, xq_nti, xb_nti, xv_i, lgb_key in (\n",
    "        (xt_q, xt_b, xnt_q, xnt_b, xv, \"lgb\"),\n",
    "        (xt_q_broad, xt_b_broad, xnt_q_broad, xnt_b_broad, xv_broad, \"lgb-rel\"),\n",
    "    ):\n",
    "        xt_train, xnt_train = np.concatenate([xq_ti, xb_ti]), np.concatenate([xq_nti, xb_nti])\n",
    "        lgb_model_t = lgb.train(lgb_params, lgb.Dataset(xt_train, label=yt_train))\n",
    "        lgb_model_nt = lgb.train(lgb_params, lgb.Dataset(xnt_train, ynt_train))\n",
    "        tau_lgb = lgb_model_t.predict(xv_i) - lgb_model_nt.predict(xv_i)\n",
    "        metrics[lgb_key].append(mean_squared_error(tau_true, tau_lgb))\n",
    "\n",
    "    # TabRel\n",
    "    def train_relnet_shorthand(x_: np.ndarray, y_: np.ndarray, r_: np.ndarray, \n",
    "                               bi_: np.ndarray, qi_: np.ndarray, vi_: np.ndarray) -> torch.Tensor:\n",
    "        _, _, _, y_pred, _ = train_relnet(\n",
    "            x=x_,\n",
    "            y=y_,\n",
    "            r=r_,\n",
    "            backgnd_indices=bi_,\n",
    "            query_indices=qi_,\n",
    "            val_indices=vi_,\n",
    "            lr=.007,\n",
    "            n_epochs=1500,\n",
    "            n_layers=2,\n",
    "            periodic_embed_dim=None,\n",
    "            embed_dim=32,\n",
    "            num_heads=2,\n",
    "            progress_bar=False,\n",
    "        )\n",
    "        return y_pred\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    xt = np.concatenate([xt_b, xt_q, xv])\n",
    "    bi_t = np.array(range(len(xt_b)))\n",
    "    qi_t = np.array(range(len(xt_q))) + len(xt_b)\n",
    "    vi_t = np.array(range(len(xv))) + len(xt_b) + len(xt_q)\n",
    "\n",
    "    xnt = np.concatenate([xnt_b, xnt_q, xv])\n",
    "    yt = np.concatenate([yt_b, yt_q, yv_t])\n",
    "    ynt = np.concatenate([ynt_b, ynt_q, yv_nt])\n",
    "    bi_nt = np.array(range(len(xnt_b)))\n",
    "    qi_nt = np.array(range(len(xnt_q))) + len(xnt_b)\n",
    "    vi_nt = np.array(range(len(xv))) + len(xnt_b) + len(xnt_q)\n",
    "\n",
    "    i_t = np.concatenate([ib_t, iq_t, val_indices])\n",
    "    i_nt = np.concatenate([ib_nt, iq_nt, val_indices])\n",
    "    r_bqv_t = r[i_t][:, i_t]\n",
    "    r_bqv_nt = r[i_nt][:, i_nt]\n",
    "    y_pred_relnet_t = train_relnet_shorthand(x_=xt, y_=yt, r_=r_bqv_t, bi_=bi_t, qi_=qi_t, vi_=vi_t)\n",
    "    y_pred_relnet_nt = train_relnet_shorthand(x_=xnt, y_=ynt, r_=r_bqv_nt, bi_=bi_nt, qi_=qi_nt, vi_=vi_nt)\n",
    "    tau_pred_relnet = y_pred_relnet_t - y_pred_relnet_nt\n",
    "\n",
    "    metrics[\"relnet\"].append(mean_squared_error(tau_true, tau_pred_relnet))\n",
    "\n",
    "    for key, models in trained_models.items():\n",
    "        y_pred_treated = models[label_t].y_val_pred\n",
    "        y_pred_nt = models[label_nt].y_val_pred\n",
    "\n",
    "        tau_pred = y_pred_treated - y_pred_nt\n",
    "        metrics[key].append(mean_squared_error(tau_true, tau_pred)) # PEHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: f\"{np.mean(v):.2f} & {np.std(v):.2f}\" for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d839a85",
   "metadata": {},
   "source": [
    "# X-learner\n",
    "## Step 1: two base learners as in T-learner\n",
    "\n",
    "Copilot prompt:\n",
    "<blockquote>\n",
    "Write the first stage of X-learner using LightGBM only. Split the data into train and validation set, then split each set into treated and non-treated. Train two separate models for treated and non-treated patients, use cross-validation. Save their predictions for training and validation sets. For each point in the training set, calculate estimated tau values (difference between outcomes when treated and when not treated). Reduce code duplication by introducing functions when necessary\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "def split_by_treatment(x: np.ndarray, y: np.ndarray, treatment: np.ndarray):\n",
    "    \"\"\"Split features and labels into treated and non-treated groups.\"\"\"\n",
    "    treated_mask = treatment == 1\n",
    "    return (\n",
    "        x[treated_mask], y[treated_mask],\n",
    "        x[~treated_mask], y[~treated_mask]\n",
    "    )\n",
    "\n",
    "def lgb_cv_train_predict(x_train, y_train, x_pred, params, n_splits=5, random_state=0):\n",
    "    \"\"\"Train LightGBM with cross-validation and predict on x_pred.\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    preds = np.zeros(x_pred.shape[0])\n",
    "    for train_idx, _ in kf.split(x_train):\n",
    "        dtrain = lgb.Dataset(x_train[train_idx], label=y_train[train_idx])\n",
    "        model = lgb.train(params, dtrain)\n",
    "        preds += model.predict(x_pred) / n_splits\n",
    "    return preds\n",
    "\n",
    "# Prepare data\n",
    "X = x.to_numpy()\n",
    "y = data_y_fact.to_numpy()\n",
    "treatment = data_treatment.to_numpy()\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val, tr_train, tr_val = train_test_split(\n",
    "    X, y, treatment, test_size=0.2, random_state=42, stratify=treatment\n",
    ")\n",
    "\n",
    "# Split train and val sets into treated and non-treated\n",
    "X_train_t, y_train_t, X_train_nt, y_train_nt = split_by_treatment(X_train, y_train, tr_train)\n",
    "X_val_t, y_val_t, X_val_nt, y_val_nt = split_by_treatment(X_val, y_val, tr_val)\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}\n",
    "\n",
    "# Train models with CV and predict on train and val sets\n",
    "y_hat_treated_train = lgb_cv_train_predict(X_train_t, y_train_t, X_train, lgb_params)\n",
    "y_hat_nontreated_train = lgb_cv_train_predict(X_train_nt, y_train_nt, X_train, lgb_params)\n",
    "y_hat_treated_val = lgb_cv_train_predict(X_train_t, y_train_t, X_val, lgb_params)\n",
    "y_hat_nontreated_val = lgb_cv_train_predict(X_train_nt, y_train_nt, X_val, lgb_params)\n",
    "\n",
    "# Save predictions for the training and validation sets\n",
    "train_preds = {\n",
    "    \"y_hat_treated\": y_hat_treated_train,\n",
    "    \"y_hat_nontreated\": y_hat_nontreated_train,\n",
    "    \"treatment\": tr_train,\n",
    "    \"y_train\": y_train\n",
    "}\n",
    "val_preds = {\n",
    "    \"y_hat_treated\": y_hat_treated_val,\n",
    "    \"y_hat_nontreated\": y_hat_nontreated_val,\n",
    "    \"treatment\": tr_val,\n",
    "    \"y_val\": y_val\n",
    "}\n",
    "\n",
    "# Compute estimated tau for each training point\n",
    "# For treated: tau_hat = y_train - y_hat_nontreated\n",
    "# For non-treated: tau_hat = y_hat_treated - y_train\n",
    "tau_hat_train = np.where(\n",
    "    tr_train == 1,\n",
    "    y_train - y_hat_nontreated_train,\n",
    "    y_hat_treated_train - y_train\n",
    ")\n",
    "train_preds[\"tau_hat\"] = tau_hat_train\n",
    "\n",
    "# Optionally, save as DataFrame for further analysis\n",
    "xlearner_stage1_train_df = pd.DataFrame({\n",
    "    \"y_train\": y_train,\n",
    "    \"treatment\": tr_train,\n",
    "    \"y_hat_treated\": y_hat_treated_train,\n",
    "    \"y_hat_nontreated\": y_hat_nontreated_train,\n",
    "    \"tau_hat\": tau_hat_train\n",
    "})\n",
    "xlearner_stage1_val_df = pd.DataFrame({\n",
    "    \"y_val\": y_val,\n",
    "    \"treatment\": tr_val,\n",
    "    \"y_hat_treated\": y_hat_treated_val,\n",
    "    \"y_hat_nontreated\": y_hat_nontreated_val\n",
    "})\n",
    "\n",
    "xlearner_stage1_train_df.head(), xlearner_stage1_val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbfad8",
   "metadata": {},
   "source": [
    "# Step 2: $\\tau$ imputation for training set\n",
    "\n",
    "Copilot prompt:\n",
    "<blockquote>\n",
    "Now write the second stage of X-learner. Just like for S-learner, split data into query, background, validation sets; train models (NW, NW without R, NW with broad features (rels as features), LightGBM, LightGBM with rels as features, relnet), but do not include the treatment indicator in the feature set and predict tau, not the factual outcome. Calculate metrics (PEHE calculated as MSE between estimated and real tau) for the validation set\n",
    "\n",
    "instead of using generate_indices, use the fixed validation set from the previous X-learner stage. Split train into query and background for each seed\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ae6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Use tau_hat from X-learner stage 1 as y for training, and true tau for validation\n",
    "tau_true = ihdp_data[ihdp_tau_colname].to_numpy()\n",
    "tau_hat = xlearner_stage1_train_df[\"tau_hat\"].to_numpy()\n",
    "\n",
    "# Use the same train/val split as in X-learner stage 1\n",
    "X = x.to_numpy()\n",
    "train_idx = xlearner_stage1_train_df.index.values\n",
    "val_idx = xlearner_stage1_val_df.index.values\n",
    "\n",
    "X_train, X_val = X[train_idx], X[val_idx]\n",
    "y_train, y_val = tau_hat, tau_true[val_idx]\n",
    "\n",
    "metrics_x = defaultdict(list)\n",
    "n_train = len(X_train)\n",
    "\n",
    "for seed in tqdm(range(20)):\n",
    "    np.random.seed(seed)\n",
    "    # Split train into query and background for this seed\n",
    "    perm = np.random.permutation(n_train)\n",
    "    n_query, n_back = 200, 300\n",
    "    ids_q = perm[:n_query]\n",
    "    ids_b = perm[n_query:n_query + n_back]\n",
    "    ids_train = np.concatenate([ids_q, ids_b])\n",
    "\n",
    "    # Prepare features (no treatment indicator)\n",
    "    xq, xb = X_train[ids_q], X_train[ids_b]\n",
    "    yq, yb = y_train[ids_q], y_train[ids_b]\n",
    "    xv, yv = X_val, y_val\n",
    "\n",
    "    # NW with rel\n",
    "    r_q_b = r[train_idx[ids_q]][:, train_idx[ids_b]]\n",
    "    r_val_train = r[val_idx][:, train_idx[ids_train]]\n",
    "    _, _, _, model_rel = train_nw_arbitrary(\n",
    "        x_backgnd=xb,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq,\n",
    "        y_query=yq,\n",
    "        x_val=xv,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=r_q_b,\n",
    "        r_val_nonval=r_val_train,\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics_x[\"rel\"].append(mean_squared_error(yv, model_rel.y_val_pred))\n",
    "\n",
    "    # NW without rel\n",
    "    _, _, _, model_nrel = train_nw_arbitrary(\n",
    "        x_backgnd=xb,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq,\n",
    "        y_query=yq,\n",
    "        x_val=xv,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=np.zeros_like(r_q_b),\n",
    "        r_val_nonval=np.zeros_like(r_val_train),\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics_x[\"nrel\"].append(mean_squared_error(yv, model_nrel.y_val_pred))\n",
    "\n",
    "    # LightGBM\n",
    "    x_train = np.concatenate([xq, xb])\n",
    "    y_train_lgb = np.concatenate([yq, yb])\n",
    "    lgb_model = lgb.train(lgb_params, lgb.Dataset(x_train, label=y_train_lgb))\n",
    "    y_pred_lgb = lgb_model.predict(xv)\n",
    "    metrics_x[\"lgb\"].append(mean_squared_error(yv, y_pred_lgb))\n",
    "\n",
    "    # NW and LGB with rel as features\n",
    "    x_broad = np.concatenate((X, r), axis=1)\n",
    "    x_train_broad = x_broad[train_idx[ids_train]]\n",
    "    xq_broad, xb_broad = x_broad[train_idx[ids_q]], x_broad[train_idx[ids_b]]\n",
    "    xv_broad = x_broad[val_idx]\n",
    "\n",
    "    # NW with rel as features\n",
    "    _, _, _, model_relfts = train_nw_arbitrary(\n",
    "        x_backgnd=xb_broad,\n",
    "        y_backgnd=yb,\n",
    "        x_query=xq_broad,\n",
    "        y_query=yq,\n",
    "        x_val=xv_broad,\n",
    "        y_val=yv,\n",
    "        r_query_backgnd=np.zeros((len(xq_broad), len(xb_broad))),\n",
    "        r_val_nonval=np.zeros((len(xv_broad), len(xb_broad) + len(xq_broad))),\n",
    "        cfg=model_cfg,\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "    )\n",
    "    metrics_x[\"rel-fts\"].append(mean_squared_error(yv, model_relfts.y_val_pred))\n",
    "\n",
    "    # LightGBM with rel as features\n",
    "    lgb_model_rel = lgb.train(lgb_params, lgb.Dataset(x_train_broad, label=y_train_lgb))\n",
    "    y_pred_lgb_rel = lgb_model_rel.predict(xv_broad)\n",
    "    metrics_x[\"lgb-rel\"].append(mean_squared_error(yv, y_pred_lgb_rel))\n",
    "\n",
    "    # RelNet (TabRel)\n",
    "    relnet_pehe, _, _, _, _ = train_relnet(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        r=r[train_idx][:, train_idx],\n",
    "        backgnd_indices=ids_b,\n",
    "        query_indices=ids_q,\n",
    "        val_indices=np.arange(len(X_val)),\n",
    "        lr=0.01,\n",
    "        n_epochs=800,\n",
    "        n_layers=2,\n",
    "        periodic_embed_dim=None,\n",
    "        num_heads=2,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "    metrics_x[\"relnet\"].append(relnet_pehe)\n",
    "\n",
    "# Show mean metrics\n",
    "{ k: round(np.mean(v), 1) for k, v in metrics_x.items() }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
