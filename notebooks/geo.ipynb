{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9889f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Final\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR: Final[Path] = Path('~/.cache/us_maps').resolve()\n",
    "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def plot_us_state_choropleth(\n",
    "    df: pd.DataFrame,\n",
    "    value_col: str,\n",
    "    cmap: str = 'OrRd',\n",
    "    set_col: str | None = None,\n",
    "    cache_dir: Path = CACHE_DIR,\n",
    "    out_path: Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a U.S. state choropleth map with optional border color coding by set.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame with state names (index) and a statistics column.\n",
    "    - value_col: Name of the statistics column.\n",
    "    - cmap: Matplotlib colormap to use (default: 'OrRd').\n",
    "    - set_col: Optional column name for border coloring with values: 'backgnd', 'query', 'val'.\n",
    "    - cache_dir: Directory to store or load the cached GeoJSON.\n",
    "    - out_path: Optional output path (pdf, png, etc) for the figure\n",
    "    \"\"\"\n",
    "    geojson_path = cache_dir / 'us_states.geojson'\n",
    "\n",
    "    if not geojson_path.exists():\n",
    "        url = 'https://raw.githubusercontent.com/jgoodall/us-maps/master/geojson/state.geo.json'\n",
    "        print(\"Downloading U.S. states GeoJSON...\")\n",
    "        import requests\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with geojson_path.open('wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Done downloading\")\n",
    "\n",
    "    usa_states = gpd.read_file(geojson_path)\n",
    "\n",
    "    # Normalize index casing\n",
    "    df = df.copy()\n",
    "    df.index = df.index.str.title()\n",
    "\n",
    "    # Rename and merge\n",
    "    usa_states = usa_states.rename(columns={'NAME10': 'state'})\n",
    "    merged = usa_states.set_index('state').join(df)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "    # Base plot\n",
    "    # Plot without legend\n",
    "    merged.plot(column=value_col, cmap=cmap, linewidth=0.8, ax=ax, edgecolor='0.8', legend=False)\n",
    "\n",
    "    # Create scalar mappable for colorbar\n",
    "    norm = mpl.colors.Normalize(vmin=merged[value_col].min(), vmax=merged[value_col].max())\n",
    "    sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "    # Add smaller colorbar\n",
    "    fig.colorbar(sm, ax=ax, fraction=0.025, pad=0.04)\n",
    "    linewidth: Final[float] = 2.5\n",
    "\n",
    "    if set_col and set_col in merged.columns:\n",
    "        # Color mappings for 'backgnd', 'query', 'val'\n",
    "        set_colors = {'backgnd': 'blue', 'query': 'green', 'val': 'orange'}\n",
    "\n",
    "        for set_value, color in set_colors.items():\n",
    "            subset = merged[merged[set_col] == set_value]\n",
    "            if not subset.empty:\n",
    "                subset.boundary.plot(ax=ax, edgecolor=color, linewidth=linewidth)\n",
    "\n",
    "        # Add legend boxes as text annotations with background color\n",
    "        ax.text(0.1, 0.24, \n",
    "                'background', \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=14,\n",
    "                bbox=dict(facecolor=\"white\", edgecolor=set_colors[\"backgnd\"], boxstyle=\"round,pad=0.3\", alpha=0.8, linewidth=linewidth),\n",
    "                )\n",
    "\n",
    "        ax.text(0.1, 0.17, \n",
    "                'query', \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=14,\n",
    "                bbox=dict(facecolor=\"white\", edgecolor=set_colors[\"query\"], boxstyle=\"round,pad=0.3\", alpha=0.8, linewidth=linewidth)\n",
    "                )\n",
    "\n",
    "        ax.text(0.1, 0.1, \n",
    "                'validate',\n",
    "                transform=ax.transAxes, \n",
    "                fontsize=14,\n",
    "                bbox=dict(facecolor=\"white\", edgecolor=set_colors[\"val\"], boxstyle=\"round,pad=0.3\", alpha=0.8, linewidth=linewidth),\n",
    "                )\n",
    "\n",
    "    ax.set_title(f'U.S. States Colored by {value_col}', fontsize=16)\n",
    "    ax.axis('off')\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed716d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_borders = {\n",
    "        'Alabama': ['Florida', 'Georgia', 'Mississippi', 'Tennessee'],\n",
    "        'Alaska': [],\n",
    "        'Arizona': ['California', 'Colorado', 'Nevada', 'New Mexico', 'Utah'],\n",
    "        'Arkansas': ['Louisiana', 'Mississippi', 'Missouri', 'Oklahoma', 'Tennessee', 'Texas'],\n",
    "        'California': ['Arizona', 'Nevada', 'Oregon'],\n",
    "        'Colorado': ['Arizona', 'Kansas', 'Nebraska', 'New Mexico', 'Oklahoma', 'Utah', 'Wyoming'],\n",
    "        'Connecticut': ['Massachusetts', 'New York', 'Rhode Island'],\n",
    "        'Delaware': ['Maryland', 'New Jersey', 'Pennsylvania'],\n",
    "        'Florida': ['Alabama', 'Georgia'],\n",
    "        'Georgia': ['Alabama', 'Florida', 'North Carolina', 'South Carolina', 'Tennessee'],\n",
    "        'Hawaii': [],\n",
    "        'Idaho': ['Montana', 'Nevada', 'Oregon', 'Utah', 'Washington', 'Wyoming'],\n",
    "        'Illinois': ['Indiana', 'Iowa', 'Michigan', 'Kentucky', 'Missouri', 'Wisconsin'],\n",
    "        'Indiana': ['Illinois', 'Kentucky', 'Michigan', 'Ohio'],\n",
    "        'Iowa': ['Illinois', 'Minnesota', 'Missouri', 'Nebraska', 'South Dakota', 'Wisconsin'],\n",
    "        'Kansas': ['Colorado', 'Missouri', 'Nebraska', 'Oklahoma'],\n",
    "        'Kentucky': ['Illinois', 'Indiana', 'Missouri', 'Ohio', 'Tennessee', 'Virginia', 'West Virginia'],\n",
    "        'Louisiana': ['Arkansas', 'Mississippi', 'Texas'],\n",
    "        'Maine': ['New Hampshire'],\n",
    "        'Maryland': ['Delaware', 'Pennsylvania', 'Virginia', 'West Virginia'],\n",
    "        'Massachusetts': ['Connecticut', 'New Hampshire', 'New York', 'Rhode Island', 'Vermont'],\n",
    "        'Michigan': ['Illinois', 'Indiana', 'Minnesota', 'Ohio', 'Wisconsin'],\n",
    "        'Minnesota': ['Iowa', 'Michigan', 'North Dakota', 'South Dakota', 'Wisconsin'],\n",
    "        'Mississippi': ['Alabama', 'Arkansas', 'Louisiana', 'Tennessee'],\n",
    "        'Missouri': ['Arkansas', 'Illinois', 'Iowa', 'Kansas', 'Kentucky', 'Nebraska', 'Oklahoma', 'Tennessee'],\n",
    "        'Montana': ['Idaho', 'North Dakota', 'South Dakota', 'Wyoming'],\n",
    "        'Nebraska': ['Colorado', 'Iowa', 'Kansas', 'Missouri', 'South Dakota', 'Wyoming'],\n",
    "        'Nevada': ['Arizona', 'California', 'Idaho', 'Oregon', 'Utah'],\n",
    "        'New Hampshire': ['Maine', 'Massachusetts', 'Vermont'],\n",
    "        'New Jersey': ['Delaware', 'New York', 'Pennsylvania'],\n",
    "        'New Mexico': ['Arizona', 'Colorado', 'Oklahoma', 'Texas', 'Utah'],\n",
    "        'New York': ['Connecticut', 'Massachusetts', 'New Jersey', 'Pennsylvania', 'Vermont'],\n",
    "        'North Carolina': ['Georgia', 'South Carolina', 'Tennessee', 'Virginia'],\n",
    "        'North Dakota': ['Minnesota', 'Montana', 'South Dakota'],\n",
    "        'Ohio': ['Indiana', 'Kentucky', 'Michigan', 'Pennsylvania', 'West Virginia'],\n",
    "        'Oklahoma': ['Arkansas', 'Colorado', 'Kansas', 'Missouri', 'New Mexico', 'Texas'],\n",
    "        'Oregon': ['California', 'Idaho', 'Nevada', 'Washington'],\n",
    "        'Pennsylvania': ['Delaware', 'Maryland', 'New Jersey', 'New York', 'Ohio', 'West Virginia'],\n",
    "        'Rhode Island': ['Connecticut', 'Massachusetts'],\n",
    "        'South Carolina': ['Georgia', 'North Carolina'],\n",
    "        'South Dakota': ['Iowa', 'Minnesota', 'Montana', 'Nebraska', 'North Dakota', 'Wyoming'],\n",
    "        'Tennessee': ['Alabama', 'Arkansas', 'Georgia', 'Kentucky', 'Mississippi', 'Missouri', 'North Carolina', 'Virginia'],\n",
    "        'Texas': ['Arkansas', 'Louisiana', 'New Mexico', 'Oklahoma'],\n",
    "        'Utah': ['Arizona', 'Colorado', 'Idaho', 'Nevada', 'New Mexico', 'Wyoming'],\n",
    "        'Vermont': ['Massachusetts', 'New Hampshire', 'New York'],\n",
    "        'Virginia': ['Kentucky', 'Maryland', 'North Carolina', 'Tennessee', 'West Virginia'],\n",
    "        'Washington': ['Idaho', 'Oregon'],\n",
    "        'West Virginia': ['Kentucky', 'Maryland', 'Ohio', 'Pennsylvania', 'Virginia'],\n",
    "        'Wisconsin': ['Illinois', 'Iowa', 'Michigan', 'Minnesota'],\n",
    "        'Wyoming': ['Colorado', 'Idaho', 'Montana', 'Nebraska', 'South Dakota', 'Utah']\n",
    "}\n",
    "\n",
    "def share_common_border_us_states(s1: str, s2: str) -> bool:\n",
    "    # Normalize inputs\n",
    "    s1 = s1.strip().title()\n",
    "    s2 = s2.strip().title()\n",
    "\n",
    "    if s1 not in state_borders or s2 not in state_borders:\n",
    "        raise ValueError(f\"State '{s1}' or '{s2}' is not a valid U.S. state\")\n",
    "\n",
    "    return s2 in state_borders[s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download laquery version\n",
    "path = kagglehub.dataset_download(\"justin2028/unemployment-in-america-per-us-state\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(list(Path(path).glob(\"*.csv\"))[0])\n",
    "example_year: Final[int] = 1976\n",
    "example_month: Final[int] = 1\n",
    "\n",
    "filtered_data = data[\n",
    "    data[\"State/Area\"].isin(state_borders.keys()) & \n",
    "    ~data[\"State/Area\"].isin([\"Alaska\", \"Hawaii\"])\n",
    "]\n",
    "filtered_data = filtered_data.set_index(filtered_data[\"State/Area\"])\n",
    "\n",
    "x_cols = [\"Total Civilian Labor Force in State/Area\",\n",
    "          \"Total Civilian Non-Institutional Population in State/Area\",\n",
    "          \"Percent (%) of State/Area\\'s Population\"\n",
    "          ]\n",
    "y_col = \"Percent (%) of Labor Force Employed in State/Area\"\n",
    "filtered_data[x_cols[:2]] = filtered_data[x_cols[:2]].apply(lambda col: pd.to_numeric(col.str.replace(\",\", \"\"), errors=\"coerce\"))\n",
    "\n",
    "\n",
    "def subset_month(year: int, month: int) -> pd.DataFrame:\n",
    "    data_year = filtered_data[filtered_data[\"Year\"] == year]\n",
    "    return data_year[data_year[\"Month\"] == month]\n",
    "\n",
    "\n",
    "\n",
    "data_month = subset_month(year=example_year, month=example_month)\n",
    "data_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_us_state_choropleth(data_month, value_col=y_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc83843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "x_cols_short = [\"Labor Force\", \"Population\", \"Percent Eligible\"]\n",
    "y_col_short = \"Percent Employed\"\n",
    "cols_short = x_cols_short + [y_col_short]\n",
    "renamed_data = data_month.rename(\n",
    "    columns={k:v for k, v in zip(x_cols + [y_col], cols_short)}\n",
    ")\n",
    "sns.pairplot(renamed_data, vars=cols_short)\n",
    "plt.savefig(\"employment_pairplot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d336f",
   "metadata": {},
   "source": [
    "First two features are almost perfectly correlated, so getting rid of the first feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols_reduced = x_cols[1:]\n",
    "x_cols_short_reduced = x_cols_short[1:]\n",
    "sns.scatterplot(data_month, x=x_cols_reduced[0], y=x_cols_reduced[1], hue=y_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Northwestern US states\n",
    "query_states = [\n",
    "    \"Washington\", \"Oregon\", \"Idaho\", \"Montana\", \"Wyoming\",\n",
    "    \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Minnesota\", \"Iowa\",\n",
    "    \"Colorado\", \"Utah\", \"Nevada\", \"Kansas\", \"Missouri\"\n",
    "]\n",
    "\n",
    "# Eastern US states\n",
    "val_states = [\n",
    "    \"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Connecticut\",\n",
    "    \"New York\", \"New Jersey\", \"Pennsylvania\", \"Delaware\", \"Maryland\",\n",
    "    \"Rhode Island\", \"Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\"\n",
    "]\n",
    "\n",
    "set_col: Final[str] = \"set\"\n",
    "data_month[set_col] = \"backgnd\"\n",
    "data_month.loc[query_states, set_col] = \"query\"\n",
    "data_month.loc[val_states, set_col] = \"val\"\n",
    "\n",
    "plot_us_state_choropleth(data_month, value_col=y_col, set_col=set_col, out_path=Path(f\"emloyment_{example_year}_{example_month}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ab29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from typing import Final\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tabrel.benchmark.nw_regr import FittedNwRegr, NwModelConfig, RelNwRegr\n",
    "\n",
    "LR: Final[float] = 0.006\n",
    "N_EPOCHS: Final[int] = 100\n",
    "\n",
    "def compute_relation_matrix(states: list[str]) -> np.ndarray:\n",
    "    n = len(states)\n",
    "    r = np.zeros((n, n))\n",
    "    for i, j in product(range(n), range(n)):\n",
    "        if share_common_border_us_states(states[i], states[j]):\n",
    "            r[i, j] = 1\n",
    "    return r\n",
    "\n",
    "def run_training(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    r: np.ndarray,\n",
    "    backgnd_indices: np.ndarray,\n",
    "    query_indices: np.ndarray,\n",
    "    val_indices: np.ndarray,\n",
    "    lr: float,\n",
    "    n_epochs: int,\n",
    ") -> dict[str, tuple[float, float, float, FittedNwRegr]]:\n",
    "    x_backgnd = x[backgnd_indices]\n",
    "    y_backgnd = y[backgnd_indices]\n",
    "    x_query = x[query_indices]\n",
    "    y_query = y[query_indices]\n",
    "    x_val = x[val_indices]\n",
    "    y_val = y[val_indices]\n",
    "\n",
    "    x_mean = np.mean(x_backgnd, axis=0, keepdims=True)\n",
    "    x_std = np.std(x_backgnd, axis=0, keepdims=True) + 1e-8\n",
    "\n",
    "    x_backgnd_norm = (x_backgnd - x_mean) / x_std\n",
    "    x_query_norm = (x_query - x_mean) / x_std\n",
    "    x_val_norm = (x_val - x_mean) / x_std\n",
    "\n",
    "    r_query_backgnd = r[np.ix_(query_indices, backgnd_indices)]\n",
    "    x_nonval_norm = np.concatenate((x_backgnd_norm, x_query_norm))\n",
    "    y_nonval = np.concatenate((y_backgnd, y_query))\n",
    "    r_val_nonval = r[np.ix_(val_indices, np.concatenate((backgnd_indices, query_indices)))]\n",
    "\n",
    "    # Convert to torch\n",
    "    x_backgnd_norm = torch.tensor(x_backgnd_norm, dtype=torch.float32)\n",
    "    y_backgnd = torch.tensor(y_backgnd, dtype=torch.float32)\n",
    "    x_query_norm = torch.tensor(x_query_norm, dtype=torch.float32)\n",
    "    y_query = torch.tensor(y_query, dtype=torch.float32)\n",
    "    r_query_backgnd = torch.tensor(r_query_backgnd, dtype=torch.float32)\n",
    "    x_val_norm = torch.tensor(x_val_norm, dtype=torch.float32)\n",
    "    x_nonval_norm = torch.tensor(x_nonval_norm, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    y_nonval = torch.tensor(y_nonval, dtype=torch.float32)\n",
    "    r_val_nonval = torch.tensor(r_val_nonval, dtype=torch.float32)\n",
    "\n",
    "    results_local = {}\n",
    "    model_cfg = NwModelConfig()\n",
    "\n",
    "    for use_rel in (True, False):\n",
    "        model = RelNwRegr(model_cfg)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_backgnd_norm, y_backgnd, x_query_norm,\n",
    "                           r_query_backgnd if use_rel else torch.zeros_like(r_query_backgnd))\n",
    "            loss = loss_fn(y_pred, y_query)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(x_nonval_norm, y_nonval, x_val_norm,\n",
    "                               r_val_nonval if use_rel else torch.zeros_like(r_val_nonval))\n",
    "            y_pred_val_np = y_pred_val.numpy()\n",
    "            y_val_np = y_val.numpy()\n",
    "\n",
    "            mse = mean_squared_error(y_val_np, y_pred_val_np)\n",
    "            r2 = r2_score(y_val_np, y_pred_val_np)\n",
    "            mae = mean_absolute_error(y_val_np, y_pred_val_np)\n",
    "\n",
    "            fitted_model = FittedNwRegr(\n",
    "                model=model,\n",
    "                x_backgnd=x_backgnd_norm,\n",
    "                x_query=x_query_norm,\n",
    "                y_backgnd=y_backgnd,\n",
    "                y_query_true=y_query,\n",
    "                clusters_query=None,\n",
    "                clusters_backgnd=None,\n",
    "                use_rel=None,\n",
    "            )\n",
    "            results_local[f\"rel={use_rel}\"] = (mse, r2, mae, fitted_model)\n",
    "\n",
    "    return results_local\n",
    "\n",
    "results = defaultdict(list)\n",
    "\n",
    "for year in tqdm(filtered_data[\"Year\"].unique()):\n",
    "    for month in range(1, 13):\n",
    "        data_month: pd.DataFrame = subset_month(year, month)\n",
    "        if data_month.empty:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            query_indices = data_month.index.get_indexer(query_states)\n",
    "            val_indices = data_month.index.get_indexer(val_states)\n",
    "            if np.any(query_indices == -1) or np.any(val_indices == -1):\n",
    "                continue\n",
    "\n",
    "            n_samples: Final[int] = len(data_month)\n",
    "            all_indices = np.arange(n_samples)\n",
    "            nonbackgnd_indices = np.array([*query_indices, *val_indices])\n",
    "            backgnd_indices = np.setdiff1d(all_indices, query_indices)\n",
    "\n",
    "            x = data_month[x_cols_reduced].to_numpy()\n",
    "            y = data_month[y_col].to_numpy()\n",
    "\n",
    "            r = compute_relation_matrix(data_month.index)\n",
    "\n",
    "            month_results = run_training(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                r=r,\n",
    "                backgnd_indices=backgnd_indices,\n",
    "                query_indices=query_indices,\n",
    "                val_indices=val_indices,\n",
    "                lr=LR,\n",
    "                n_epochs=N_EPOCHS,\n",
    "            )\n",
    "            for k, v in month_results.items():\n",
    "                results[k].append(v)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping year={year}, month={month}: {e}\")\n",
    "            continue\n",
    "    break  # to make execution faster, remove to reproduce results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c309a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract labels\n",
    "labels = list(results.keys())\n",
    "bar_locs = np.arange(len(labels))\n",
    "\n",
    "# Prepare data arrays\n",
    "metrics_all = {\n",
    "    \"Validation MSE\": [np.array([x[0] for x in results[k]]) for k in labels],\n",
    "    \"Validation $R^2$\": [np.array([x[1] for x in results[k]]) for k in labels],\n",
    "    # \"Validation MAE\": [np.array([x[2] for x in results[k]]) for k in labels],\n",
    "}\n",
    "colors = {\n",
    "    \"Validation MSE\": \"skyblue\",\n",
    "    \"Validation $R^2$\": \"lightgreen\",\n",
    "    # \"Validation MAE\": \"lightcoral\",\n",
    "}\n",
    "\n",
    "def plot_violin(ax: plt.Axes, data, title, color):\n",
    "    vp = ax.violinplot(data, positions=bar_locs, showmeans=True, showmedians=False)\n",
    "    for i, group in enumerate(data):\n",
    "        jitter = np.random.normal(0, 0.05, size=len(group))\n",
    "        ax.scatter(np.full(len(group), bar_locs[i]) + jitter, group, color='black', s=10, alpha=0.6)\n",
    "    for pc in vp['bodies']:\n",
    "        pc.set_facecolor(color)\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(bar_locs)\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, len(metrics_all), figsize=(5, 5), sharex=True)\n",
    "\n",
    "for ax, (label, data) in zip(axes, metrics_all.items()):\n",
    "    plot_violin(ax, data, title=label, color=colors[label])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"employment_violinplots.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc973bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7e867",
   "metadata": {},
   "source": [
    "# Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/vzuev/Documents/git/gh_zuevval/tabrel/data/birthrate_usStates.csv\", sep=\";\")\n",
    "df = df[:-2]\n",
    "for col in (\"Urban\", \"Birth rate\", \"PCPI\"):\n",
    "    df[col] = list(map(lambda s: float(s.replace(\",\", \".\")), df[col]))\n",
    "df = df.set_index(df[\"State\"])\n",
    "df = df[~df[\"State\"].isin([\"Alaska\", \"Hawaii\"])]\n",
    "\n",
    "df[set_col] = \"backgnd\"\n",
    "plot_us_state_choropleth(df, value_col=\"Birth rate\")\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db737e6a",
   "metadata": {},
   "source": [
    "# United States Energy, Census, and GDP 2010-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e484392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "path = kagglehub.dataset_download(\"lislejoem/us_energy_census_gdp_10-14\")\n",
    "gdp_df = pd.read_csv(next(Path(path).glob(\"*.csv\")))\n",
    "gdp_df = gdp_df[~gdp_df[\"State\"].isin((\"Alaska\", \"Hawaii\", \"District of Columbia\", \"United States\"))]\n",
    "\n",
    "gdp_year: Final[int] = 2014\n",
    "data_year = gdp_df.loc[:, gdp_df.columns.str.endswith(str(gdp_year))]\n",
    "data_year.index = gdp_df[\"State\"]\n",
    "\n",
    "def compute_gdp_per_capita(year: int) -> list[float]:\n",
    "    return [g/c for g, c in zip(gdp_df[f\"GDP{year}\"], gdp_df[\"CENSUS2010POP\"])]\n",
    "\n",
    "data_r = data_year.loc[:, data_year.columns.str.startswith(\"R\")]\n",
    "gdp_pc_col: Final[str] = \"gdpPerCap\"\n",
    "data_r.loc[:, gdp_pc_col] = compute_gdp_per_capita(gdp_year)\n",
    "\n",
    "\n",
    "# data_r.loc[:, \"rnatbd\"] = data_r[\"RBIRTH2014\"] - data_r[\"RDEATH2014\"]\n",
    "# data_r.loc[:, \"rdomintmig\"] = data_r[\"RINTERNATIONALMIG2014\"] + data_r[\"RDOMESTICMIG2014\"]\n",
    "# sns.pairplot(data_r)\n",
    "\n",
    "\n",
    "mig_col: Final[str] = f\"RINTERNATIONALMIG{gdp_year}\"\n",
    "sns.scatterplot(data_r, x=gdp_pc_col, y=mig_col)\n",
    "\n",
    "# Add dashed lines between neighboring states\n",
    "for s1, s2 in combinations(data_r.index, 2):\n",
    "    if share_common_border_us_states(s1, s2):\n",
    "        x1, y1 = data_r.loc[s1, gdp_pc_col], data_r.loc[s1, mig_col]\n",
    "        x2, y2 = data_r.loc[s2, gdp_pc_col], data_r.loc[s2, mig_col]\n",
    "        plt.plot([x1, x2], [y1, y2], linestyle='--', color='gray', linewidth=0.5)\n",
    "\n",
    "\n",
    "plot_us_state_choropleth(data_r, value_col=mig_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = gdp_df[\"State\"]\n",
    "gdp_df = gdp_df.set_index(states)\n",
    "states = states.tolist()\n",
    "r = compute_relation_matrix(states)\n",
    "\n",
    "all_indices = np.arange(len(states))\n",
    "val_indices = gdp_df.index.get_indexer(val_states)\n",
    "query_indices = gdp_df.index.get_indexer(query_states)\n",
    "backgnd_indices = np.setdiff1d(all_indices, np.concatenate((val_indices,  query_indices)))\n",
    "print(val_indices, query_indices, backgnd_indices)\n",
    "\n",
    "for year in range(2011, 2015):\n",
    "    gdp_col = f\"GDP{year}\"\n",
    "    mig_col = f\"RINTERNATIONALMIG{year}\"\n",
    "    gdp_df.loc[:, gdp_pc_col] = compute_gdp_per_capita(year)\n",
    "\n",
    "    x = gdp_df[[gdp_pc_col]].to_numpy()\n",
    "    y = gdp_df[mig_col].to_numpy()\n",
    "\n",
    "    \n",
    "    # Run training\n",
    "    results = run_training(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        r=r,\n",
    "        backgnd_indices=backgnd_indices,\n",
    "        query_indices=query_indices,\n",
    "        val_indices=val_indices,\n",
    "        lr=LR,\n",
    "        n_epochs=N_EPOCHS,\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(f\"=== Year {year} ===\")\n",
    "    # model = results[\"rel=False\"][-1][-1]\n",
    "    for key, values in results.items():\n",
    "        mse, r2, mae = values[:-1]\n",
    "        print(f\"{key}: MSE={mse:.3f}, R^2={r2:.3f}, MAE={mae:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr: FittedNwRegr = results[\"rel=False\"][3]\n",
    "\n",
    "# Step 1: Create x_grid from min to max of x_backgnd\n",
    "x_min = nr.x_backgnd.min().item()\n",
    "x_max = nr.x_backgnd.max().item()\n",
    "x_grid = torch.linspace(start=x_min, end=x_max, steps=200).unsqueeze(1)\n",
    "\n",
    "# Step 2: Create r (relative positions, assumed to be zero here)\n",
    "r = torch.zeros((len(x_grid), nr.x_backgnd.shape[0]))\n",
    "\n",
    "# Step 3: Predict y_grid using the model\n",
    "y_grid = nr.model(nr.x_backgnd, nr.y_backgnd, x_grid, r)\n",
    "\n",
    "# Step 4: Plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Original data (scatter)\n",
    "ax.scatter(nr.x_query.numpy(), nr.y_query_true.numpy(), color='blue', label='Query points')\n",
    "ax.scatter(nr.x_backgnd.numpy(), nr.y_backgnd.numpy(), color=\"black\", label=\"Background points\")\n",
    "\n",
    "# Model predictions (dashed line)\n",
    "ax.plot(x_grid.numpy(), y_grid.detach().numpy(), linestyle='--', color='red', label='Model prediction')\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494984b5",
   "metadata": {},
   "source": [
    "# World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pycountry\n",
    "from pathlib import Path\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mlippo/average-global-iq-per-country-with-other-stats\")\n",
    "\n",
    "df = pd.read_csv(list(Path(path).glob(\"*.csv\"))[0])\n",
    "\n",
    "def clean_data(df):\n",
    "    df['Population - 2023'] = df['Population - 2023'].str.replace('[,.]', '', regex=True)\n",
    "   \n",
    "    df = df.astype({'Population - 2023': 'int64'})\n",
    "    return df\n",
    "\n",
    "df_clean = clean_data(df.copy())\n",
    "df_clean['ISO_alpha'] = df_clean['Country'].apply(lambda x: pycountry.countries.get(name=x).alpha_3 if pycountry.countries.get(name=x) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig_geo = px.choropleth(\n",
    "    df_clean,\n",
    "    locations='ISO_alpha',\n",
    "    color='Literacy Rate',\n",
    "    color_continuous_scale=px.colors.sequential.YlOrRd,\n",
    "    # labels={'Average IQ': 'Average IQ'},\n",
    "    # title='Average IQ by Country',\n",
    ")\n",
    "# fig_geo.update_layout(title_x=0.5)\n",
    "fig_geo.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_geo.write_image(\"literacy_rate_by_country.png\", width=1000, height=600, scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f451b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load shapefile (adjust the path to where you extracted the data)\n",
    "world = gpd.read_file(\"/Users/vzuev/Documents/git/gh_zuevval/tabrel/data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\")\n",
    "\n",
    "# Ensure ISO_A3 codes exist and are valid\n",
    "world = world[world['ISO_A3_EH'] != '-99']  # Remove invalid entries\n",
    "len(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "def preprocess_geometry(geom):\n",
    "    \"\"\"\n",
    "    Selects the largest polygon and includes all others that touch it.\n",
    "    This avoids including remote territories like French Guiana (France).\n",
    "    \"\"\"\n",
    "    if isinstance(geom, Polygon):\n",
    "        return geom\n",
    "\n",
    "    elif isinstance(geom, MultiPolygon):\n",
    "        # Find the largest polygon (assumed to be mainland)\n",
    "        parts = list(geom.geoms)\n",
    "        mainland = max(parts, key=lambda p: p.area)\n",
    "\n",
    "        # Include all polygons that touch the mainland\n",
    "        touching_parts = [p for p in parts if p == mainland or p.touches(mainland)]\n",
    "\n",
    "        return unary_union(touching_parts)\n",
    "\n",
    "    return None\n",
    "\n",
    "def build_border_map(world):\n",
    "    border_map = {}\n",
    "\n",
    "    for _, country in tqdm(list(world.iterrows())):\n",
    "        iso_a3 = country['ISO_A3_EH']\n",
    "        geom = preprocess_geometry(country.geometry)\n",
    "\n",
    "        # Skip if geometry is missing or invalid\n",
    "        if not isinstance(geom, (Polygon, MultiPolygon)):\n",
    "            continue\n",
    "\n",
    "        neighbors = set()\n",
    "\n",
    "        for idx2, other_country in world.iterrows():\n",
    "            other_iso = other_country['ISO_A3_EH']\n",
    "            if iso_a3 == other_iso:\n",
    "                continue\n",
    "\n",
    "            other_geom = preprocess_geometry(other_country.geometry)\n",
    "\n",
    "            if not isinstance(other_geom, (Polygon, MultiPolygon)):\n",
    "                continue\n",
    "\n",
    "            # Check for shared border\n",
    "            if geom.touches(other_geom):\n",
    "                neighbors.add(other_iso)\n",
    "\n",
    "        border_map[iso_a3] = neighbors\n",
    "\n",
    "    return border_map\n",
    "\n",
    "border_map = build_border_map(world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f99489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def share_common_border(iso3_country1: str, iso3_country2: str, bm: dict = border_map) -> bool:\n",
    "    iso3_country1 = iso3_country1.upper()\n",
    "    iso3_country2 = iso3_country2.upper()\n",
    "    return iso3_country2 in bm.get(iso3_country1, set())\n",
    "\n",
    "print(share_common_border(\"FRA\", \"DEU\"))  # True\n",
    "print(share_common_border(\"FRA\", \"USA\"))  # False\n",
    "print(share_common_border(\"MEX\", \"GTM\"))  # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def get_connected_country_set(seed_iso3: str, bm: dict, max_size=10) -> set:\n",
    "    visited = set()\n",
    "    queue = deque([seed_iso3])\n",
    "\n",
    "    while queue and len(visited) < max_size:\n",
    "        country = queue.popleft()\n",
    "        if country in visited:\n",
    "            continue\n",
    "        visited.add(country)\n",
    "\n",
    "        # Enqueue unvisited neighbors\n",
    "        neighbors = bm.get(country, [])\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in visited:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ba6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "feature_cols = [\"Mean years of schooling - 2021\"]\n",
    "target_col = \"Literacy Rate\"\n",
    "\n",
    "df_filtered = df_clean.dropna().copy()\n",
    "df_filtered.set_index(\"ISO_alpha\", inplace=True)\n",
    "\n",
    "X = df_filtered[feature_cols].to_numpy()\n",
    "y = df_filtered[target_col].to_numpy()\n",
    "\n",
    "iso_list = list(df_filtered.index)\n",
    "iso_to_idx = {iso: i for i, iso in enumerate(iso_list)}\n",
    "all_isos = set(iso_list)\n",
    "\n",
    "N = len(df_filtered)\n",
    "R = np.zeros((N, N), dtype=int)\n",
    "\n",
    "# Build full adjacency matrix once\n",
    "for i, iso_i in enumerate(iso_list):\n",
    "    neighbors = border_map.get(iso_i, set())\n",
    "    for neighbor in neighbors:\n",
    "        j = iso_to_idx.get(neighbor)\n",
    "        if j is not None:\n",
    "            R[i, j] = 1\n",
    "            R[j, i] = 1  # symmetric\n",
    "\n",
    "r2s_rel = []\n",
    "r2s_norel = []\n",
    "\n",
    "mses_rel = []\n",
    "mses_norel = []\n",
    "\n",
    "for query_iso, val_iso in tqdm(\n",
    "    # list(combinations(iso_list, 2))\n",
    "    [(\"MNE\", \"EGY\")],  # for execution speed\n",
    "    ):\n",
    "    max_query_size, max_val_size = 40, 30\n",
    "    query_set = get_connected_country_set(query_iso, border_map, max_size=max_query_size)\n",
    "    validate_set = get_connected_country_set(val_iso, border_map, max_size=max_val_size)\n",
    "\n",
    "    if query_set & validate_set or len(query_set) < max_query_size - 5 or len(validate_set) < max_val_size - 5:\n",
    "        continue  # skip overlapping sets and those with too small connected countries\n",
    "\n",
    "    backgnd_set = all_isos - query_set - validate_set\n",
    "\n",
    "    backgnd_indices = [i for i, iso in enumerate(iso_list) if iso in backgnd_set]\n",
    "    query_indices = [i for i, iso in enumerate(iso_list) if iso in query_set]\n",
    "    val_indices = [i for i, iso in enumerate(iso_list) if iso in validate_set]\n",
    "\n",
    "    try:\n",
    "        res = run_training(\n",
    "            x=X, y=y, r=R,\n",
    "            backgnd_indices=np.array(backgnd_indices),\n",
    "            query_indices=np.array(query_indices),\n",
    "            val_indices=np.array(val_indices),\n",
    "            lr=LR,\n",
    "            n_epochs=50\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    r2_rel, r2_norel = res[\"rel=True\"][1], res[\"rel=False\"][1]\n",
    "    mse_rel, mse_norel = res[\"rel=True\"][0], res[\"rel=False\"][0]\n",
    "    if r2_norel < .5:\n",
    "        continue\n",
    "    r2s_rel.append(r2_rel)\n",
    "    r2s_norel.append(r2_norel)\n",
    "    mses_rel.append(mse_rel)\n",
    "    mses_norel.append(mse_norel)\n",
    "\n",
    "    if r2_rel < r2_norel + .02:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nQuery seed: {query_iso}, Validation seed: {val_iso}\")\n",
    "    print(\"With relation:\", res[\"rel=True\"][:-1])\n",
    "    print(\"Without relation:\", res[\"rel=False\"][:-1])\n",
    "\n",
    "    # Get fitted model from rel=True\n",
    "    fitted_rel = res[\"rel=True\"][-1]\n",
    "    fitted_norel = res[\"rel=False\"][-1]\n",
    "\n",
    "    # Plot\n",
    "    x_feat = X[:, 0]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(x_feat[query_indices], y[query_indices], label=\"Query\", color=\"blue\")\n",
    "    plt.scatter(x_feat[val_indices], y[val_indices], label=\"Validate\", color=\"orange\")\n",
    "    plt.scatter(x_feat[backgnd_indices], y[backgnd_indices], label=\"Background\", color=\"gray\")\n",
    "\n",
    "    # Model prediction line (from min to max of x)\n",
    "    x_min, x_max = x_feat.min(), x_feat.max()\n",
    "    x_grid = torch.linspace(x_min, x_max, steps=200).unsqueeze(1)\n",
    "\n",
    "    x_back = torch.tensor(X[backgnd_indices], dtype=torch.float32)\n",
    "    y_back = torch.tensor(y[backgnd_indices], dtype=torch.float32)\n",
    "\n",
    "    y_grid_norel = fitted_norel.model(x_back, y_back, x_grid, torch.zeros((len(x_grid), len(backgnd_indices))))\n",
    "    plt.plot(x_grid.numpy(), y_grid_norel.detach().numpy(), \"--\", label=\"Model fit (no rel)\", color=\"red\")\n",
    "\n",
    "    plt.xlabel(feature_cols[0])\n",
    "    plt.ylabel(target_col)\n",
    "    # plt.title(f\"{query_iso} as Query, {val_iso} as Val\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"q{query_iso}_val{val_iso}_literacy.pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5efd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(r2s_rel).mean(), np.array(r2s_norel).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ead026",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(5, 5), sharex=True)\n",
    "for ax, (label, data) in zip(\n",
    "    axes,\n",
    "    (\n",
    "        (\"Validation MSE\", [np.array(mses_rel), np.array(mses_norel)]),\n",
    "        (\"Validation $R^2$\", [np.array(r2s_rel), np.array(r2s_norel)]),\n",
    "    ),\n",
    "):\n",
    "    plot_violin(ax, data, title=label, color=colors[label])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"literacy_rate_violinplots.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_win = np.array(r2s_rel) > np.array(r2s_norel)\n",
    "print(sum(cases_win), len(cases_win))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209f0d7",
   "metadata": {},
   "source": [
    "# Birds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_species = Path(kagglehub.dataset_download(\"mexwell/bird-genetic-diversity\"))\n",
    "\n",
    "birds_df = pd.read_csv(next(path_species.glob(\"*\")))\n",
    "sns.pairplot(birds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41923b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(birds_df, x=\"Body mass\", y=\"Breeding range size\", hue=\"Allelic richness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78287fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    birds_df,\n",
    "    x=\"Body mass\",\n",
    "    y=\"Breeding range size\",\n",
    "    z=\"Allelic richness\",\n",
    "    color=\"Allelic richness\",  # You can use another column if you prefer\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_type=\"log\",\n",
    "        yaxis_type=\"log\",\n",
    "        zaxis_type=\"log\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_df[\"bm_log\"] = np.log(birds_df[\"Body mass\"])\n",
    "birds_df[\"range_log\"] = np.log(birds_df[\"Breeding range size\"])\n",
    "birds_df[\"richness_log\"] = np.log(birds_df[\"Allelic richness\"])\n",
    "birds_df = birds_df[[\"Species\", \"bm_log\", \"range_log\", \"richness_log\"]]\n",
    "birds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_taxa = Path(kagglehub.dataset_download(\"willianoliveiragibin/animal-analyzing\"))\n",
    "df_taxa = pd.read_csv(next(path_taxa.glob(\"*\")))\n",
    "df_taxa = df_taxa[[# \"Kingdom\", \"Subphylum\", \"Class\", # all birds belong to the same class - Aves\n",
    "                    \"Order\", \"Family\", \"Genus\", \"Species\"]]\n",
    "birds_df_merged = pd.merge(birds_df, df_taxa, on=\"Species\", how=\"inner\")\n",
    "birds_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = birds_df_merged[\"Order\"].unique()\n",
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceba750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycirclize import Circos\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import to_hex\n",
    "\n",
    "# --- Preprocessing ---\n",
    "# Filter orders with >=4 species\n",
    "species_counts = birds_df_merged.groupby(\"Order\")[\"Species\"].count()\n",
    "valid_orders = species_counts[species_counts >= 9].index\n",
    "\n",
    "\n",
    "# Filter the dataframe\n",
    "merged_df = birds_df_merged.copy()\n",
    "merged_df = merged_df[merged_df[\"Order\"].isin(valid_orders)]\n",
    "\n",
    "# Sort species within each Order by Family and Genus (and optionally Species)\n",
    "merged_df = merged_df.sort_values([\"Order\", \"Family\", \"Genus\", \"Species\"])\n",
    "\n",
    "merged_df[\"bm_log\"] -= merged_df[\"bm_log\"].min()\n",
    "merged_df[\"range_log\"] -= merged_df[\"range_log\"].min()\n",
    "merged_df[\"richness_log\"] -= merged_df[\"richness_log\"].min()\n",
    "\n",
    "# Shared value scales\n",
    "bm_min, bm_max = merged_df['bm_log'].min(), merged_df['bm_log'].max()\n",
    "range_min, range_max = merged_df['range_log'].min(), merged_df['range_log'].max()\n",
    "richness_min, richness_max = merged_df['richness_log'].min(), merged_df['richness_log'].max()\n",
    "\n",
    "# Sector = Order\n",
    "sector_sizes = merged_df.groupby('Order').size().to_dict()\n",
    "circos = Circos(sector_sizes, space=2)\n",
    "\n",
    "# Use local color maps per sector for better contrast\n",
    "from collections import defaultdict\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "# Store maps: (order -> {family: color}), etc.\n",
    "order_family_colors = defaultdict(dict)\n",
    "order_genus_colors = defaultdict(dict)\n",
    "\n",
    "family_cmaps = [cm.get_cmap(\"tab10\"), cm.get_cmap(\"Set1\"), cm.get_cmap(\"Dark2\")]\n",
    "genus_cmaps = [cm.get_cmap(\"tab20\"), cm.get_cmap(\"Paired\"), cm.get_cmap(\"tab20c\")]\n",
    "\n",
    "# Cycle through a few high-contrast maps\n",
    "family_map_count = len(family_cmaps)\n",
    "genus_map_count = len(genus_cmaps)\n",
    "\n",
    "for sector_idx, order in enumerate(merged_df[\"Order\"].unique()):\n",
    "    sub_df = merged_df[merged_df[\"Order\"] == order]\n",
    "    \n",
    "    families = sub_df[\"Family\"].unique()\n",
    "    genera = sub_df[\"Genus\"].unique()\n",
    "    \n",
    "    fam_cmap = family_cmaps[sector_idx % family_map_count]\n",
    "    gen_cmap = genus_cmaps[sector_idx % genus_map_count]\n",
    "    \n",
    "    for i, fam in enumerate(families):\n",
    "        order_family_colors[order][fam] = to_hex(fam_cmap(i % fam_cmap.N))\n",
    "    \n",
    "    for i, gen in enumerate(genera):\n",
    "        order_genus_colors[order][gen] = to_hex(gen_cmap(i % gen_cmap.N))\n",
    "\n",
    "\n",
    "# --- Build Circos Plot ---\n",
    "for sector in circos.sectors:\n",
    "    order = sector.name\n",
    "    sub_df = merged_df[merged_df['Order'] == order].reset_index(drop=True)\n",
    "    x = np.arange(len(sub_df)) + 0.5\n",
    "\n",
    "    # Family track\n",
    "    fam_colors = sub_df[\"Family\"].map(order_family_colors[order]).values\n",
    "    fam_track = sector.add_track((98, 100))\n",
    "    fam_track.axis()\n",
    "    fam_track.bar(x, np.ones_like(x), color=fam_colors, width=1)\n",
    "\n",
    "    # Genus track\n",
    "    gen_colors = sub_df[\"Genus\"].map(order_genus_colors[order]).values\n",
    "    gen_track = sector.add_track((95, 97))\n",
    "    gen_track.axis()\n",
    "    gen_track.bar(x, np.ones_like(x), color=gen_colors, width=1)\n",
    "\n",
    "    bm_track = sector.add_track((30, 49))\n",
    "    bm_track.axis()\n",
    "    bm_track.bar(x, sub_df['bm_log'].values, color='blue', width=0.6, vmin=bm_min, vmax=bm_max)\n",
    "\n",
    "    range_track = sector.add_track((50, 69))\n",
    "    range_track.axis()\n",
    "    range_track.bar(x, sub_df['range_log'].values, color='green', width=0.6, vmin=range_min, vmax=range_max)\n",
    "\n",
    "    richness_track = sector.add_track((70, 94))\n",
    "    richness_track.axis()\n",
    "    richness_track.bar(x, sub_df['richness_log'].values, color='red', width=0.6, vmin=richness_min, vmax=richness_max)\n",
    "\n",
    "\n",
    "    # Add order label\n",
    "    sector.text(order, \n",
    "                r=102, \n",
    "                size=8)\n",
    "\n",
    "# Plot the figure\n",
    "fig = circos.plotfig(dpi=300)\n",
    "\n",
    "# Add variable legend\n",
    "legend_patches = [\n",
    "    Patch(color='blue'),\n",
    "    Patch(color='green'),\n",
    "    Patch(color='red')\n",
    "]\n",
    "fig.legend(legend_patches, \n",
    "           ['Body mass (log)', 'Breeding range (log)', 'Genetic richness (log)'],\n",
    "           loc='upper left', \n",
    "           fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"circos_species_by_order.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume merged_df is already defined as in your notebook\n",
    "orders = merged_df[\"Order\"].astype(str).values\n",
    "families = merged_df[\"Family\"].astype(str).values\n",
    "n = len(merged_df)\n",
    "\n",
    "# Build r_birds matrix\n",
    "r_birds = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if families[i] == families[j]:\n",
    "            r_birds[i, j] = 1.0\n",
    "        elif orders[i] == orders[j]:\n",
    "            r_birds[i, j] = 0.5\n",
    "        else:\n",
    "            r_birds[i, j] = 0.0\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(n)\n",
    "n_test = int(0.2 * n)\n",
    "n_query = int(0.2 * n)\n",
    "test_indices = indices[:n_test]\n",
    "query_indices = indices[n_test:n_test + n_query]\n",
    "back_indices = indices[n_test + n_query:]\n",
    "\n",
    "# Prepare data\n",
    "X = merged_df[[\"bm_log\", \"range_log\"]].to_numpy()\n",
    "y = merged_df[\"richness_log\"].to_numpy()\n",
    "\n",
    "res_birds = run_training(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    r=r_birds,\n",
    "    backgnd_indices=back_indices,\n",
    "    query_indices=query_indices,\n",
    "    val_indices=test_indices,\n",
    "    lr=0.05,\n",
    "    n_epochs=200,\n",
    ")\n",
    "\n",
    "print(\"With relationships:\", res_birds[\"rel=True\"][:3])\n",
    "print(\"Without relationships:\", res_birds[\"rel=False\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cad750",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_birds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb833562",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_birds[\"rel=True\"][3].model.r_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f578c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Make sure categories are treated as such\n",
    "merged_df[\"Order\"] = merged_df[\"Order\"].astype(\"category\")\n",
    "merged_df[\"Family\"] = merged_df[\"Family\"].astype(\"category\")\n",
    "merged_df[\"Genus\"] = merged_df[\"Genus\"].astype(\"category\")\n",
    "\n",
    "# Order-level model\n",
    "model_order = smf.ols(\"richness_log ~ C(Order)\", data=merged_df).fit()\n",
    "anova_order = sm.stats.anova_lm(model_order, typ=2)\n",
    "print(anova_order)\n",
    "\n",
    "# Family-level model\n",
    "model_family = smf.ols(\"richness_log ~ C(Family)\", data=merged_df).fit()\n",
    "anova_family = sm.stats.anova_lm(model_family, typ=2)\n",
    "print(anova_family)\n",
    "\n",
    "# Genus-level model\n",
    "model_genus = smf.ols(\"richness_log ~ C(Genus)\", data=merged_df).fit()\n",
    "anova_genus = sm.stats.anova_lm(model_genus, typ=2)\n",
    "print(anova_genus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415601fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed-effects model\n",
    "model = smf.mixedlm(\n",
    "    \"richness_log ~ bm_log + range_log\",         # fixed effects\n",
    "    data=merged_df,\n",
    "    groups=merged_df[\"Order\"],                   # main grouping\n",
    "    re_formula=\"1\",                              # random intercepts\n",
    "    vc_formula={                                 # variance components\n",
    "        \"Family\": \"0 + C(Family)\",\n",
    "        \"Genus\": \"0 + C(Genus)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21dd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "coefs = result.fe_params\n",
    "conf_int = result.conf_int()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(coefs.index, coefs.values, \n",
    "            yerr=(conf_int[1][:3] - coefs.values),\n",
    "             fmt='o', capsize=5, color='black')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title(\"Fixed Effects Estimates\")\n",
    "plt.ylabel(\"Coefficient Estimate\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_comps = result.cov_re\n",
    "vc = result.vcomp\n",
    "\n",
    "# Print variance estimates by group\n",
    "print(\"Random effect variances:\")\n",
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fitted = result.fittedvalues\n",
    "resid = result.resid\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x=fitted, y=resid)\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(resid, line='s')\n",
    "plt.title(\"QQ Plot of Residuals\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8fd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
