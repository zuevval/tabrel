{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b407b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "from datetime import datetime\n",
    "from typing import Final\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from tabrel.benchmark.nw_regr import generate_multidim_noisy_data, make_random_r, train_nw_arbitrary, NwModelConfig\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 out_dim: int,\n",
    "                 dropout: float = .1,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "def to_torch(arr: np.ndarray) -> torch.Tensor:\n",
    "    return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def test_mlp() -> None:\n",
    "    housing = fetch_california_housing()\n",
    "    x, y = to_torch(housing.data), to_torch(housing.target)\n",
    "    mlp = Mlp(in_dim=x.shape[1], hidden_dim=64, out_dim=8)\n",
    "    print(mlp(x).shape)\n",
    "\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelNwMlpRegr(nn.Module):\n",
    "    \"\"\"\n",
    "    Relationship-aware Nadaraya-Watson kernel regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 init_sigma: float,\n",
    "                 init_r_scale: float,\n",
    "                 dropout: float,\n",
    "                 input_dim: int,\n",
    "                 mlp_hidden_dim: int = 64,\n",
    "                 mlp_out_dim: int = 8,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.sigma = nn.Parameter(torch.tensor([float(init_sigma)]))\n",
    "        self.r_scale = nn.Parameter(torch.tensor([float(init_r_scale)]))\n",
    "        self.x_tranform = Mlp(\n",
    "            in_dim=input_dim,\n",
    "            hidden_dim=mlp_hidden_dim,\n",
    "            out_dim=mlp_out_dim,\n",
    "            dropout=dropout)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_backgnd: torch.Tensor,  # (n_backgnd, n_features)\n",
    "        y_backgnd: torch.Tensor,  # (n_backgnd,)\n",
    "        x_query: torch.Tensor,  # (n_query, n_features)\n",
    "        r: torch.Tensor,  # (n_query, n_backgnd)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns predicted y: (n_query,)\n",
    "        \"\"\"\n",
    "        x_backgnd, x_query = self.x_tranform(x_backgnd), self.x_tranform(x_query)\n",
    "        \n",
    "        n_query, n_backgnd = r.shape\n",
    "        x_query_exp = x_query.unsqueeze(1).expand(\n",
    "            -1, n_backgnd, -1\n",
    "        )  # (n_query, n_backgnd, n_features)\n",
    "        x_backgnd_exp = x_backgnd.unsqueeze(0).expand(\n",
    "            n_query, -1, -1\n",
    "        )  # (n_query, n_backgnd, n_features)\n",
    "\n",
    "        dists = torch.norm(x_query_exp - x_backgnd_exp, dim=2)\n",
    "\n",
    "        # Compute kernel weights: (n_query, n_backgnd)\n",
    "        k_vals = torch.exp(-dists / self.sigma + self.r_scale * r)\n",
    "\n",
    "        # Normalize weights\n",
    "        k_sum = k_vals.sum(dim=1, keepdim=True) + 1e-8  # avoid division by zero\n",
    "        weights = k_vals / k_sum  # (n_query, n_backgnd)\n",
    "\n",
    "        # Weighted sum of y_backgnd: (n_query,)\n",
    "        y_pred = torch.matmul(weights, y_backgnd)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd89d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples: Final[int] = 1000\n",
    "seed: Final[int] = 42\n",
    "x_dim: Final[int] = 7\n",
    "x_np, y_np, c = generate_multidim_noisy_data(n_samples, n_clusters=3, x_dim=x_dim, seed=seed)\n",
    "\n",
    "n_query = n_back = n_samples // 3\n",
    "n_train = n_query + n_back\n",
    "n_val = n_samples - n_train\n",
    "back_ids = np.arange(n_back)\n",
    "query_ids = np.arange(n_query) + n_back\n",
    "train_ids = np.arange(n_train)\n",
    "val_ids = np.arange(n_val) + n_train\n",
    "\n",
    "r = make_random_r(seed, c)\n",
    "\n",
    "n_epochs: Final[int] = 5000\n",
    "mse, r2, _, _ = train_nw_arbitrary(\n",
    "    x_backgnd=x_np[back_ids],\n",
    "    y_backgnd=y_np[back_ids],\n",
    "    x_query=x_np[query_ids],\n",
    "    y_query=y_np[query_ids],\n",
    "    x_val=x_np[val_ids],\n",
    "    y_val=y_np[val_ids],\n",
    "    r_query_backgnd=r[query_ids][:, back_ids],\n",
    "    r_val_nonval=r[val_ids][:, train_ids],\n",
    "    cfg=NwModelConfig(input_dim=x_dim, trainable_weights_matrix=True,),\n",
    "    lr=1e-3,\n",
    "    n_epochs=n_epochs\n",
    ")\n",
    "\n",
    "print(mse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfe17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nw_mlp(\n",
    "        seed: int,\n",
    "        mlp_hid_dim: int,\n",
    "        mlp_out_dim: int,\n",
    "        dropout: float,\n",
    "        weight_decay: float,\n",
    "        writer: SummaryWriter | None) -> float:\n",
    "    x_torch, y_torch = to_torch(x_np), to_torch(y_np)\n",
    "    x_train, y_train = x_torch[train_ids], y_torch[train_ids]\n",
    "    x_mean, x_std = x_train.mean(dim=0), x_train.std(dim=0)\n",
    "    x_norm = (x_torch - x_mean) / (x_std + 1e-8)\n",
    "\n",
    "    x_b, y_b = x_norm[back_ids], y_torch[back_ids]\n",
    "    x_q, y_q = x_norm[query_ids], y_torch[query_ids]\n",
    "    x_v, y_v = x_norm[val_ids], y_torch[val_ids]\n",
    "\n",
    "    r_q_b = to_torch(r)[query_ids][:, back_ids]\n",
    "    r_val_nval = to_torch(r)[val_ids][:, train_ids]\n",
    "\n",
    "    torch.random.manual_seed(seed)\n",
    "    model = RelNwMlpRegr(\n",
    "        init_sigma=1.,\n",
    "        init_r_scale=1.,\n",
    "        input_dim=x_dim,\n",
    "        mlp_hidden_dim=mlp_hid_dim,\n",
    "        mlp_out_dim=mlp_out_dim,\n",
    "        dropout=.2,\n",
    "\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    last_val_r2: float | None = None\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_b, y_b, x_q, r_q_b)\n",
    "        loss = loss_fn(y_pred, y_q)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if writer:\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), epoch)\n",
    "\n",
    "        if epoch % 20 != 0: continue\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_v_pred = model(\n",
    "                x_norm[train_ids],\n",
    "                y_train,\n",
    "                x_v,\n",
    "                r_val_nval,\n",
    "            )\n",
    "            y_v_pred_np = y_v_pred.numpy()\n",
    "            y_v_np = y_v.numpy()\n",
    "\n",
    "        r2 = r2_score(y_v_np, y_v_pred_np)\n",
    "        if writer:\n",
    "            writer.add_scalar(\"val/mse\", mean_squared_error(y_v_np, y_v_pred_np), epoch)\n",
    "            writer.add_scalar(\"val/r2\", r2, epoch)\n",
    "        \n",
    "        last_val_r2 = r2\n",
    "        model.train()\n",
    "    \n",
    "    if last_val_r2 is None:\n",
    "        raise ValueError(\"last_val_r2 is None, probably not enough iterations\")\n",
    "    return last_val_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24646be",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir: Final[str] = \"tb_logs\" + datetime.isoformat(datetime.now()).replace(\":\", \"_\")\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {log_dir}\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "train_nw_mlp(\n",
    "    seed=seed,\n",
    "    mlp_hid_dim=64,\n",
    "    mlp_out_dim=6,\n",
    "    dropout=.2,\n",
    "    weight_decay=1e-4,\n",
    "    writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9134147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0., 1e-2)\n",
    "    mlp_hid_dim = trial.suggest_int(\"mlp_hid_dim\", 6, 100)\n",
    "    mlp_out_dim = trial.suggest_int(\"mlp_out_dim\", 1, 20)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0., .5)\n",
    "    return train_nw_mlp(\n",
    "        seed=seed,\n",
    "        mlp_hid_dim=mlp_hid_dim,\n",
    "        mlp_out_dim=mlp_out_dim,\n",
    "        dropout=dropout,\n",
    "        weight_decay=weight_decay,\n",
    "        writer=None,\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=f\"r_deterministic_{datetime.now()}\", storage=\"sqlite:///db.sqlite3\")\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0e74",
   "metadata": {},
   "source": [
    "code for loading an existing trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e86e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    storage=\"sqlite:///db.sqlite3\",\n",
    "    study_name=\"r_deterministic_2025-11-07 14:52:56.356900\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(f\"Best trial: {study.best_trial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO transform Y (with another MLP?)\n",
    "# TODO another synthetic?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
