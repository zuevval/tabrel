{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b407b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "from datetime import datetime\n",
    "from typing import Final\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from tabrel.benchmark.nw_regr import Mlp, MlpConfig, generate_multidim_noisy_data, make_random_r, to_torch, train_nw_arbitrary, NwModelConfig, RelNwRegr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_mlp() -> None:\n",
    "    housing = fetch_california_housing()\n",
    "    x, y = to_torch(housing.data), to_torch(housing.target)\n",
    "    mlp = Mlp(MlpConfig(in_dim=x.shape[1], hidden_dim=64, out_dim=8, dropout=.1))\n",
    "    print(mlp(x).shape)\n",
    "\n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd89d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples: Final[int] = 300\n",
    "seed: Final[int] = 42\n",
    "x_dim: Final[int] = 6\n",
    "n_epochs: Final[int] = 5000\n",
    "\n",
    "# mse, r2, _, _ = train_nw_arbitrary(\n",
    "#     x_backgnd=x_np[back_ids],\n",
    "#     y_backgnd=y_np[back_ids],\n",
    "#     x_query=x_np[query_ids],\n",
    "#     y_query=y_np[query_ids],\n",
    "#     x_val=x_np[val_ids],\n",
    "#     y_val=y_np[val_ids],\n",
    "#     r_query_backgnd=r[query_ids][:, back_ids],\n",
    "#     r_val_nonval=r[val_ids][:, train_ids],\n",
    "#     cfg=NwModelConfig(input_dim=x_dim, trainable_weights_matrix=True,),\n",
    "#     lr=1e-3,\n",
    "#     n_epochs=n_epochs\n",
    "# )\n",
    "\n",
    "# print(mse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfe17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nw_mlp(\n",
    "        seed: int,\n",
    "        mlp_hid_dim: int,\n",
    "        mlp_out_dim: int,\n",
    "        dropout: float,\n",
    "        weight_decay: float,\n",
    "        writer: SummaryWriter | None,\n",
    "        trainable_weights: bool = False, # TODO try True\n",
    "        _n_epochs: int = n_epochs,\n",
    "        ) -> tuple[float, float]:\n",
    "    x_np, y_np, c = generate_multidim_noisy_data(n_samples, n_clusters=3, x_dim=x_dim, seed=seed)\n",
    "    r = make_random_r(seed, c)\n",
    "\n",
    "    n_query = n_back = n_samples // 3\n",
    "    n_train = n_query + n_back\n",
    "    n_val = n_samples - n_train\n",
    "    back_ids = np.arange(n_back)\n",
    "    query_ids = np.arange(n_query) + n_back\n",
    "    train_ids = np.arange(n_train)\n",
    "    val_ids = np.arange(n_val) + n_train\n",
    "\n",
    "    x_torch, y_torch = to_torch(x_np), to_torch(y_np)\n",
    "    x_train, y_train = x_torch[train_ids], y_torch[train_ids]\n",
    "    x_mean, x_std = x_train.mean(dim=0), x_train.std(dim=0)\n",
    "    x_norm = (x_torch - x_mean) / (x_std + 1e-8)\n",
    "\n",
    "    x_b, y_b = x_norm[back_ids], y_torch[back_ids]\n",
    "    x_q, y_q = x_norm[query_ids], y_torch[query_ids]\n",
    "    x_v, y_v = x_norm[val_ids], y_torch[val_ids]\n",
    "\n",
    "    r_q_b = to_torch(r)[query_ids][:, back_ids]\n",
    "    r_val_nval = to_torch(r)[val_ids][:, train_ids]\n",
    "\n",
    "    torch.random.manual_seed(seed)\n",
    "    config = NwModelConfig(\n",
    "        input_dim=mlp_out_dim,\n",
    "        init_sigma=1.,\n",
    "        init_r_scale=1.,\n",
    "        trainable_weights_matrix=trainable_weights,\n",
    "        mlp_config=MlpConfig(\n",
    "            in_dim=x_dim,\n",
    "            hidden_dim=mlp_hid_dim,\n",
    "            out_dim=mlp_out_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    )\n",
    "    model = RelNwRegr(config)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    last_val_r2: float | None = None\n",
    "    last_val_mse: float | None = None\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(_n_epochs)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_b, y_b, x_q, r_q_b)\n",
    "        loss = loss_fn(y_pred, y_q)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if writer:\n",
    "            writer.add_scalar(\"train/loss\", loss.item(), epoch)\n",
    "\n",
    "        if epoch % 20 != 0: continue\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_v_pred = model(\n",
    "                x_norm[train_ids],\n",
    "                y_train,\n",
    "                x_v,\n",
    "                r_val_nval,\n",
    "            )\n",
    "            y_v_pred_np = y_v_pred.numpy()\n",
    "            y_v_np = y_v.numpy()\n",
    "\n",
    "        last_val_mse = mean_squared_error(y_v_np, y_v_pred_np)\n",
    "        last_val_r2 = r2_score(y_v_np, y_v_pred_np)\n",
    "        if writer:\n",
    "            writer.add_scalar(\"val/mse\", last_val_mse, epoch)\n",
    "            writer.add_scalar(\"val/r2\", last_val_r2, epoch)\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    if last_val_r2 is None:\n",
    "        raise ValueError(\"last_val_r2 is None, probably not enough iterations\")\n",
    "    return last_val_mse if last_val_mse is not None else float('inf'), last_val_r2 if last_val_r2 is not None else -float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24646be",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir: Final[str] = \"tb_logs\" + datetime.isoformat(datetime.now()).replace(\":\", \"_\")\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir {log_dir}\n",
    "\n",
    "# writer = SummaryWriter(log_dir=log_dir)\n",
    "# train_nw_mlp(\n",
    "#     seed=seed,\n",
    "#     mlp_hid_dim=64,\n",
    "#     mlp_out_dim=6,\n",
    "#     dropout=.2,\n",
    "#     weight_decay=1e-4,\n",
    "#     writer=writer)\n",
    "\n",
    "mses, r2s = [], []\n",
    "for _seed in range(15):\n",
    "    # mse, r2 = train_nw_mlp(\n",
    "    #     seed=_seed,\n",
    "    #     weight_decay=0.0006913422,\n",
    "    #     mlp_hid_dim=8,\n",
    "    #     mlp_out_dim=20,\n",
    "    #     dropout=0.09302061,\n",
    "    #     writer=None,\n",
    "    # )\n",
    "    # 0.23309 var 0.00074\n",
    "    # 0.81283 var 0.00048\n",
    "    mse, r2 = train_nw_mlp(\n",
    "        seed=_seed,\n",
    "        weight_decay=0.002,\n",
    "        mlp_hid_dim=10,\n",
    "        mlp_out_dim=20,\n",
    "        dropout=.19,\n",
    "        _n_epochs=n_epochs,\n",
    "        trainable_weights=False,\n",
    "        writer=None,\n",
    "    )\n",
    "    # 0.23555 var 0.00118\n",
    "    # 0.81012 var 0.00048\n",
    "    mses.append(mse)\n",
    "    r2s.append(r2)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for metrics in (mses, r2s):\n",
    "    print(f\"{np.mean(metrics):.5f} var {np.var(metrics):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9134147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0., 1e-2)\n",
    "    mlp_hid_dim = trial.suggest_int(\"mlp_hid_dim\", 6, 100)\n",
    "    mlp_out_dim = trial.suggest_int(\"mlp_out_dim\", 1, 20)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0., .5)\n",
    "    val_r2 = train_nw_mlp(\n",
    "        seed=seed,\n",
    "        mlp_hid_dim=mlp_hid_dim,\n",
    "        mlp_out_dim=mlp_out_dim,\n",
    "        dropout=dropout,\n",
    "        weight_decay=weight_decay,\n",
    "        writer=None,\n",
    "    )[1]\n",
    "    return val_r2\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=f\"r_random_{datetime.now()}\", storage=\"sqlite:///db.sqlite3\")\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba0e74",
   "metadata": {},
   "source": [
    "code for loading an existing trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e86e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    storage=\"sqlite:///db.sqlite3\",\n",
    "    study_name=\"r_deterministic_2025-11-07 14:52:56.356900\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "# study.optimize(objective, n_trials=10)\n",
    "print(f\"Best trial: {study.best_trial}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00de955",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {log_dir} \n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "train_nw_mlp(\n",
    "    seed=seed,\n",
    "    mlp_hid_dim=study.best_params[\"mlp_hid_dim\"],\n",
    "    mlp_out_dim=study.best_params[\"mlp_out_dim\"],\n",
    "    dropout=study.best_params[\"dropout\"],\n",
    "    weight_decay=study.best_params[\"weight_decay\"],\n",
    "    writer=writer,\n",
    "    _n_epochs = 10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO transform Y (with another MLP?)\n",
    "# TODO embeddings AND learnable norm (+ weight decay?)\n",
    "# TODO try RTDL num embeddings instead of or before perceptrons\n",
    "# TODO another synthetic?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
