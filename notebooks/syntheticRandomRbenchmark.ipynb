{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tabrel.train import train_relnet\n",
    "from tabrel.benchmark.nw_regr import run_training\n",
    "from tabrel.benchmark.nw_regr import generate_toy_regr_data\n",
    "\n",
    "\n",
    "def generate_multidim_noisy_data(n_samples: int, n_clusters: int, x_dim: int, seed: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.uniform(-1, 1, (n_samples, x_dim))\n",
    "    clusters = np.random.randint(0, n_clusters, (n_samples,))\n",
    "    y = np.sin(x[:, 0]) + np.cos(x[:, 1]) + x[:, 2] + clusters\n",
    "    return x, y, clusters\n",
    "\n",
    "def make_random_r(seed: int, clusters: np.ndarray) -> np.ndarray:\n",
    "    np.random.seed(seed)\n",
    "    n_samples = len(clusters)\n",
    "    r = np.eye(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i):\n",
    "            if clusters[i] == clusters[j] and np.random.choice((True, False)):\n",
    "                r[i, j] = 1\n",
    "                r[j, i] = 1\n",
    "    return r\n",
    "\n",
    "n_samples = 300\n",
    "n_ones = 70\n",
    "\n",
    "for y_func in (\n",
    "    # \"square\",\n",
    "    # \"sign\", \n",
    "    \"noisy\",\n",
    "    ):\n",
    "    r2_list = []\n",
    "    mse_list = []\n",
    "    for seed in tqdm(range(1), desc=y_func):\n",
    "        if y_func == \"noisy\":\n",
    "            x, y, c = generate_multidim_noisy_data(n_samples=n_samples, n_clusters=3, x_dim=6, seed=seed)\n",
    "        else:\n",
    "            x, y, c = generate_toy_regr_data(n_samples=n_samples, n_clusters=3, y_func=y_func, seed=seed)\n",
    "            x = x.numpy()\n",
    "            y = y.numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        n_query = n_validate = n_samples // 3\n",
    "        samples_indices = np.array(list(range(n_samples)))\n",
    "        validate_indices = samples_indices[:n_validate]\n",
    "        query_indices = samples_indices[n_validate: n_query]\n",
    "        back_indices = samples_indices[n_query:]\n",
    "\n",
    "        r = make_random_r(seed, c)\n",
    "        results = run_training(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            r=r,\n",
    "            backgnd_indices=back_indices,\n",
    "            query_indices=query_indices,\n",
    "            val_indices=validate_indices,\n",
    "            lr=3e-3,\n",
    "            n_epochs=1500,\n",
    "            rel_as_feats=r,\n",
    "        )\n",
    "\n",
    "        # results[\"relnet\"] = train_relnet(\n",
    "        #     x=x,\n",
    "        #     y=y,\n",
    "        #     r=r,\n",
    "        #     backgnd_indices=back_indices,\n",
    "        #     query_indices=query_indices,\n",
    "        #     val_indices=validate_indices,\n",
    "        #     lr=1e-3,\n",
    "        #     num_heads=4,\n",
    "        #     progress_bar=False,\n",
    "        #     n_epochs=1000,\n",
    "        # )\n",
    "        \n",
    "        mse_list.append({k:v[0] for k,v in results.items()})\n",
    "        r2_list.append({k:v[1] for k, v in results.items()})\n",
    "    \n",
    "    print(f\"=== {y_func} ===\")\n",
    "    for label, lst in ((\"mse\", mse_list), (\"r2\", r2_list)):\n",
    "        results_df = pd.DataFrame(lst)\n",
    "        print(\"\\n\")\n",
    "        print(label)\n",
    "        print(pd.DataFrame({\"mean\": results_df.mean().round(3), \"std\": results_df.std().round(3)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba72b3",
   "metadata": {},
   "source": [
    "Well, I give up on finding errors in my initial code, so I will just rewrite everything in a single cell. Of course, I'll copy some code from my vectorized NW implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f617fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "class RelNw(nn.Module):\n",
    "    w: torch.Tensor | None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_sigma: float,\n",
    "        init_r_scale: float,\n",
    "        input_dim: int,\n",
    "        learnable_norm: bool,\n",
    "        # TODO w\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.sigma = nn.Parameter(torch.tensor([float(init_sigma)]))\n",
    "        self.r_scale = nn.Parameter(torch.tensor([float(init_r_scale)]))\n",
    "        if learnable_norm:\n",
    "            self.w = nn.Parameter(torch.ones((input_dim,)))\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_backgnd: torch.Tensor,  # (n_backgnd, n_features)\n",
    "        y_backgnd: torch.Tensor,  # (n_backgnd,)\n",
    "        x_query: torch.Tensor,  # (n_query, n_features)\n",
    "        r: torch.Tensor,  # (n_query, n_backgnd)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns predicted y: (n_query,)\n",
    "        \"\"\"\n",
    "        n_query, n_backgnd = r.shape\n",
    "        # Expand x_backgnd and x_query to (n_query, n_backgnd, n_features)\n",
    "        x_query_exp = x_query.unsqueeze(1).expand(\n",
    "            -1, n_backgnd, -1\n",
    "        )  # (n_query, n_backgnd, n_features)\n",
    "        x_backgnd_exp = x_backgnd.unsqueeze(0).expand(\n",
    "            n_query, -1, -1\n",
    "        )  # (n_query, n_backgnd, n_features)\n",
    "\n",
    "        if self.w is not None:\n",
    "            x_dif = x_query_exp - x_backgnd_exp\n",
    "            dists = torch.norm(x_dif * self.w, dim=2)\n",
    "\n",
    "            # w_mtx = torch.eye(len(self.w)) * self.w**2\n",
    "            # dists = torch.sqrt(batched_quadratic_form(x_query_exp - x_backgnd_exp, w_mtx))\n",
    "        else:\n",
    "            # Compute L2 distances: (n_query, n_backgnd)\n",
    "            dists = torch.norm(x_query_exp - x_backgnd_exp, dim=2)\n",
    "\n",
    "        # Compute kernel weights: (n_query, n_backgnd)\n",
    "        k_vals = torch.exp(-dists / self.sigma + self.r_scale * r)\n",
    "\n",
    "        # Normalize weights\n",
    "        k_sum = k_vals.sum(dim=1, keepdim=True) + 1e-8  # avoid division by zero\n",
    "        weights = k_vals / k_sum  # (n_query, n_backgnd)\n",
    "\n",
    "        # Weighted sum of y_backgnd: (n_query,)\n",
    "        y_pred = torch.matmul(weights, y_backgnd)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "n_feats = 7\n",
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(15)):\n",
    "    x_initial, y, c = generate_multidim_noisy_data(\n",
    "        n_samples=n_samples, n_clusters=3, x_dim=n_feats, seed=seed\n",
    "    )\n",
    "    r = make_random_r(seed, c)\n",
    "\n",
    "    x_extended = np.concatenate([x_initial, r], axis=1)\n",
    "\n",
    "    for x, x_label in ((x_initial, \"xInit\"), (x_extended, \"xExtended\")):\n",
    "\n",
    "        x_mean = np.mean(x, axis=0, keepdims=True)\n",
    "        x_std = np.std(x, axis=0, keepdims=True)\n",
    "        x_norm = (x - x_mean) / x_std\n",
    "\n",
    "        r_torch = torch.Tensor(r)\n",
    "        x_torch = torch.Tensor(x_norm)\n",
    "        y_torch = torch.Tensor(y)\n",
    "\n",
    "        n_back = n_query = n_samples // 3\n",
    "        n_test = n_samples - (n_back + n_query)\n",
    "        x_back, y_back = x_torch[:n_back], y_torch[:n_back]\n",
    "        x_q, y_q = x_torch[n_back : n_query + n_back], y_torch[n_back : n_query + n_back]\n",
    "        x_val, y_val = x_torch[n_back + n_query :], y_torch[n_back + n_query :]\n",
    "        r_q_b = r_torch[n_back : n_query + n_back, :n_back]\n",
    "        \n",
    "        if x_label == \"xExtended\":\n",
    "            r_q_b = torch.zeros_like(r_q_b)\n",
    "        \n",
    "        x_train, y_train = x_torch[: n_back + n_query], y_torch[: n_back + n_query]\n",
    "        r_val_train = r_torch[n_back + n_query :, : n_back + n_query]\n",
    "\n",
    "        if x_label == \"xExtended\":\n",
    "                r_val_train = torch.zeros_like(r_val_train)\n",
    "        \n",
    "        lgb_params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}\n",
    "        train_data = lgb.Dataset(x_train.numpy(), label=y_train.numpy())\n",
    "        model_lgb = lgb.train(lgb_params, train_data)\n",
    "        y_pred = model_lgb.predict(x_val)\n",
    "        metrics[f\"lgb_{x_label}_mse\"].append(mean_squared_error(y_val, y_pred))\n",
    "        metrics[f\"lgb_{x_label}_r2\"].append(r2_score(y_val, y_pred))\n",
    "\n",
    "        for learnable_norm in (True, False):\n",
    "            model = RelNw(\n",
    "                init_sigma=1.0,\n",
    "                init_r_scale=1.0,\n",
    "                input_dim=n_feats if x_label == \"xInit\" else n_feats + n_samples,\n",
    "                learnable_norm=learnable_norm,\n",
    "            )\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            torch.manual_seed(seed)\n",
    "            model.train()\n",
    "\n",
    "            n_epochs = 1000\n",
    "            for _ in range(n_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_back, y_back, x_q, r_q_b)\n",
    "                loss = loss_fn(y_pred, y_q)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(\n",
    "                    x_train,\n",
    "                    y_train,\n",
    "                    x_val,\n",
    "                    r_val_train,\n",
    "                )\n",
    "                y_pred_np = y_pred.numpy()\n",
    "                y_val_np = y_val.numpy()\n",
    "\n",
    "                mse = mean_squared_error(y_val_np, y_pred_np)\n",
    "                r2 = r2_score(y_val_np, y_pred_np)\n",
    "                metrics[f\"learnNorm{learnable_norm}_{x_label}_mse\"].append(mse)\n",
    "                metrics[f\"learnNorm{learnable_norm}_{x_label}_r2\"].append(r2)\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}:\\tmean {np.mean(v):.4f}\\tstd {np.std(v):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69de419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
