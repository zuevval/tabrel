{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tabrel.train import train_relnet\n",
    "from tabrel.benchmark.nw_regr import run_training, generate_toy_regr_data, make_r\n",
    "\n",
    "def generate_multidim_noisy_data(n_samples: int, n_clusters: int, x_dim: int, seed: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.uniform(-1, 1, (n_samples, x_dim))\n",
    "    clusters = np.random.randint(0, n_clusters, (n_samples,))\n",
    "    y = np.sin(x[:, 0]) + np.cos(x[:, 1]) + x[:, 2] + clusters\n",
    "    return x, y, clusters\n",
    "\n",
    "def make_random_r(seed: int, clusters: np.ndarray) -> np.ndarray:\n",
    "    np.random.seed(seed)\n",
    "    n_samples = len(clusters)\n",
    "    r = np.eye(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i):\n",
    "            if clusters[i] == clusters[j] and np.random.choice((True, False)):\n",
    "                r[i, j] = 1\n",
    "                r[j, i] = 1\n",
    "    return r\n",
    "\n",
    "n_samples = 300\n",
    "n_ones = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327838e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_func in (\n",
    "    # \"square\",\n",
    "    # \"sign\", \n",
    "    \"noisy\",\n",
    "    ):\n",
    "    r2_list = []\n",
    "    mse_list = []\n",
    "    for seed in tqdm(range(1), desc=y_func):\n",
    "        if y_func == \"noisy\":\n",
    "            x, y, c = generate_multidim_noisy_data(n_samples=n_samples, n_clusters=3, x_dim=6, seed=seed)\n",
    "        else:\n",
    "            x, y, c = generate_toy_regr_data(n_samples=n_samples, n_clusters=3, y_func=y_func, seed=seed)\n",
    "            x = x.numpy()\n",
    "            y = y.numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        n_query = n_validate = n_samples // 3\n",
    "        samples_indices = np.array(list(range(n_samples)))\n",
    "        validate_indices = samples_indices[:n_validate]\n",
    "        query_indices = samples_indices[n_validate: n_query]\n",
    "        back_indices = samples_indices[n_query:]\n",
    "\n",
    "        r = make_random_r(seed, c)\n",
    "        results = run_training( # this does not work for whatever reason\n",
    "            x=x,\n",
    "            y=y,\n",
    "            r=r,\n",
    "            backgnd_indices=back_indices,\n",
    "            query_indices=query_indices,\n",
    "            val_indices=validate_indices,\n",
    "            lr=3e-3,\n",
    "            n_epochs=1500,\n",
    "            rel_as_feats=r,\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        mse_list.append({k:v[0] for k,v in results.items()})\n",
    "        r2_list.append({k:v[1] for k, v in results.items()})\n",
    "    \n",
    "    print(f\"=== {y_func} ===\")\n",
    "    for label, lst in ((\"mse\", mse_list), (\"r2\", r2_list)):\n",
    "        results_df = pd.DataFrame(lst)\n",
    "        print(\"\\n\")\n",
    "        print(label)\n",
    "        print(pd.DataFrame({\"mean\": results_df.mean().round(3), \"std\": results_df.std().round(3)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba72b3",
   "metadata": {},
   "source": [
    "Well, I give up on finding errors in my initial code, so I will just rewrite everything in a single cell. Of course, I'll copy some code from my vectorized NW implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f617fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tabrel.benchmark.nw_regr import RelNwRegr, NwModelConfig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "for seed in tqdm(range(15)):\n",
    "    # n_feats = 1\n",
    "    # x_initial, y, c = generate_toy_regr_data(\n",
    "    #      n_samples=n_samples, n_clusters=3, seed=seed, y_func=\"sign\",\n",
    "    # )\n",
    "    # r = make_r(clusters=c)\n",
    "    # x_initial, y = x_initial.numpy(), y.numpy()\n",
    "\n",
    "    n_feats = 7\n",
    "    x_initial, y, c = generate_multidim_noisy_data(\n",
    "        n_samples=n_samples, n_clusters=3, x_dim=n_feats, seed=seed\n",
    "    )\n",
    "    r = make_random_r(seed, c)\n",
    "\n",
    "    x_extended = np.concatenate([x_initial, r], axis=1)\n",
    "\n",
    "    for x, x_label in ((x_initial, \"xInit\"), (x_extended, \"xExtended\")):\n",
    "\n",
    "        x_mean = np.mean(x, axis=0, keepdims=True)\n",
    "        x_std = np.std(x, axis=0, keepdims=True)\n",
    "        x_norm = (x - x_mean) / x_std\n",
    "\n",
    "        r_torch = torch.Tensor(r)\n",
    "        x_torch = torch.Tensor(x_norm)\n",
    "        y_torch = torch.Tensor(y)\n",
    "\n",
    "        n_back = n_query = n_samples // 3\n",
    "        n_test = n_samples - (n_back + n_query)\n",
    "        x_back, y_back = x_torch[:n_back], y_torch[:n_back]\n",
    "        x_q, y_q = x_torch[n_back : n_query + n_back], y_torch[n_back : n_query + n_back]\n",
    "        x_val, y_val = x_torch[n_back + n_query :], y_torch[n_back + n_query :]\n",
    "        r_q_b = r_torch[n_back : n_query + n_back, :n_back]\n",
    "        \n",
    "        if x_label == \"xExtended\":\n",
    "            r_q_b = torch.zeros_like(r_q_b)\n",
    "        \n",
    "        x_train, y_train = x_torch[: n_back + n_query], y_torch[: n_back + n_query]\n",
    "        r_val_train = r_torch[n_back + n_query :, : n_back + n_query]\n",
    "\n",
    "        if x_label == \"xExtended\":\n",
    "                r_val_train = torch.zeros_like(r_val_train)\n",
    "        \n",
    "        # lgb_params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbosity\": -1}\n",
    "        # train_data = lgb.Dataset(x_train.numpy(), label=y_train.numpy())\n",
    "        # model_lgb = lgb.train(lgb_params, train_data)\n",
    "        # y_pred = model_lgb.predict(x_val)\n",
    "        # metrics[f\"lgb_{x_label}_mse\"].append(mean_squared_error(y_val, y_pred))\n",
    "        # metrics[f\"lgb_{x_label}_r2\"].append(r2_score(y_val, y_pred))\n",
    "\n",
    "        if x_label == \"xInit\":\n",
    "            back_indices = np.arange(n_back)\n",
    "            query_indices = np.arange(n_query) + n_back\n",
    "            val_indices = np.arange(len(x_val)) + n_back + n_query\n",
    "            results_relnet = train_relnet(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                r=r,\n",
    "                backgnd_indices=back_indices,\n",
    "                query_indices=query_indices,\n",
    "                val_indices=validate_indices,\n",
    "                lr=1e-3,\n",
    "                num_heads=4,\n",
    "                progress_bar=False,\n",
    "                n_epochs=1000,\n",
    "            )\n",
    "            relnet_mse, relnet_r2 = results_relnet[:2]\n",
    "            metrics[\"tabrel_mse\"].append(relnet_mse)\n",
    "            metrics[\"tabrel_r2\"].append(relnet_r2) \n",
    "\n",
    "        for learnable_norm in (True, False):\n",
    "            config = NwModelConfig(\n",
    "                 init_sigma=1.0,\n",
    "                init_r_scale=1.0,\n",
    "                 input_dim=n_feats if x_label == \"xInit\" else n_feats + n_samples,\n",
    "                 trainable_weights_matrix=learnable_norm,\n",
    "            )\n",
    "            model = RelNwRegr(config)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            torch.manual_seed(seed)\n",
    "            model.train()\n",
    "\n",
    "            n_epochs = 1000\n",
    "            for _ in range(n_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_back, y_back, x_q, r_q_b)\n",
    "                loss = loss_fn(y_pred, y_q)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(\n",
    "                    x_train,\n",
    "                    y_train,\n",
    "                    x_val,\n",
    "                    r_val_train,\n",
    "                )\n",
    "                y_pred_np = y_pred.numpy()\n",
    "                y_val_np = y_val.numpy()\n",
    "\n",
    "                mse = mean_squared_error(y_val_np, y_pred_np)\n",
    "                r2 = r2_score(y_val_np, y_pred_np)\n",
    "                metrics[f\"learnNorm{learnable_norm}_{x_label}_mse\"].append(mse)\n",
    "                metrics[f\"learnNorm{learnable_norm}_{x_label}_r2\"].append(r2)\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}:\\tmean {np.mean(v):.4f}\\tstd {np.std(v):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabrel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
